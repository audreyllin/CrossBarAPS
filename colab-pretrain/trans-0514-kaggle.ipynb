# Cell 1: Complete Environment Setup for Kaggle
# ========================================================

# 1. First, clean up everything
!pip uninstall -y torch torchvision torchaudio transformers peft bitsandbytes numpy 2>/dev/null || echo "No packages to uninstall"
!pip cache purge

# 2. Install NumPy first (must be done before other packages)
!pip install -q numpy==1.26.4  # Critical for compatibility

# 3. Install PyTorch with CUDA 12.1 (Kaggle's version)
!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121

# 4. Install transformer-related packages with compatible versions
!pip install -q transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1
!pip install -q bitsandbytes==0.43.0 einops==0.7.0

# 5. Handle gymnasium separately to avoid conflicts
!pip install -q gymnasium==0.29.0 --no-deps  # Force this version without dependencies

# 6. Verify installations
import os
import sys
import subprocess
import psutil
import numpy as np
import torch
import torchvision

print("\n=== Core Package Versions ===")
print(f"Python: {sys.version}")
print(f"NumPy: {np.__version__}")
print(f"PyTorch: {torch.__version__}")
print(f"Torchvision: {torchvision.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"\n=== CUDA Information ===")
    print(f"CUDA version: {torch.version.cuda}")
    print(f"Current device: {torch.cuda.current_device()}")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB")
else:
    print("\n⚠️ CUDA not available - attempting repair...")
    !pip install -q --force-reinstall torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121
    import torch
    print(f"\nAfter reinstall - CUDA available: {torch.cuda.is_available()}")

# 7. Now import transformer-related packages
from datasets import Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)

print("\n=== Transformer Packages Loaded Successfully ===")
