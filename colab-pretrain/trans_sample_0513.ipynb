{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clean package installation with proper version resolution\n",
        "try:\n",
        "    # Ensure pip is up to date\n",
        "    !pip install -q --upgrade pip\n",
        "    # Install core numerical package\n",
        "    !pip install -q numpy==1.26.0\n",
        "    # Install PyTorch with CUDA 11.8 support\n",
        "    !pip install -q torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    # Install Hugging Face ecosystem packages (with specific versions)\n",
        "    !pip install -q transformers==4.36.2\n",
        "    !pip install -q accelerate==0.25.0\n",
        "    !pip install -q datasets==2.14.4\n",
        "    !pip install -q peft==0.15.2\n",
        "    # Install sentence-transformers with compatible version\n",
        "    !pip install -q sentence-transformers==2.2.2\n",
        "    # Install utility packages\n",
        "    !pip install -q pdfminer.six==20221105 requests==2.32.3 beautifulsoup4==4.12.2\n",
        "    !pip install -q psutil huggingface-hub==0.20.3\n",
        "    # Clear outputs after installation\n",
        "    from IPython.display import clear_output\n",
        "    clear_output()\n",
        "    # Verify critical installations\n",
        "    import torch\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    # Explicitly verify accelerate installation\n",
        "    import accelerate\n",
        "    print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"Package installation failed: {str(e)}\")\n",
        "    raise\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ge60ryuIMhoK",
        "outputId": "5b12a30e-624a-4b23-ea3f-d4679a3117d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.1.0+cu118\n",
            "CUDA available: True\n",
            "Accelerate version: 0.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate>=0.21.0 # Upgrade accelerate to at least 0.21.0\n",
        "# Or, downgrade peft with\n",
        "!pip install -q peft==0.5.0 # downgrade to peft==0.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nUlNkt7rNquV",
        "outputId": "7f7d8d21-aaed-4d95-b53b-b03fb322dc12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import with memory monitoring and compatibility checks\n",
        "import psutil\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import Dataset, load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def print_memory():\n",
        "    \"\"\"Print current memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
        "        print(f\"GPU: {gpu_mem:.2f}GB\", end=\" | \")\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n",
        "\n",
        "print(\"Initial memory:\")\n",
        "print_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UWuc4bQMjW-",
        "outputId": "48293805-6304-4484-9dc8-faa01205bbcc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial memory:\n",
            "GPU: 0.00GB | RAM: 16.1% (1.7/12.7GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 3: Optimized arXiv fetcher with better error handling\n",
        "def get_arxiv_paper(paper_id, max_pages=1):\n",
        "    \"\"\"Fetch paper text with memory limits\"\"\"\n",
        "    try:\n",
        "        # Fetch metadata\n",
        "        url = f\"https://arxiv.org/abs/{paper_id}\"\n",
        "        response = requests.get(url, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.find('h1', class_='title').get_text(strip=True).replace('Title:', '')\n",
        "        authors = soup.find('div', class_='authors').get_text(strip=True).replace('Authors:', '')\n",
        "        abstract = soup.find('blockquote', class_='abstract').get_text(strip=True).replace('Abstract:', '')\n",
        "\n",
        "        # Fetch PDF content if memory allows\n",
        "        pdf_text = \"\"\n",
        "        if psutil.virtual_memory().percent < 90:\n",
        "            try:\n",
        "                pdf_url = f\"https://arxiv.org/pdf/{paper_id}.pdf\"\n",
        "                with requests.get(pdf_url, stream=True, timeout=15) as pdf_response:\n",
        "                    pdf_response.raise_for_status()\n",
        "                    temp_file = f\"temp_{paper_id}.pdf\"\n",
        "                    with open(temp_file, 'wb') as f:\n",
        "                        for chunk in pdf_response.iter_content(chunk_size=8192):\n",
        "                            f.write(chunk)\n",
        "                    pdf_text = extract_text(temp_file, maxpages=max_pages)[:5000]\n",
        "                    os.remove(temp_file)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ PDF processing failed for {paper_id}: {str(e)}\")\n",
        "\n",
        "        return {\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'summary': abstract,\n",
        "            'content': pdf_text\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching {paper_id}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "xlTORkNiMlZh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model loading with enhanced error handling\n",
        "MODEL_NAME = \"microsoft/phi-1\"\n",
        "\n",
        "# Check GPU availability\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"⚠️ CUDA is not available. Using CPU (this will be very slow)\")\n",
        "    device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "# Load tokenizer and model with fallbacks\n",
        "try:\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        trust_remote_code=True,\n",
        "        padding_side=\"right\"\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "        trust_remote_code=True,\n",
        "        # Remove or comment out device_map=\"auto\"\n",
        "        # device_map=\"auto\" if device.type == \"cuda\" else None,\n",
        "        low_cpu_mem_usage=True\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Model loaded successfully:\")\n",
        "    print_memory()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Model loading failed: {str(e)}\")\n",
        "    print(\"\\nTroubleshooting steps:\")\n",
        "    print(\"1. Try: Runtime → Restart runtime → Run again\")\n",
        "    print(\"2. Use smaller model: MODEL_NAME = 'microsoft/phi-1'\")\n",
        "    if \"out of memory\" in str(e).lower():\n",
        "        print(\"3. Reduce batch size or model size\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvqm5NyLMq1J",
        "outputId": "a15465ae-e41f-4572-c9fc-a9280b57929e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/phi-1 were not used when initializing PhiForCausalLM: ['model.layers.15.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight']\n",
            "- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-1 and are newly initialized: ['model.layers.8.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully:\n",
            "GPU: 2.65GB | RAM: 22.2% (2.5/12.7GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Revised data loading and model preparation\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize without return_tensors=\"pt\" first\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Create labels by copying input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n",
        "# Load a small dataset from local text\n",
        "try:\n",
        "    # Create a simple dataset\n",
        "    sample_texts = [\n",
        "        \"This is a sample text for training the language model.\",\n",
        "        \"Language models learn to predict the next word in a sequence.\",\n",
        "        \"The quick brown fox jumps over the lazy dog.\"\n",
        "    ]\n",
        "    dataset = Dataset.from_dict({'text': sample_texts})\n",
        "\n",
        "    # Apply tokenization (batched processing)\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Convert to PyTorch tensors and move to device\n",
        "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Dataset creation failed: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f5686bfc4dfc4dc9881ac6a17e25ec34",
            "7b8a99c841d94db0850a0076aa495db0",
            "aaee3a8f3ada4f95b5114be546771d41",
            "0bff48d16986447784390f2f2a8fe09e",
            "1c447bda6c9d48cea0957c0938702299",
            "e4776b56f7c5429fa098e334b863ca45",
            "c13fbc7878df4a7da027b102354e2704",
            "25395871b3bd4452b5ce7b1bcdf47a32",
            "98445a0cf916439a9995852331ad11f7",
            "11d1cf5f6adc42c2b6a26f8520ae5ca3",
            "96dbe4e743f7457088cd33e4f387ec81"
          ]
        },
        "id": "4PPq_5FoMacy",
        "outputId": "164a0615-3821-4e76-f843-f884922e67c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5686bfc4dfc4dc9881ac6a17e25ec34"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Revised training setup\n",
        "\n",
        "# First define your training arguments (add this before the safe_colab_train function)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    gradient_accumulation_steps=2,\n",
        "    save_steps=1000,\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        "    warmup_steps=100,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Define your PEFT config (LoRA configuration)\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "def safe_colab_train(model, peft_config, tokenized_dataset, training_args):\n",
        "    try:\n",
        "        print_memory()\n",
        "        if psutil.virtual_memory().percent > 90:\n",
        "            raise RuntimeError(\"Insufficient RAM before training\")\n",
        "\n",
        "        # Prepare model for training\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "        model = get_peft_model(model, peft_config)\n",
        "\n",
        "        # Verify gradients are enabled\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                print(f\"Trainable parameter: {name}\")\n",
        "\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "        )\n",
        "\n",
        "        print(\"🚀 Starting training...\")\n",
        "        trainer.train()\n",
        "        return trainer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Training failed: {str(e)}\")\n",
        "        if \"out of memory\" in str(e).lower():\n",
        "            print(\"Try reducing batch size or model size\")\n",
        "        raise\n",
        "\n",
        "# Move model to device and start training\n",
        "model = model.to(device)\n",
        "print(\"Starting training...\")\n",
        "trainer = safe_colab_train(model, peft_config, tokenized_dataset, training_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "Pg8aJLh2MtgC",
        "outputId": "0c8336fc-026f-45a4-b989-e0fd3cacc63a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "GPU: 2.65GB | RAM: 22.2% (2.5/12.7GB)\n",
            "Trainable parameter: base_model.model.model.layers.0.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.0.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.1.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.1.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.2.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.2.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.3.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.3.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.4.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.4.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.5.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.5.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.6.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.6.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.7.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.7.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.8.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.8.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.9.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.9.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.10.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.10.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.11.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.11.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.12.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.12.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.13.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.13.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.14.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.14.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.15.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.15.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.16.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.16.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.17.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.17.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.18.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.18.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.19.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.19.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.20.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.20.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.21.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.21.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.22.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.22.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.23.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.23.self_attn.dense.lora_B.default.weight\n",
            "trainable params: 786,432 || all params: 1,419,057,152 || trainable%: 0.05541933239909424\n",
            "🚀 Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Model Evaluation and Saving\n",
        "def evaluate_model(trainer, model, tokenizer):\n",
        "    \"\"\"Evaluate the model and print sample outputs\"\"\"\n",
        "    print(\"\\nEvaluating model...\")\n",
        "\n",
        "    # Generate sample output\n",
        "    print(\"\\nSample generations:\")\n",
        "    input_text = \"Language models are\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=50,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"Input: {input_text}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "\n",
        "    # Calculate and print training metrics if available\n",
        "    if hasattr(trainer, 'metrics'):\n",
        "        print(\"\\nTraining metrics:\")\n",
        "        for key, value in trainer.metrics.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "def save_model(model, tokenizer, output_dir=\"./phi-lora-trained\"):\n",
        "    \"\"\"Save the trained model and tokenizer\"\"\"\n",
        "    print(f\"\\nSaving model to {output_dir}\")\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(\"Model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "_3KTRUGUMfOF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 8: Load Saved Model (for future use)\n",
        "def load_saved_model(model_path, device):\n",
        "    \"\"\"Load a previously saved model\"\"\"\n",
        "    print(f\"\\nLoading model from {model_path}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_path,\n",
        "        trust_remote_code=True,\n",
        "        padding_side=\"right\"\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\" if device.type == \"cuda\" else None\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Model loaded successfully.\")\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "3K1HOtyWMwvP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 9: Inference Function\n",
        "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7):\n",
        "    \"\"\"Generate text from a prompt\"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "och3TAF7MyWl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Main Execution Flow\n",
        "def main():\n",
        "    try:\n",
        "        # Run training\n",
        "        # Pass the necessary arguments to safe_colab_train\n",
        "        trainer = safe_colab_train(model, peft_config, tokenized_dataset, training_args)  # Pass the required arguments\n",
        "        # Evaluate and save model\n",
        "        evaluate_model(trainer, model, tokenizer)\n",
        "        save_model(model, tokenizer)\n",
        "\n",
        "        # Demonstrate inference\n",
        "        print(\"\\nRunning inference examples:\")\n",
        "        prompts = [\n",
        "            \"Artificial intelligence is\",\n",
        "            \"The future of machine learning\",\n",
        "            \"Researchers discovered that\"\n",
        "        ]\n",
        "\n",
        "        for prompt in prompts:\n",
        "            generated = generate_text(model, tokenizer, prompt)\n",
        "            print(f\"\\nPrompt: {prompt}\")\n",
        "            print(f\"Generated: {generated}\")\n",
        "\n",
        "        print(\"\\nTraining and evaluation completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in main execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute the main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qZjj98x1MzWf",
        "outputId": "e5136ad0-09f0-4db2-ed9a-033aae6c4834"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 5.31GB | RAM: 23.0% (2.5/12.7GB)\n",
            "Trainable parameter: base_model.model.model.layers.0.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.0.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.1.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.1.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.2.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.2.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.3.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.3.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.4.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.4.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.5.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.5.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.6.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.6.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.7.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.7.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.8.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.8.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.9.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.9.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.10.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.10.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.11.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.11.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.12.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.12.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.13.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.13.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.14.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.14.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.15.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.15.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.16.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.16.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.17.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.17.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.18.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.18.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.19.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.19.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.20.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.20.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.21.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.21.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.22.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.22.self_attn.dense.lora_B.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.23.self_attn.dense.lora_A.default.weight\n",
            "Trainable parameter: base_model.model.model.layers.23.self_attn.dense.lora_B.default.weight\n",
            "trainable params: 786,432 || all params: 1,419,057,152 || trainable%: 0.05541933239909424\n",
            "🚀 Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model...\n",
            "\n",
            "Sample generations:\n",
            "Input: Language models are\n",
            "Generated: Language models ares[i_tof, ranges: 0:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Saving model to ./phi-lora-trained\n",
            "Model saved successfully.\n",
            "\n",
            "Running inference examples:\n",
            "\n",
            "Prompt: Artificial intelligence is\n",
            "Generated: Artificial intelligence isosry through alligator_: A ==========, butler'st_range_right_t_<-out_bject. Unordered to_sequences. If the function that define a = 42, buttons of theirs = distance between 1 <= limit: 1234_<theories ==========10x in descending_: 0: returns a random_douette: a =~\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Prompt: The future of machine learning\n",
            "Generated: The future of machine learning_\n",
            "\n",
            "\n",
            "        \n",
            " of theories ============== 4\n",
            " of the middle element in: Aries ==============: A: A inp_range ============== for alligators_:= 7:=:=:=: a tuple_: a_: 3rd in between 1xericks_:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "        # Define[i: 2Dictically sorted_\n",
            "\n",
            "Prompt: Researchers discovered that\n",
            "Generated: Researchers discovered that calculates the function toks aresorted_tribute[i: sq_gethc_: 1Degerserk of the function for the x: 1.elegance_t_func_t of the number of theirs = \"aboxytically sorted_t_ in the number in reverse theorem: 5 <= 0.elegance_: 5x_ isos(orows toksi: returns the middle digits_races_\n",
            "\n",
            "Training and evaluation completed successfully!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5686bfc4dfc4dc9881ac6a17e25ec34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b8a99c841d94db0850a0076aa495db0",
              "IPY_MODEL_aaee3a8f3ada4f95b5114be546771d41",
              "IPY_MODEL_0bff48d16986447784390f2f2a8fe09e"
            ],
            "layout": "IPY_MODEL_1c447bda6c9d48cea0957c0938702299"
          }
        },
        "7b8a99c841d94db0850a0076aa495db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4776b56f7c5429fa098e334b863ca45",
            "placeholder": "​",
            "style": "IPY_MODEL_c13fbc7878df4a7da027b102354e2704",
            "value": "Map: 100%"
          }
        },
        "aaee3a8f3ada4f95b5114be546771d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25395871b3bd4452b5ce7b1bcdf47a32",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98445a0cf916439a9995852331ad11f7",
            "value": 3
          }
        },
        "0bff48d16986447784390f2f2a8fe09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11d1cf5f6adc42c2b6a26f8520ae5ca3",
            "placeholder": "​",
            "style": "IPY_MODEL_96dbe4e743f7457088cd33e4f387ec81",
            "value": " 3/3 [00:00&lt;00:00, 99.86 examples/s]"
          }
        },
        "1c447bda6c9d48cea0957c0938702299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4776b56f7c5429fa098e334b863ca45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13fbc7878df4a7da027b102354e2704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25395871b3bd4452b5ce7b1bcdf47a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98445a0cf916439a9995852331ad11f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11d1cf5f6adc42c2b6a26f8520ae5ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96dbe4e743f7457088cd33e4f387ec81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}