{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install spacy PyPDF2 requests pandas faiss-cpu scikit-learn python-docx sumy\n!pip install pdfplumber\n!python -m spacy download en_core_web_md  # Medium-sized NLP model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import hashlib\nimport requests\nimport PyPDF2\nimport pdfplumber\nfrom urllib.parse import urlparse\nimport io\nimport spacy\nimport pandas as pd\nimport faiss\nimport numpy as np\nfrom sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.lsa import LsaSummarizer\nimport os\nfrom docx import Document","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Enhanced PDF Downloader with Caching ---\ndef get_url_hash(url):\n    \"\"\"Generate consistent hash for each URL to detect duplicates\"\"\"\n    return hashlib.md5(url.strip().encode()).hexdigest()\n\ndef download_paper(url, save_dir=\"/kaggle/working/papers\", force_redownload=False):\n    \"\"\"\n    Download PDF with:\n    - Deduplication\n    - Content validation\n    - Error handling\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    url_hash = get_url_hash(url)\n    pdf_path = os.path.join(save_dir, f\"{url_hash}.pdf\")\n    \n    # Skip if already downloaded (unless forced)\n    if os.path.exists(pdf_path) and not force_redownload:\n        print(f\"üìÅ Already exists: {pdf_path}\")\n        return pdf_path\n        \n    headers = {\n        \"User-Agent\": \"Mozilla/5.0\",\n        \"Accept\": \"application/pdf,text/html\"\n    }\n    \n    try:\n        with requests.get(url, headers=headers, stream=True, timeout=30) as response:\n            response.raise_for_status()\n            \n            # Validate content type\n            content_type = response.headers.get('Content-Type', '').lower()\n            if not ('pdf' in content_type or url.lower().endswith('.pdf')):\n                print(f\"‚ö†Ô∏è Skipping non-PDF content at {url} (Content-Type: {content_type})\")\n                return None\n            \n            # Download without progress bar\n            with open(pdf_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:  # Filter out keep-alive chunks\n                        f.write(chunk)\n                        \n        print(f\"‚úÖ Downloaded: {pdf_path}\")\n        return pdf_path\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to download {url}: {str(e)}\")\n        if os.path.exists(pdf_path):\n            os.remove(pdf_path)  # Clean up partial downloads\n        return None\n\n# --- 2. Robust PDF Processing ---\ndef validate_pdf(pdf_path):\n    \"\"\"Comprehensive PDF validation with error recovery\"\"\"\n    try:\n        # Quick magic number check\n        with open(pdf_path, 'rb') as f:\n            if f.read(4) != b'%PDF':\n                return False\n                \n        # Full structural validation\n        try:\n            with pdfplumber.open(pdf_path) as pdf:\n                if len(pdf.pages) == 0:\n                    return False\n            return True\n        except:\n            with PyPDF2.PdfReader(pdf_path) as pdf:\n                return len(pdf.pages) > 0\n                \n    except Exception as e:\n        print(f\"‚ö†Ô∏è PDF validation failed for {pdf_path}: {str(e)}\")\n        return False\n\ndef extract_text(pdf_path, max_pages=50):\n    \"\"\"Extract text with fallback mechanisms and page limits\"\"\"\n    methods = [\n        # Try pdfplumber first (better formatting preservation)\n        lambda: \" \".join(\n            p.extract_text() or \"\" \n            for p in pdfplumber.open(pdf_path).pages[:max_pages]\n        ),\n        # Fallback to PyPDF2\n        lambda: \" \".join(\n            p.extract_text() or \"\" \n            for p in PyPDF2.PdfReader(pdf_path).pages[:max_pages]\n        )\n    ]\n    \n    for method in methods:\n        try:\n            text = method()\n            if text.strip():\n                return text\n        except Exception as e:\n            continue\n            \n    print(f\"‚ö†Ô∏è All extraction methods failed for {pdf_path}\")\n    return None\n\n# --- 3. Main Pipeline ---\ndef process_papers(paper_urls, output_dir=\"/kaggle/working/papers\"):\n    \"\"\"Full processing pipeline with deduplication\"\"\"\n    papers = []\n    processed_hashes = set()\n    \n    for url in paper_urls:\n        # Skip duplicates\n        url_hash = get_url_hash(url)\n        if url_hash in processed_hashes:\n            print(f\"‚è© Skipping duplicate: {url}\")\n            continue\n            \n        # Download and validate\n        pdf_path = download_paper(url, output_dir)\n        if not pdf_path or not validate_pdf(pdf_path):\n            continue\n            \n        # Extract text\n        text = extract_text(pdf_path)\n        if not text:\n            continue\n            \n        # Store results\n        papers.append({\n            \"id\": url_hash,\n            \"title\": os.path.basename(pdf_path),\n            \"source_url\": url,\n            \"local_path\": pdf_path,\n            \"text_length\": len(text),\n            \"text_preview\": text[:1000] + \"...\" if len(text) > 1000 else text,\n            \"full_text\": text  # Warning: may be memory-intensive for many papers\n        })\n        processed_hashes.add(url_hash)\n        print(f\"‚úî Processed: {url}\")\n        \n    return papers\n\n# --- 4. Execution ---\npaper_urls = [\n    \"https://arxiv.org/pdf/2307.12874\",    \n    \"https://arxiv.org/pdf/1802.04351\",\n    \"https://arxiv.org/pdf/2306.08168\",\n    \"https://arxiv.org/pdf/2503.15964\",\n    \"https://www.jetir.org/papers/JETIR2405D82.pdf\",\n    \"https://www.cs.ucf.edu/~czou/research/subWallet-Blockchain-2019.pdf\",\n    \"https://www.cs.ucf.edu/~czou/research/Hossein-TrustCom-2020.pdf\",\n    \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n    \"https://dl.gi.de/server/api/core/bitstreams/aaa640a1-f8dd-4514-ad72-b809932072cc/content\",\n    \"https://eprint.iacr.org/2023/062.pdf\",\n    \"https://eprint.iacr.org/2022/075.pdf\",    \n    \"https://eprint.iacr.org/2023/1234.pdf\",\n    \"https://eprint.iacr.org/2020/300.pdf\",\n    \"https://eprint.iacr.org/2023/312.pdf\",\n    \"https://eprint.iacr.org/2016/013.pdf\",\n    \"https://researchmgt.monash.edu/ws/portalfiles/portal/468554595/430334621_oa.pdf\",\n    \"https://openaccess.uoc.edu/bitstream/10609/151551/1/Rahmanikivi_cbt22_empirical.pdf\",\n    \"https://ics.uci.edu/~dabrowsa/dabrowski-defi21-hwwallet.pdf\",\n    \"https://fc19.ifca.ai/preproceedings/93-preproceedings.pdf\",\n    \"https://www.jkroll.com/papers/bitcoin_threshold_signatures.pdf\",\n    \"https://corporates.db.com/files/documents/publications/db-polygo-digital-id-wp-42pp-web-secured.pdf\",\n    \"https://www.napier.ac.uk/-/media/worktribe/output-2839021/smart-contract-attacks-and-protections.ashx\",\n    \"https://www.cyprusbarassociation.org/images/6._Crypto_Wallets.pdf\",\n    \"https://computerscience.unicam.it/marcantoni/tesi/Ethereum%20Smart%20Contracts%20Optimization.pdf\",\n    \"https://cspecc.utsa.edu/publications/files/Refereed_Papers/2020_Choo_BCPPA-blockchain-cond-priv-auth-prot.pdf\",\n    \"https://www.ekonomika.org.rs/sr/PDF/ekonomika/2019/clanci19-3/7.pdf\",\n    \"https://assets.cureusjournals.com/artifacts/upload/review_article/pdf/1099/20250319-214523-194a3z.pdf\"\n]\n\n# Run pipeline\nprint(\"Starting paper processing...\")\nresults = process_papers(paper_urls)\n\n# --- 5. Results Analysis ---\nprint(f\"\\n{'='*40}\\nProcessing Complete\\n{'='*40}\")\nprint(f\"Total URLs processed: {len(paper_urls)}\")\nprint(f\"Unique valid papers extracted: {len(results)}\")\nprint(\"\\nSample results:\")\nfor paper in results[:3]:\n    print(f\"\\nüìÑ {paper['title']}\")\n    print(f\"üîó {paper['source_url']}\")\n    print(f\"üìù Length: {paper['text_length']} chars\")\n    print(f\"Preview:\\n{paper['text_preview'][:500]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_text_from_pdf(pdf_path):\n    with open(pdf_path, 'rb') as f:\n        reader = PyPDF2.PdfReader(f)\n        text = \" \".join([page.extract_text() for page in reader.pages])\n    return text\n\n# Load all papers into a list\npapers = []\nfor i in range(len(paper_urls)):\n    pdf_path = f\"/kaggle/working/papers/paper_{i}.pdf\"\n    text = extract_text_from_pdf(pdf_path)\n    papers.append({\"title\": f\"Paper_{i}\", \"text\": text})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_md\")  # For word vectors & NER","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for paper in papers:\n    paper[\"doc\"] = nlp(paper[\"text\"])  # Store spaCy doc objects","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(papers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def keyword_search(df, keyword):\n    results = []\n    for _, row in df.iterrows():\n        doc = row[\"doc\"]\n        matches = [sent.text for sent in doc.sents if keyword.lower() in sent.text.lower()]\n        if matches:\n            results.append({\"title\": row[\"title\"], \"matches\": matches})\n    return results\n\n# Example: Search for \"blockchain\"\nkeyword_search(df, \"blockchain\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert spaCy vectors to a matrix\nvectors = np.array([doc.vector for doc in df[\"doc\"]])\nindex = faiss.IndexFlatL2(vectors.shape[1])\nindex.add(vectors)\n\ndef semantic_search(query, df, top_k=3):\n    query_doc = nlp(query)\n    query_vector = np.array([query_doc.vector])\n    distances, indices = index.search(query_vector, top_k)\n    return df.iloc[indices[0]]\n\n# Example: Find papers similar to \"privacy in MPC\"\nsemantic_search(\"privacy in MPC\", df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_entities(doc):\n    return [(ent.text, ent.label_) for ent in doc.ents]\n\n# Example: Extract entities from the first paper\nentities = extract_entities(df.iloc[0][\"doc\"])\nprint(\"Entities:\", entities)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def summarize_text(text, sentences_count=3):\n    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n    summarizer = LsaSummarizer()\n    summary = summarizer(parser.document, sentences_count)\n    return \" \".join([str(sentence) for sentence in summary])\n\n# Example: Summarize the first paper\nsummary = summarize_text(df.iloc[0][\"text\"])\nprint(\"Summary:\", summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/papers_database.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r papers_database.zip /kaggle/working/papers*","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
