# -*- coding: utf-8 -*-
"""trans-0514-colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z0YNwlb1Rb_Z3hxiT7tQzX6F2JbZBJeS

Enhanced Model Trianing with Contextual Fitting
"""

!pip install --upgrade transformers

# Enhanced Model Training with Contextual Fitting
import torch
import re
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import psutil
import warnings
warnings.filterwarnings('ignore')

# Memory management functions
def print_memory():
    """Print current memory usage"""
    if torch.cuda.is_available():
        gpu_mem = torch.cuda.memory_allocated() / 1024**3
        print(f"GPU Memory: {gpu_mem:.2f}GB", end=" | ")
    ram = psutil.virtual_memory()
    print(f"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)")

def check_memory_safety(threshold=85):
    """Check if memory usage is safe"""
    if psutil.virtual_memory().percent > threshold:
        raise MemoryError(f"Memory usage exceeds {threshold}% threshold")

# Configuration
MODEL_NAME = "microsoft/phi-1.5"  # More powerful base model
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Quantization configuration for memory efficiency
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# Enhanced tokenizer with better padding
try:
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        padding_side="right"
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
except Exception as e:
    print(f"Tokenizer loading failed: {str(e)}")
    raise

# Model loading with fallbacks
try:
    print("Loading model with 4-bit quantization...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype=torch.float16
    )
    print_memory()
except Exception as e:
    print(f"Model loading failed: {str(e)}")
    print("Falling back to non-quantized version with manual device placement")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True
    ).to(DEVICE)
    print_memory()

# Enable gradient checkpointing to save memory
model.gradient_checkpointing_enable()

# Data preparation with better cleaning
def clean_text(text):
    """Enhanced text cleaning function"""
    text = re.sub(r'[^\w\s.,;!?\'"-]', '', text)  # Keep basic punctuation
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def prepare_dataset(file_path="your_data.txt", max_samples=1000):
    """Prepare dataset with streaming for large files"""
    try:
        with open(file_path) as f:
            lines = [clean_text(line) for line in f if len(line.split()) > 3][:max_samples]
            return Dataset.from_dict({"text": lines})
    except Exception as e:
        print(f"Dataset preparation failed: {str(e)}")
        # Fallback to sample data
        sample_texts = [
            "Language models are transforming AI applications.",
            "Efficient training requires careful memory management.",
            "Contextual learning improves model performance."
        ]
        return Dataset.from_dict({"text": sample_texts})

# Tokenization with better handling
def tokenize_function(examples):
    """Enhanced tokenization with attention masks"""
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
        return_tensors="pt"
    )
    tokenized["labels"] = tokenized["input_ids"].clone()
    return tokenized

# Prepare dataset
dataset = prepare_dataset()
tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# LoRA configuration for better tuning
peft_config = LoraConfig(
    r=16,  # Increased rank for better adaptation
    lora_alpha=32,
    target_modules=["Wqkv", "out_proj", "fc1", "fc2"],  # Phi-2 specific modules
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Training arguments with better defaults
training_args = TrainingArguments(
    output_dir="./phi2-lora-results",
    per_device_train_batch_size=1,  # Increased with gradient accumulation
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    optim="adamw_torch",  # Using standard adamw_torch instead of fused version
    logging_steps=10,
    save_steps=500,
    fp16=True,
    max_grad_norm=0.3,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    report_to="none",
    # The following arguments are removed/renamed in newer versions
    # evaluation_strategy="no",  # Disable evaluation if not supported
    # load_best_model_at_end=False,  # Disabled if evaluation is not supported
)

# Enhanced training function
def safe_train(model, dataset, training_args):
    try:
        check_memory_safety()
        # Prepare model
        model = prepare_model_for_kbit_training(model)
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()

        # Trainer with enhanced settings
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            # eval_dataset=dataset.select(range(10)),  # Small eval set (commented out if evaluation is disabled)
        )
        print("Starting training...")
        trainer.train()
        return trainer
    except Exception as e:
        print(f"Training failed: {str(e)}")
        if "out of memory" in str(e).lower():
            print("Try reducing batch size or using gradient checkpointing")
        raise

# Run training
trainer = safe_train(model, tokenized_dataset, training_args)

# Enhanced evaluation
def generate_with_context(model, tokenizer, prompt, context=None, max_length=100):
    """Generate text with optional context"""
    if context:
        prompt = f"Context: {context}\n\nQuestion: {prompt}\n\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example usage
context = "Language models are AI systems trained on vast amounts of text data."
prompt = "What are the main applications of these models?"
print(generate_with_context(model, tokenizer, prompt, context))

# Save model with better organization
def save_artifacts(model, tokenizer, output_dir):
    """Save all training artifacts"""
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    # Save training arguments
    training_args.save_to_json(f"{output_dir}/training_args.json")
    print(f"Model and artifacts saved to {output_dir}")

save_artifacts(model, tokenizer, "./phi2-lora-trained")