{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":241495234,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:04:42.068549Z","iopub.execute_input":"2025-05-23T23:04:42.069027Z","iopub.status.idle":"2025-05-23T23:04:42.074218Z","shell.execute_reply.started":"2025-05-23T23:04:42.068998Z","shell.execute_reply":"2025-05-23T23:04:42.073161Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Cell 1: Complete Environment Setup\n# ========================================================\n\n# 1. Initial imports and setup\nimport sys\nimport os\nimport shutil\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 2. Clean up existing installations\nprint(\"=== Cleaning up existing installations ===\")\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Remove problematic directories\nproblematic_path = \"/usr/local/lib/python3.11/dist-packages/~vidia-cudnn-cu12\"\nif os.path.exists(problematic_path):\n    shutil.rmtree(problematic_path)\n\n# Clear pip cache\n!pip cache purge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:04:42.076138Z","iopub.execute_input":"2025-05-23T23:04:42.076479Z","iopub.status.idle":"2025-05-23T23:04:45.743270Z","shell.execute_reply.started":"2025-05-23T23:04:42.076444Z","shell.execute_reply":"2025-05-23T23:04:45.741546Z"}},"outputs":[{"name":"stdout","text":"=== Cleaning up existing installations ===\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1+cu121\nUninstalling torch-2.2.1+cu121:\n  Successfully uninstalled torch-2.2.1+cu121\nFound existing installation: torchvision 0.17.1+cu121\nUninstalling torchvision-0.17.1+cu121:\n  Successfully uninstalled torchvision-0.17.1+cu121\nFound existing installation: torchaudio 2.2.1+cu121\nUninstalling torchaudio-2.2.1+cu121:\n  Successfully uninstalled torchaudio-2.2.1+cu121\nFiles removed: 70 (790.5 MB)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# 3. Install core packages with precise version control\nprint(\"\\n=== Installing core packages ===\")\n!pip install -q --upgrade pip\n!pip install -q --force-reinstall numpy==1.26.4\n\n# Force reload numpy\nif 'numpy' in sys.modules:\n    del sys.modules['numpy']\nimport numpy as np\nprint(f\"NumPy version: {np.__version__}\")\n\n# 4. Install PyTorch with CUDA 12.1\nprint(\"\\n=== Installing PyTorch ===\")\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# 5. Install transformer ecosystem with compatible versions\nprint(\"\\n=== Installing transformer packages ===\")\n!pip install -q \\\n    transformers==4.41.2 \\\n    peft==0.10.0 \\\n    datasets==2.18.0 \\\n    accelerate==0.29.1 \\\n    einops==0.7.0 \\\n    scipy==1.13.0 \\\n    scikit-learn==1.3.2 \\\n    matplotlib==3.8.0 \\\n    pandas==2.2.2\n\n# 6. Install bitsandbytes with GPU support\nprint(\"\\n=== Installing bitsandbytes ===\")\ntry:\n    !pip install -q bitsandbytes==0.43.0\nexcept:\n    print(\"Failed to install bitsandbytes with GPU support, installing CPU-only version\")\n    !pip install -q bitsandbytes-cpu==0.43.0\n\n# 7. Verify installations\nprint(\"\\n=== Verifying installations ===\")\nimport torch\nimport torchvision\n\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Torchvision: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"\\n=== CUDA Information ===\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:04:45.744911Z","iopub.execute_input":"2025-05-23T23:04:45.745381Z","iopub.status.idle":"2025-05-23T23:06:44.843700Z","shell.execute_reply.started":"2025-05-23T23:04:45.745333Z","shell.execute_reply":"2025-05-23T23:06:44.842263Z"}},"outputs":[{"name":"stdout","text":"\n=== Installing core packages ===\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nkaggle-environments 1.16.11 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNumPy version: 1.26.4\n\n=== Installing PyTorch ===\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]3\u001b[0m [torchaudio]]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n=== Installing transformer packages ===\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/6\u001b[0m [peft][32m5/6\u001b[0m [peft]formers]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.0 which is incompatible.\nkaggle-environments 1.16.11 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n=== Installing bitsandbytes ===\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\n=== Verifying installations ===\n\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.2.1+cu121\nTorchvision: 0.17.1+cu121\nCUDA available: False\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# 8. Import transformer packages\ntry:\n    from datasets import Dataset, load_dataset\n    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoTokenizer,\n        TrainingArguments,\n        Trainer,\n        BitsAndBytesConfig,\n        pipeline\n    )\n    print(\"\\n=== Transformer Packages Loaded Successfully ===\")\nexcept ImportError as e:\n    print(f\"\\nError importing transformer packages: {e}\")\n    raise\n\n# 9. Additional verification\ntry:\n    import scipy\n    import sklearn\n    print(f\"\\nAdditional Package Versions:\")\n    print(f\"SciPy: {scipy.__version__}\")\n    print(f\"scikit-learn: {sklearn.__version__}\")\nexcept ImportError as e:\n    print(f\"\\nWarning: Could not verify some package versions - {e}\")\n\n# 10. Clean up warnings\nwarnings.filterwarnings('ignore', message='.*The installed version of bitsandbytes.*')\nwarnings.filterwarnings('ignore', message='.*was compiled without GPU support.*')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:06:44.845374Z","iopub.execute_input":"2025-05-23T23:06:44.845736Z","iopub.status.idle":"2025-05-23T23:06:44.854678Z","shell.execute_reply.started":"2025-05-23T23:06:44.845689Z","shell.execute_reply":"2025-05-23T23:06:44.853640Z"}},"outputs":[{"name":"stdout","text":"\n=== Transformer Packages Loaded Successfully ===\n\nAdditional Package Versions:\nSciPy: 1.15.2\nscikit-learn: 1.2.2\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Cell 2: Model Loading with Robust Error Handling\n# ================================================\n\nMODEL_NAME = \"gpt2\"  # Default model\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics\"\"\"\n    import psutil\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    \"\"\"Improved model loading with fallback options\"\"\"\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    trust_remote_code = True\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    # Try loading with different configurations\n    configs_to_try = [\n        {\n            \"name\": \"4-bit quantized\",\n            \"quantization_config\": BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch_dtype\n            ),\n            \"device_map\": \"auto\"\n        },\n        {\n            \"name\": \"8-bit quantized\",\n            \"quantization_config\": BitsAndBytesConfig(load_in_8bit=True),\n            \"device_map\": \"auto\"\n        },\n        {\n            \"name\": \"Full precision\",\n            \"quantization_config\": None,\n            \"device_map\": \"auto\" if torch.cuda.is_available() else None\n        },\n        {\n            \"name\": \"CPU-only\",\n            \"quantization_config\": None,\n            \"device_map\": None,\n            \"torch_dtype\": torch.float32\n        }\n    ]\n    \n    for config in configs_to_try:\n        try:\n            print(f\"\\nAttempting {config['name']} load...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                quantization_config=config.get(\"quantization_config\"),\n                trust_remote_code=trust_remote_code,\n                device_map=config[\"device_map\"],\n                torch_dtype=config.get(\"torch_dtype\", torch_dtype)\n            )\n            print(f\"\\nâœ… Model loaded successfully ({config['name']})!\")\n            print_memory()\n            return model\n        except Exception as e:\n            print(f\"âŒ {config['name']} load failed: {str(e)}\")\n            continue\n    \n    raise RuntimeError(\"All model loading attempts failed\")\n\ntry:\n    model = load_model(MODEL_NAME)\nexcept Exception as e:\n    print(f\"\\nâŒ Final model loading error: {str(e)}\")\n    print(\"Trying basic CPU load as last resort...\")\n    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cpu\", torch_dtype=torch.float32)\n    print(\"\\nâœ… Model loaded on CPU\")\n    print_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:06:44.857759Z","iopub.execute_input":"2025-05-23T23:06:44.858089Z","iopub.status.idle":"2025-05-23T23:06:45.629595Z","shell.execute_reply.started":"2025-05-23T23:06:44.858066Z","shell.execute_reply":"2025-05-23T23:06:45.628531Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 10.2% (2.8/31.4GB)\n\nAttempting 4-bit quantized load...\nâŒ 4-bit quantized load failed: No GPU found. A GPU is needed for quantization.\n\nAttempting 8-bit quantized load...\nâŒ 8-bit quantized load failed: No GPU found. A GPU is needed for quantization.\n\nAttempting Full precision load...\n\nâœ… Model loaded successfully (Full precision)!\nRAM: 11.5% (3.1/31.4GB)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup\n# =======================\n\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:06:45.630764Z","iopub.execute_input":"2025-05-23T23:06:45.631465Z","iopub.status.idle":"2025-05-23T23:06:45.884711Z","shell.execute_reply.started":"2025-05-23T23:06:45.631425Z","shell.execute_reply":"2025-05-23T23:06:45.883468Z"}},"outputs":[{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Cell 4: Robust Data Preparation from CSV\n# =============================================\n\n# 0. Set critical environment variables first\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n# 1. First verify the exact file path\nimport pandas as pd\nfrom datasets import Dataset\n\n# Check the exact path to your CSV file\ndataset_path = \"/kaggle/input/database-0522/papers_database.csv\"\nif not os.path.exists(dataset_path):\n    # Try to find the correct path if the default doesn't work\n    print(\"Searching for CSV file...\")\n    # !find /kaggle/input -name \"*.csv\" 2>/dev/null\n    raise FileNotFoundError(f\"Could not find papers_database.csv at {dataset_path}\")\n\n# 2. Enhanced dataset preparation with better CSV handling\ndef prepare_dataset(file_path, max_samples=None):\n    \"\"\"Prepare dataset from CSV with robust error handling\"\"\"\n    try:\n        # First try with pandas for better CSV handling\n        df = pd.read_csv(file_path)\n        \n        # Apply max_samples if specified\n        if max_samples is not None and len(df) > max_samples:\n            df = df.sample(max_samples)\n        \n        # Standardize the text column name\n        text_col = None\n        possible_text_columns = ['text', 'content', 'paper_text', 'abstract', 'body', 'article', 'paper']\n        \n        for col in possible_text_columns:\n            if col in df.columns:\n                text_col = col\n                break\n        \n        if text_col is None:\n            # Find first string-type column\n            for col in df.columns:\n                if pd.api.types.is_string_dtype(df[col]):\n                    text_col = col\n                    break\n            else:\n                # If no string column found, convert first column to string\n                text_col = df.columns[0]\n                df[text_col] = df[text_col].astype(str)\n        \n        # Rename to 'text' if needed\n        if text_col != 'text':\n            df = df.rename(columns={text_col: 'text'})\n        \n        # Clean text data\n        df['text'] = df['text'].str.strip().replace(r'\\s+', ' ', regex=True)\n        \n        # Convert to HuggingFace Dataset\n        dataset = Dataset.from_pandas(df)\n        print(f\"âœ… Loaded dataset with {len(dataset)} samples\")\n        return dataset\n        \n    except Exception as e:\n        print(f\"\\nâŒ Dataset preparation failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\"Sample text \" + str(i) for i in range(10)]})\n\n# 3. Enhanced Tokenization function\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with multiple fallback strategies\"\"\"\n    try:\n        # First try standard tokenization\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=512,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n            \n    except Exception as e:\n        print(f\"âš ï¸ Tokenization warning: {str(e)}\")\n        # Fallback strategy\n        seq_length = min(512, tokenizer.model_max_length)\n        return {\n            \"input_ids\": [[tokenizer.pad_token_id or 0] * seq_length],\n            \"attention_mask\": [[1] * seq_length],\n            \"labels\": [[tokenizer.pad_token_id or 0] * seq_length]\n        }\n\n# 4. Main processing with comprehensive error handling\ntry:\n    print(\"\\n=== Starting Processing ===\")\n    \n    # Initialize tokenizer if not already defined\n    if 'tokenizer' not in globals():\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Default model\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load and prepare dataset\n    dataset = prepare_dataset(dataset_path)\n    \n    # Small batch test first\n    test_size = min(2, len(dataset))\n    test_batch = dataset.select(range(test_size))\n    test_tokenized = test_batch.map(safe_tokenize, batched=True)\n    print(\"\\nTest batch processed successfully!\")\n    print(f\"Sample text: {test_batch[0]['text'][:100]}...\")\n    print(f\"Tokenized length: {len(test_tokenized[0]['input_ids'])}\")\n    \n    # Full dataset processing with progress reporting\n    print(\"\\nProcessing full dataset...\")\n    tokenized_dataset = dataset.map(\n        safe_tokenize,\n        batched=True,\n        batch_size=8,\n        remove_columns=['text'],\n        writer_batch_size=1000\n    )\n    \n    # Set format for PyTorch\n    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    \n    print(\"\\nâœ… Processing completed successfully!\")\n    print(f\"Final dataset features: {tokenized_dataset.features}\")\n    print(f\"Number of samples: {len(tokenized_dataset)}\")\n    \nexcept Exception as e:\n    print(f\"\\nâŒ Critical error in processing: {str(e)}\")\n    print(\"Creating minimal fallback dataset...\")\n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": [[0] * 512],\n        \"attention_mask\": [[1] * 512],\n        \"labels\": [[0] * 512]\n    })\n    tokenized_dataset.set_format(type='torch')\n    print(\"âš ï¸ Using fallback dataset - model may not train properly\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:33:17.180333Z","iopub.execute_input":"2025-05-23T23:33:17.180957Z","iopub.status.idle":"2025-05-23T23:33:17.797128Z","shell.execute_reply.started":"2025-05-23T23:33:17.180923Z","shell.execute_reply":"2025-05-23T23:33:17.795874Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Processing ===\nâœ… Loaded dataset with 60 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d056f6cb4b0147f4b287d9d4fb4a0bf1"}},"metadata":{}},{"name":"stdout","text":"\nTest batch processed successfully!\nSample text: a009a494d6d220b47368efac02c33672...\nTokenized length: 512\n\nProcessing full dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9031e9b3ff3e4d3cb8ffbd67e716ae84"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Processing completed successfully!\nFinal dataset features: {'title': Value(dtype='string', id=None), 'source_url': Value(dtype='string', id=None), 'local_path': Value(dtype='string', id=None), 'text_length': Value(dtype='int64', id=None), 'text_preview': Value(dtype='string', id=None), 'full_text': Value(dtype='string', id=None), 'doc': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\nNumber of samples: 60\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Cell 5: Training Configuration\n# =============================\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\n\n# LoRA configuration\nfrom peft import LoraConfig\n\npeft_config = LoraConfig(\n    r=16,  \n    lora_alpha=32,\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # GPT-2 compatible modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    fan_in_fan_out=True\n)\n\n# Training arguments optimized for Kaggle\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/phi1.5-lora-results\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,  # Reduced for Kaggle\n    learning_rate=2e-5,\n    optim=\"adamw_torch\",\n    logging_steps=10,\n    save_steps=500,\n    fp16=torch.cuda.is_available(),\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\"\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:33:17.799093Z","iopub.execute_input":"2025-05-23T23:33:17.799448Z","iopub.status.idle":"2025-05-23T23:33:17.891784Z","shell.execute_reply.started":"2025-05-23T23:33:17.799422Z","shell.execute_reply":"2025-05-23T23:33:17.890664Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Cell 6: Training Execution\n# =========================\n\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:33:17.893151Z","iopub.execute_input":"2025-05-23T23:33:17.893519Z","iopub.status.idle":"2025-05-23T23:38:01.759968Z","shell.execute_reply.started":"2025-05-23T23:33:17.893488Z","shell.execute_reply":"2025-05-23T23:38:01.758877Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nRAM: 12.3% (3.4/31.4GB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 04:24, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>8.227700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# Cell 7: Enhanced Model Saving with Shard Support\n# ===============================================\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\nğŸ’¾ Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"ğŸ’½ Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"ğŸ”¤ Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"ğŸ“ Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"âš ï¸ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"âš ï¸ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\nâš ï¸ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\nğŸ” Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:38:01.762481Z","iopub.execute_input":"2025-05-23T23:38:01.762839Z","iopub.status.idle":"2025-05-23T23:38:01.775532Z","shell.execute_reply.started":"2025-05-23T23:38:01.762796Z","shell.execute_reply":"2025-05-23T23:38:01.774226Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Cell 8: Robust Model Loading and Testing with PEFT support\n# ========================================================\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 160,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\nğŸ” Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\nğŸ“‚ Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\nğŸ”„ Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\nğŸ”„ Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"âš ï¸ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\nğŸ‰ Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline - REMOVED device parameter since we're using device_map=\"auto\"\n        print(\"\\nğŸš€ Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        # Run tests\n        print(\"\\nğŸ§ª Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\nğŸ”¹ Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"ğŸ’¬ Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\nâŒ Critical error loading model: {str(e)}\")\n        print(\"\\nğŸ› ï¸ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:38:01.776730Z","iopub.execute_input":"2025-05-23T23:38:01.777723Z","iopub.status.idle":"2025-05-23T23:38:01.806064Z","shell.execute_reply.started":"2025-05-23T23:38:01.777683Z","shell.execute_reply":"2025-05-23T23:38:01.804976Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    model_path = \"/kaggle/working/gpt2-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    load_and_test_model(model_path, is_peft_model=True)\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \"What is software wallet, and what's the difference between hardware and software wallet? \",\n        \"What is PoW? \",\n        \"Explain PoW in 1 sentence. \",\n        \"Describe the key features of PoW using 3 words. \",\n        \"What is PoM? Is it something related to cryptography? \",\n        \"What is a cryptographic product? \",\n        \"What is P2P? \",\n        \"What is block chain? \",\n        \"What is public key, and what's the difference between private and public key? \"\n    ]\n    load_and_test_model(model_path, test_prompts=custom_prompts, is_peft_model=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:38:01.807408Z","iopub.execute_input":"2025-05-23T23:38:01.807794Z","iopub.status.idle":"2025-05-23T23:38:02.093072Z","shell.execute_reply.started":"2025-05-23T23:38:01.807758Z","shell.execute_reply":"2025-05-23T23:38:02.091669Z"}},"outputs":[{"name":"stdout","text":"\nğŸ’¾ Saving model artifacts to: /kaggle/working/gpt2-lora-trained\nğŸ’½ Saving model and adapter...\nğŸ”¤ Saving tokenizer...\nğŸ“ Saving training arguments...\n\nğŸ” Verifying saved files:\n- tokenizer_config.json (0.49 KB)\n- adapter_model.safetensors (9234.25 KB)\n- adapter_config.json (0.66 KB)\n- merges.txt (445.62 KB)\n- vocab.json (779.45 KB)\n- tokenizer.json (2058.55 KB)\n- README.md (4.96 KB)\n- training_args.json (3.86 KB)\n- special_tokens_map.json (0.13 KB)\n\nğŸ” Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\nğŸ“‚ Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.66 KB)\n- adapter_model.safetensors (9234.25 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\nğŸ”„ Loading tokenizer...\n\nğŸ”„ Loading model...\nFound adapter files: ['adapter_model.safetensors', 'adapter_config.json']\n\nâŒ Critical error loading model: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like None is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n\nğŸ› ï¸ Debugging info:\n- Path: /kaggle/working/gpt2-lora-trained\n- Directory exists: True\n- Contents: ['tokenizer_config.json', 'adapter_model.safetensors', 'adapter_config.json', 'merges.txt', 'vocab.json', 'tokenizer.json', 'README.md', 'training_args.json', 'special_tokens_map.json']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1635\u001b[0m             \u001b[0;34m\"Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLocalEntryNotFoundError\u001b[0m: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_64/81389042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Load with explicit path and PEFT flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mload_and_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Test with custom prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_64/480928771.py\u001b[0m in \u001b[0;36mload_and_test_model\u001b[0;34m(model_path, max_length, test_prompts, is_peft_model)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found adapter files: {adapter_files}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# Load base model first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         ):\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like None is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."],"ename":"OSError","evalue":"We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like None is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.","output_type":"error"}],"execution_count":55},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:38:02.093627Z","iopub.status.idle":"2025-05-23T23:38:02.093943Z","shell.execute_reply.started":"2025-05-23T23:38:02.093778Z","shell.execute_reply":"2025-05-23T23:38:02.093790Z"}},"outputs":[],"execution_count":null}]}