{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:37:02.127170Z","iopub.execute_input":"2025-05-23T22:37:02.128042Z","iopub.status.idle":"2025-05-23T22:37:02.132944Z","shell.execute_reply.started":"2025-05-23T22:37:02.128003Z","shell.execute_reply":"2025-05-23T22:37:02.131727Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Minimal working version with maximum compatibility\n!pip install -q \\\n    transformers==4.30.2 \\\n    torch==2.1.0 \\\n    numpy==1.23.5 \\\n    --ignore-installed\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nprint(\"Model loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:37:02.140504Z","iopub.execute_input":"2025-05-23T22:37:02.141040Z","iopub.status.idle":"2025-05-23T22:39:07.542571Z","shell.execute_reply.started":"2025-05-23T22:37:02.140998Z","shell.execute_reply":"2025-05-23T22:39:07.539718Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlcrate 0.2.0 requires pathos, which is not installed.\ntorchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.0 which is incompatible.\ntorchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.0 which is incompatible.\ndatasets 2.14.6 requires fsspec[http]<=2023.10.0,>=2023.1.0, but you have fsspec 2025.5.0 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.23.5 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nlangchain-core 0.3.50 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\nscikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.0 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nchex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nxarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/443424857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0muse_pretrained_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_pretrained_backbone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mout_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         )\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping_values\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         mapping_items = [\n\u001b[1;32m    700\u001b[0m             (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blenderbot/configuration_blenderbot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOnnxConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOnnxConfigWithPast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOnnxSeq2SeqConfigWithPast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_effective_axis_dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Backward compatibility imports, to make sure all those objects can be found in file_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mCLOUDFRONT_DISTRIB_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'get_cached_models' from 'transformers.utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'get_cached_models' from 'transformers.utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py)","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"MODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME,\n    padding_side=\"right\",\n    trust_remote_code=False\n)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    trust_remote_code=False\n)\n\nprint(\"Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.543294Z","iopub.status.idle":"2025-05-23T22:39:07.543601Z","shell.execute_reply.started":"2025-05-23T22:39:07.543460Z","shell.execute_reply":"2025-05-23T22:39:07.543473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a minimal dummy dataset\nfrom datasets import Dataset\ndummy_data = {\"text\": [\"This is a sample text.\"] * 10}\ndataset = Dataset.from_dict(dummy_data)\nprint(f\"Dummy dataset size: {len(dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.545162Z","iopub.status.idle":"2025-05-23T22:39:07.545515Z","shell.execute_reply.started":"2025-05-23T22:39:07.545342Z","shell.execute_reply":"2025-05-23T22:39:07.545362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset from Kaggle working directory\ntry:\n    df = pd.read_csv(\"/kaggle/working/papers_database.csv\")\n    print(f\"✅ Loaded {len(df)} papers from /kaggle/working/papers_database.csv\")\n    \n    # Check if required columns exist\n    required_columns = ['id', 'title', 'source_url', 'full_text']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    \n    if missing_columns:\n        print(f\"⚠️ Missing columns: {missing_columns}\")\n        # Add missing columns with empty values\n        for col in missing_columns:\n            df[col] = \"\"\n    \nexcept Exception as e:\n    print(f\"❌ CSV load failed: {str(e)}\")\n    # Create empty dataframe with required structure\n    df = pd.DataFrame(columns=['id', 'title', 'source_url', 'full_text'])\n    print(\"Created empty dataframe as fallback\")\n\n# Display sample data\nprint(\"\\nSample data:\")\nprint(df.head(2))\n# Process text into chunks\ndef chunk_text(text, chunk_size=512):\n    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)] if pd.notna(text) else []\n\nsamples = []\nfor _, row in df.iterrows():\n    chunks = chunk_text(row.get('full_text', ''))\n    samples.extend([{\"text\": chunk, \"source\": row.get('title', '')} for chunk in chunks])\n\ndataset = Dataset.from_list(samples[:1000]) if samples else Dataset.from_dict({\"text\": [\"Sample text\"]*10})\nprint(f\"Dataset size: {len(dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.546818Z","iopub.status.idle":"2025-05-23T22:39:07.547570Z","shell.execute_reply.started":"2025-05-23T22:39:07.547404Z","shell.execute_reply":"2025-05-23T22:39:07.547421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"gpt2\"\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# Model with 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.548371Z","iopub.status.idle":"2025-05-23T22:39:07.548680Z","shell.execute_reply.started":"2025-05-23T22:39:07.548547Z","shell.execute_reply":"2025-05-23T22:39:07.548560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize dataset\ntokenized_dataset = dataset.map(\n    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=512, padding=\"max_length\"),\n    batched=True\n)\ntokenized_dataset.set_format(type='torch')\n\n# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/results\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    learning_rate=2e-5,\n    fp16=True,\n    logging_steps=10,\n    save_steps=500\n)\n\n# Prepare model\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.549982Z","iopub.status.idle":"2025-05-23T22:39:07.550601Z","shell.execute_reply.started":"2025-05-23T22:39:07.550379Z","shell.execute_reply":"2025-05-23T22:39:07.550401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset\n)\n\nprint(\"Starting training...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.551730Z","iopub.status.idle":"2025-05-23T22:39:07.552124Z","shell.execute_reply.started":"2025-05-23T22:39:07.551937Z","shell.execute_reply":"2025-05-23T22:39:07.551953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/final_model\")\n!zip -r /kaggle/working/results.zip /kaggle/working/results\nprint(\"\\nFinal outputs:\")\n!ls -lh /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.553546Z","iopub.status.idle":"2025-05-23T22:39:07.553859Z","shell.execute_reply.started":"2025-05-23T22:39:07.553695Z","shell.execute_reply":"2025-05-23T22:39:07.553706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"💽 Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n⚠️ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.555621Z","iopub.status.idle":"2025-05-23T22:39:07.556049Z","shell.execute_reply.started":"2025-05-23T22:39:07.555845Z","shell.execute_reply":"2025-05-23T22:39:07.555863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 160,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline - REMOVED device parameter since we're using device_map=\"auto\"\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"💬 Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.557130Z","iopub.status.idle":"2025-05-23T22:39:07.557424Z","shell.execute_reply.started":"2025-05-23T22:39:07.557301Z","shell.execute_reply":"2025-05-23T22:39:07.557312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    model_path = \"/kaggle/working/gpt2-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    load_and_test_model(model_path, is_peft_model=True)\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \"What is software wallet, and what's the difference between hardware and software wallet? \",\n        \"What is PoW? \",\n        \"Explain PoW in 1 sentence. \",\n        \"Describe the key features of PoW using 3 words. \",\n        \"What is PoM? Is it something related to cryptography? \",\n        \"What is a cryptographic product? \",\n        \"What is P2P? \",\n        \"What is block chain? \",\n        \"What is public key, and what's the difference between private and public key? \"\n    ]\n    load_and_test_model(model_path, test_prompts=custom_prompts, is_peft_model=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.558407Z","iopub.status.idle":"2025-05-23T22:39:07.558749Z","shell.execute_reply.started":"2025-05-23T22:39:07.558550Z","shell.execute_reply":"2025-05-23T22:39:07.558567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:39:07.559755Z","iopub.status.idle":"2025-05-23T22:39:07.560069Z","shell.execute_reply.started":"2025-05-23T22:39:07.559937Z","shell.execute_reply":"2025-05-23T22:39:07.559950Z"}},"outputs":[],"execution_count":null}]}