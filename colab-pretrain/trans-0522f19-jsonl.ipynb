{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":241495234,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:05:44.943811Z","iopub.execute_input":"2025-05-24T00:05:44.944208Z","iopub.status.idle":"2025-05-24T00:05:44.953409Z","shell.execute_reply.started":"2025-05-24T00:05:44.944158Z","shell.execute_reply":"2025-05-24T00:05:44.951943Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Complete Environment Setup\n# ========================================================\n\n# 1. Initial imports and setup\nimport sys\nimport os\nimport shutil\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 2. Clean up existing installations\nprint(\"=== Cleaning up existing installations ===\")\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Remove problematic directories\nproblematic_path = \"/usr/local/lib/python3.11/dist-packages/~vidia-cudnn-cu12\"\nif os.path.exists(problematic_path):\n    shutil.rmtree(problematic_path)\n\n# Clear pip cache\n!pip cache purge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:05:44.955148Z","iopub.execute_input":"2025-05-24T00:05:44.955530Z","iopub.status.idle":"2025-05-24T00:06:13.745003Z","shell.execute_reply.started":"2025-05-24T00:05:44.955494Z","shell.execute_reply":"2025-05-24T00:06:13.743910Z"}},"outputs":[{"name":"stdout","text":"=== Cleaning up existing installations ===\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nFound existing installation: transformers 4.51.3\nUninstalling transformers-4.51.3:\n  Successfully uninstalled transformers-4.51.3\nFound existing installation: peft 0.14.0\nUninstalling peft-0.14.0:\n  Successfully uninstalled peft-0.14.0\n\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n\u001b[0mFiles removed: 0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 3. Install core packages with precise version control\nprint(\"\\n=== Installing core packages ===\")\n!pip install -q --upgrade pip\n!pip install -q --force-reinstall numpy==1.26.4\n\n# Force reload numpy\nif 'numpy' in sys.modules:\n    del sys.modules['numpy']\nimport numpy as np\nprint(f\"NumPy version: {np.__version__}\")\n\n# 4. Install PyTorch with CUDA 12.1\nprint(\"\\n=== Installing PyTorch ===\")\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# 5. Install transformer ecosystem with compatible versions\nprint(\"\\n=== Installing transformer packages ===\")\n!pip install -q \\\n    transformers==4.41.2 \\\n    peft==0.10.0 \\\n    datasets==2.18.0 \\\n    accelerate==0.29.1 \\\n    einops==0.7.0 \\\n    scipy==1.13.0 \\\n    scikit-learn==1.3.2 \\\n    matplotlib==3.8.0 \\\n    pandas==2.2.2\n\n# 6. Install bitsandbytes with GPU support\nprint(\"\\n=== Installing bitsandbytes ===\")\ntry:\n    !pip install -q bitsandbytes==0.43.0\nexcept:\n    print(\"Failed to install bitsandbytes with GPU support, installing CPU-only version\")\n    !pip install -q bitsandbytes-cpu==0.43.0\n\n# 7. Verify installations\nprint(\"\\n=== Verifying installations ===\")\nimport torch\nimport torchvision\n\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Torchvision: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"\\n=== CUDA Information ===\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:06:13.746404Z","iopub.execute_input":"2025-05-24T00:06:13.746774Z","iopub.status.idle":"2025-05-24T00:09:56.272338Z","shell.execute_reply.started":"2025-05-24T00:06:13.746736Z","shell.execute_reply":"2025-05-24T00:09:56.271341Z"}},"outputs":[{"name":"stdout","text":"\n=== Installing core packages ===\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\naccelerate 1.5.2 requires torch>=2.0.0, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNumPy version: 1.26.4\n\n=== Installing PyTorch ===\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/15\u001b[0m [torchaudio]5\u001b[0m [torchaudio]]-cu12]12]2]2]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\n=== Installing transformer packages ===\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m128.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/12\u001b[0m [peft]2m11/12\u001b[0m [peft]erate]s]x]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n=== Installing bitsandbytes ===\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\n=== Verifying installations ===\n\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.2.1+cu121\nTorchvision: 0.17.1+cu121\nCUDA available: False\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 8. Import transformer packages\ntry:\n    from datasets import Dataset, load_dataset\n    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoTokenizer,\n        TrainingArguments,\n        Trainer,\n        BitsAndBytesConfig,\n        pipeline\n    )\n    print(\"\\n=== Transformer Packages Loaded Successfully ===\")\nexcept ImportError as e:\n    print(f\"\\nError importing transformer packages: {e}\")\n    raise\n\n# 9. Additional verification\ntry:\n    import scipy\n    import sklearn\n    print(f\"\\nAdditional Package Versions:\")\n    print(f\"SciPy: {scipy.__version__}\")\n    print(f\"scikit-learn: {sklearn.__version__}\")\nexcept ImportError as e:\n    print(f\"\\nWarning: Could not verify some package versions - {e}\")\n\n# 10. Clean up warnings\nwarnings.filterwarnings('ignore', message='.*The installed version of bitsandbytes.*')\nwarnings.filterwarnings('ignore', message='.*was compiled without GPU support.*')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:09:56.274301Z","iopub.execute_input":"2025-05-24T00:09:56.274709Z","iopub.status.idle":"2025-05-24T00:10:16.757727Z","shell.execute_reply.started":"2025-05-24T00:09:56.274679Z","shell.execute_reply":"2025-05-24T00:10:16.756908Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"},{"name":"stderr","text":"2025-05-24 00:10:01.635247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748045401.900132      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748045401.979552      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n=== Transformer Packages Loaded Successfully ===\n\nAdditional Package Versions:\nSciPy: 1.13.0\nscikit-learn: 1.3.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 2: Model Loading with Robust Error Handling\n# ================================================\n\nMODEL_NAME = \"gpt2\"  # Default model\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics\"\"\"\n    import psutil\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    \"\"\"Improved model loading with fallback options\"\"\"\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    trust_remote_code = True\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    # Try loading with different configurations\n    configs_to_try = [\n        {\n            \"name\": \"4-bit quantized\",\n            \"quantization_config\": BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch_dtype\n            ),\n            \"device_map\": \"auto\"\n        },\n        {\n            \"name\": \"8-bit quantized\",\n            \"quantization_config\": BitsAndBytesConfig(load_in_8bit=True),\n            \"device_map\": \"auto\"\n        },\n        {\n            \"name\": \"Full precision\",\n            \"quantization_config\": None,\n            \"device_map\": \"auto\" if torch.cuda.is_available() else None\n        },\n        {\n            \"name\": \"CPU-only\",\n            \"quantization_config\": None,\n            \"device_map\": None,\n            \"torch_dtype\": torch.float32\n        }\n    ]\n    \n    for config in configs_to_try:\n        try:\n            print(f\"\\nAttempting {config['name']} load...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                quantization_config=config.get(\"quantization_config\"),\n                trust_remote_code=trust_remote_code,\n                device_map=config[\"device_map\"],\n                torch_dtype=config.get(\"torch_dtype\", torch_dtype)\n            )\n            print(f\"\\nâœ… Model loaded successfully ({config['name']})!\")\n            print_memory()\n            return model\n        except Exception as e:\n            print(f\"âŒ {config['name']} load failed: {str(e)}\")\n            continue\n    \n    raise RuntimeError(\"All model loading attempts failed\")\n\ntry:\n    model = load_model(MODEL_NAME)\nexcept Exception as e:\n    print(f\"\\nâŒ Final model loading error: {str(e)}\")\n    print(\"Trying basic CPU load as last resort...\")\n    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cpu\", torch_dtype=torch.float32)\n    print(\"\\nâœ… Model loaded on CPU\")\n    print_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:10:16.758597Z","iopub.execute_input":"2025-05-24T00:10:16.759265Z","iopub.status.idle":"2025-05-24T00:10:20.878415Z","shell.execute_reply.started":"2025-05-24T00:10:16.759241Z","shell.execute_reply":"2025-05-24T00:10:20.877477Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 5.8% (1.4/31.4GB)\n\nAttempting 4-bit quantized load...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ded94cf4ea4fbfb9c99c84cea7a013"}},"metadata":{}},{"name":"stdout","text":"âŒ 4-bit quantized load failed: No GPU found. A GPU is needed for quantization.\n\nAttempting 8-bit quantized load...\nâŒ 8-bit quantized load failed: No GPU found. A GPU is needed for quantization.\n\nAttempting Full precision load...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0857d242844855b715be411d767b59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19bf3efd925746799f6c865c05072405"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Model loaded successfully (Full precision)!\nRAM: 7.5% (1.9/31.4GB)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup\n# =======================\n\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:10:20.879307Z","iopub.execute_input":"2025-05-24T00:10:20.879546Z","iopub.status.idle":"2025-05-24T00:10:22.224391Z","shell.execute_reply.started":"2025-05-24T00:10:20.879527Z","shell.execute_reply":"2025-05-24T00:10:22.223402Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ecc816970584f5bb9637904e8ec7bc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764e9c772bb044b290d26bbdef6035e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94db3d7a248d46f3b582129e77b2a663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7714e946114ffcb358a3451986eb57"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 4: Robust Data Preparation from CSV\n# =============================================\n\n# 0. Set critical environment variables first\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n# 1. First verify the exact file path\nimport pandas as pd\nfrom datasets import Dataset\n\n# Check the exact path to your CSV file\ndataset_path = \"/kaggle/input/database-0522/papers_database.csv\"\nif not os.path.exists(dataset_path):\n    # Try to find the correct path if the default doesn't work\n    print(\"Searching for CSV file...\")\n    # !find /kaggle/input -name \"*.csv\" 2>/dev/null\n    raise FileNotFoundError(f\"Could not find papers_database.csv at {dataset_path}\")\n\n# 2. Enhanced dataset preparation with better CSV handling\ndef prepare_dataset(file_path, max_samples=None):\n    \"\"\"Prepare dataset from CSV with robust error handling\"\"\"\n    try:\n        # First try with pandas for better CSV handling\n        df = pd.read_csv(file_path)\n        \n        # Apply max_samples if specified\n        if max_samples is not None and len(df) > max_samples:\n            df = df.sample(max_samples)\n        \n        # Standardize the text column name\n        text_col = None\n        possible_text_columns = ['text', 'content', 'paper_text', 'abstract', 'body', 'article', 'paper']\n        \n        for col in possible_text_columns:\n            if col in df.columns:\n                text_col = col\n                break\n        \n        if text_col is None:\n            # Find first string-type column\n            for col in df.columns:\n                if pd.api.types.is_string_dtype(df[col]):\n                    text_col = col\n                    break\n            else:\n                # If no string column found, convert first column to string\n                text_col = df.columns[0]\n                df[text_col] = df[text_col].astype(str)\n        \n        # Rename to 'text' if needed\n        if text_col != 'text':\n            df = df.rename(columns={text_col: 'text'})\n        \n        # Clean text data\n        df['text'] = df['text'].str.strip().replace(r'\\s+', ' ', regex=True)\n        \n        # Convert to HuggingFace Dataset\n        dataset = Dataset.from_pandas(df)\n        print(f\"âœ… Loaded dataset with {len(dataset)} samples\")\n        return dataset\n        \n    except Exception as e:\n        print(f\"\\nâŒ Dataset preparation failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\"Sample text \" + str(i) for i in range(10)]})\n\n# 3. Enhanced Tokenization function\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with multiple fallback strategies\"\"\"\n    try:\n        # First try standard tokenization\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=512,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n            \n    except Exception as e:\n        print(f\"âš ï¸ Tokenization warning: {str(e)}\")\n        # Fallback strategy\n        seq_length = min(512, tokenizer.model_max_length)\n        return {\n            \"input_ids\": [[tokenizer.pad_token_id or 0] * seq_length],\n            \"attention_mask\": [[1] * seq_length],\n            \"labels\": [[tokenizer.pad_token_id or 0] * seq_length]\n        }\n\n# 4. Main processing with comprehensive error handling\ntry:\n    print(\"\\n=== Starting Processing ===\")\n    \n    # Initialize tokenizer if not already defined\n    if 'tokenizer' not in globals():\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Default model\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load and prepare dataset\n    dataset = prepare_dataset(dataset_path)\n    \n    # Small batch test first\n    test_size = min(2, len(dataset))\n    test_batch = dataset.select(range(test_size))\n    test_tokenized = test_batch.map(safe_tokenize, batched=True)\n    print(\"\\nTest batch processed successfully!\")\n    print(f\"Sample text: {test_batch[0]['text'][:100]}...\")\n    print(f\"Tokenized length: {len(test_tokenized[0]['input_ids'])}\")\n    \n    # Full dataset processing with progress reporting\n    print(\"\\nProcessing full dataset...\")\n    tokenized_dataset = dataset.map(\n        safe_tokenize,\n        batched=True,\n        batch_size=8,\n        remove_columns=['text'],\n        writer_batch_size=1000\n    )\n    \n    # Set format for PyTorch\n    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    \n    print(\"\\nâœ… Processing completed successfully!\")\n    print(f\"Final dataset features: {tokenized_dataset.features}\")\n    print(f\"Number of samples: {len(tokenized_dataset)}\")\n    \nexcept Exception as e:\n    print(f\"\\nâŒ Critical error in processing: {str(e)}\")\n    print(\"Creating minimal fallback dataset...\")\n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": [[0] * 512],\n        \"attention_mask\": [[1] * 512],\n        \"labels\": [[0] * 512]\n    })\n    tokenized_dataset.set_format(type='torch')\n    print(\"âš ï¸ Using fallback dataset - model may not train properly\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:10:22.225718Z","iopub.execute_input":"2025-05-24T00:10:22.226003Z","iopub.status.idle":"2025-05-24T00:10:22.821937Z","shell.execute_reply.started":"2025-05-24T00:10:22.225982Z","shell.execute_reply":"2025-05-24T00:10:22.820930Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Processing ===\nâœ… Loaded dataset with 60 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d896b8b0644afb93932c6d3593a451"}},"metadata":{}},{"name":"stdout","text":"\nTest batch processed successfully!\nSample text: a009a494d6d220b47368efac02c33672...\nTokenized length: 512\n\nProcessing full dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af7be225e1e40bba60798f096c97dd5"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Processing completed successfully!\nFinal dataset features: {'title': Value(dtype='string', id=None), 'source_url': Value(dtype='string', id=None), 'local_path': Value(dtype='string', id=None), 'text_length': Value(dtype='int64', id=None), 'text_preview': Value(dtype='string', id=None), 'full_text': Value(dtype='string', id=None), 'doc': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\nNumber of samples: 60\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 5: Training Configuration\n# =============================\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\n\n# LoRA configuration\nfrom peft import LoraConfig\n\npeft_config = LoraConfig(\n    r=16,  \n    lora_alpha=32,\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # GPT-2 compatible modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    fan_in_fan_out=True\n)\n\n# Training arguments optimized for Kaggle\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/phi1.5-lora-results\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,  # Reduced for Kaggle\n    learning_rate=2e-5,\n    optim=\"adamw_torch\",\n    logging_steps=10,\n    save_steps=500,\n    fp16=torch.cuda.is_available(),\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\"\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:10:22.823126Z","iopub.execute_input":"2025-05-24T00:10:22.824288Z","iopub.status.idle":"2025-05-24T00:10:22.962870Z","shell.execute_reply.started":"2025-05-24T00:10:22.824258Z","shell.execute_reply":"2025-05-24T00:10:22.961608Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 6: Training Execution\n# =========================\n\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:10:22.964182Z","iopub.execute_input":"2025-05-24T00:10:22.964882Z","iopub.status.idle":"2025-05-24T00:15:33.354155Z","shell.execute_reply.started":"2025-05-24T00:10:22.964843Z","shell.execute_reply":"2025-05-24T00:15:33.352230Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nRAM: 7.8% (2.0/31.4GB)\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 04:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>8.229500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    AutoConfig,\n    TrainingArguments,\n    pipeline\n)\nfrom peft import PeftModel, PeftConfig\nimport os\nimport json\nimport torch\nfrom typing import Optional\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"Save all model artifacts including complete model configuration\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\nğŸ’¾ Saving model artifacts to: {output_dir}\")\n    \n    # Save the complete model (base + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True\n    )\n    \n    # Save tokenizer\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n        except Exception as e:\n            print(f\"âš ï¸ Failed to save training args: {str(e)}\")\n    \n    print(\"\\nğŸ” Saved files:\")\n    for f in sorted(os.listdir(output_dir)):\n        size = os.path.getsize(os.path.join(output_dir, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    return output_dir\n\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 160,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"Load and test a saved model with local-only loading\"\"\"\n    print(f\"\\nğŸ” Loading from: {model_path}\")\n    \n    if not os.path.exists(model_path):\n        raise ValueError(f\"Directory not found: {model_path}\")\n    \n    try:\n        # Load tokenizer first\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        if is_peft_model:\n            # Check if we have PEFT adapter files\n            has_adapter = any(f.startswith('adapter_') for f in os.listdir(model_path))\n            \n            if has_adapter:\n                print(\"Loading PEFT model...\")\n                \n                # First load the base model from local files\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    device_map=\"auto\",\n                    local_files_only=True\n                )\n                \n                # Merge for inference\n                model = model.merge_and_unload()\n            else:\n                print(\"Loading as regular model (no adapter found)\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n        else:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n        \n        print(\"ğŸ‰ Model loaded successfully!\")\n        \n        # Test prompts\n        test_prompts = test_prompts or [\n            \"What is hardware wallet?\",\n            \"Explain Proof of Work (PoW)\",\n            \"Define cryptography in simple terms\"\n        ]\n        \n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        print(\"\\nğŸ§ª Testing model:\")\n        for prompt in test_prompts:\n            print(f\"\\nPrompt: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9\n            )\n            print(\"Response:\", output[0]['generated_text'])\n        \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\nâŒ Error loading model: {str(e)}\")\n        print(\"Directory contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:22:04.553707Z","iopub.execute_input":"2025-05-24T00:22:04.554159Z","iopub.status.idle":"2025-05-24T00:22:04.571764Z","shell.execute_reply.started":"2025-05-24T00:22:04.554129Z","shell.execute_reply":"2025-05-24T00:22:04.570913Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    # First save your model\n    save_path = save_model_artifacts(model, tokenizer, training_args)\n    \n    # Then load and test\n    try:\n        loaded_model, loaded_tokenizer = load_and_test_model(save_path, is_peft_model=True)\n        \n        # Additional custom tests\n        custom_prompts = [\n            \"Compare hardware and software wallets\",\n            \"What are the advantages of PoW?\",\n            \"How does blockchain ensure security?\"\n        ]\n        load_and_test_model(\n            save_path,\n            test_prompts=custom_prompts,\n            is_peft_model=True\n        )\n    except Exception as e:\n        print(f\"Model loading failed: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:22:04.573412Z","iopub.execute_input":"2025-05-24T00:22:04.573687Z","iopub.status.idle":"2025-05-24T00:22:49.614668Z","shell.execute_reply.started":"2025-05-24T00:22:04.573665Z","shell.execute_reply":"2025-05-24T00:22:49.613528Z"}},"outputs":[{"name":"stdout","text":"\nğŸ’¾ Saving model artifacts to: /kaggle/working/gpt2-lora-trained\n\nğŸ” Saved files:\n- README.md (4.96 KB)\n- adapter_config.json (0.66 KB)\n- adapter_model.safetensors (9227.88 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\nğŸ” Loading from: /kaggle/working/gpt2-lora-trained\nLoading PEFT model...\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ‰ Model loaded successfully!\n\nğŸ§ª Testing model:\n\nPrompt: What is hardware wallet?\nResponse: What is hardware wallet?\n\nThe hardware wallet is a software wallet that is used to store your bitcoins. It allows you to securely store your bitcoins in the same way as any other digital currency. It is also used to store your bitcoins on the blockchain.\n\nHow do I buy and sell bitcoins?\n\nThe bitcoins you buy and sell are kept in your wallet. You can purchase bitcoins from any store or online store. You can also buy bitcoins by exchanging them for other currencies.\n\nHow do I get a Bitcoin address?\n\nYou can use the wallet's address to get a bitcoin address. You can do this by using the link below. You can use any Bitcoin address you want to send bitcoins to.\n\nHow do I send bitcoins to my Bitcoin address?\n\n\nPrompt: Explain Proof of Work (PoW)\nResponse: Explain Proof of Work (PoW)\n\nI am very excited to announce a new feature that allows you to make a PoW proof of work (PoW) on your site. This feature will allow you to create a Proof of Work (PoW) that contains a URL that contains a proof of work of your choice, and the URL will be visible to all visitors to your site.\n\nWe believe that the best way to make the best PoW proof of work is to use a proof of work that has already been made. This is achieved by using a proof of work that is proof of work that is not proof of work that is not proof of work that is proof of work that is proof of work that is proof of work that is proof of work that is proof\n\nPrompt: Define cryptography in simple terms\nResponse: Define cryptography in simple terms.\n\nWhen it comes to the way that a cryptographic key is handled, there is a lot of work to be done to ensure that there are no mistakes or broken keys. In particular, the implementation of a cryptographic key should be transparent, and should not require any additional code or other technical expertise.\n\nThe use of cryptographic keys as keys does not always mean that the code is completely safe, or that the code is designed to be safe. As a result, it is important to make sure that the cryptographic key is secure.\n\nA key is a cryptographic key that is used to encrypt a piece of data. A cryptographic key that is used to encrypt a piece of data is called a cryptographic key. A cryptographic key is also called a cryptographic key if\n\nğŸ” Loading from: /kaggle/working/gpt2-lora-trained\nLoading PEFT model...\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ‰ Model loaded successfully!\n\nğŸ§ª Testing model:\n\nPrompt: Compare hardware and software wallets\nResponse: Compare hardware and software wallets.\n\nIf you want to use an offline wallet, you need to have your wallet on a secure, encrypted device. This means you need to have a secure backup device to make sure you have a backup.\n\nIf you have an offline wallet, you have to use a secure backup device. This means you need to have a secure backup device to make sure you have a backup. If you have an offline wallet, you need to use a password.\n\nIf you want to use an offline wallet, you have to use a password.\n\nIf you don't have an offline wallet, you have to use a password.\n\nIf you don't have an offline wallet, you have to use a password.\n\nIf you don't have an\n\nPrompt: What are the advantages of PoW?\nResponse: What are the advantages of PoW?\n\nPoW is a simple way to create a decentralized digital ledger. The PoW protocol is a proof-of-work system that allows users to create their own private keys (called a PoW chain). The PoW chain is not a centralized database. It is the network of trust between parties that creates the blockchain. The PoW blockchain is based on a distributed consensus system, so any parties who disagree with each other are not allowed to merge.\n\nPoW is a great way to create a decentralized digital ledger. The PoW protocol is a proof-of-work system that allows users to create their own private keys (called a PoW chain). The PoW chain is not a centralized database. It is the network of trust between\n\nPrompt: How does blockchain ensure security?\nResponse: How does blockchain ensure security?\n\nBlockchain is the foundation of modern crypto-currency. It is based on the concept of \"blockchain proof of work.\" A block is a set of inputs, and the value of each of these inputs is stored on a blockchain. A block can be verified by the block's proof of work. The proof of work is based on the block's hashing power.\n\nThe basic idea behind blockchain is that it is the most secure system to be created. It is based on the principle of Proof of Work. The proof of work is based on the fact that a block can be verified by only one person or group of people. The proof of work is based on the fact that a block can be generated with only the votes of a majority of the people\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T00:22:49.615927Z","iopub.execute_input":"2025-05-24T00:22:49.616577Z","iopub.status.idle":"2025-05-24T00:22:49.621966Z","shell.execute_reply.started":"2025-05-24T00:22:49.616541Z","shell.execute_reply":"2025-05-24T00:22:49.621046Z"}},"outputs":[{"name":"stdout","text":"Total notebook execution time: 1024.67 seconds\n","output_type":"stream"}],"execution_count":13}]}