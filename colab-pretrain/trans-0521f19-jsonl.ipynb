{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:48:42.108719Z","iopub.execute_input":"2025-05-22T00:48:42.109502Z","iopub.status.idle":"2025-05-22T00:48:42.114377Z","shell.execute_reply.started":"2025-05-22T00:48:42.109468Z","shell.execute_reply":"2025-05-22T00:48:42.113293Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Cell 1: Complete Environment Setup for Kaggle - FIXED VERSION\n# ========================================================\n\n# 1. First, clean up everything - more thorough cleanup\nimport sys\nimport shutil\nimport os  # Moved this import to the top\n\n# Clean up problematic installations\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Remove problematic directories manually\nproblematic_path = \"/usr/local/lib/python3.11/dist-packages/~vidia-cudnn-cu12\"\nif os.path.exists(problematic_path):\n    shutil.rmtree(problematic_path)\n\n# Clear pip cache\n!pip cache purge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:48:42.116196Z","iopub.execute_input":"2025-05-22T00:48:42.116446Z","iopub.status.idle":"2025-05-22T00:48:46.407157Z","shell.execute_reply.started":"2025-05-22T00:48:42.116428Z","shell.execute_reply":"2025-05-22T00:48:46.406088Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1\nUninstalling torch-2.2.1:\n  Successfully uninstalled torch-2.2.1\nFound existing installation: torchvision 0.17.1+cu121\nUninstalling torchvision-0.17.1+cu121:\n  Successfully uninstalled torchvision-0.17.1+cu121\nFound existing installation: torchaudio 2.2.1+cu121\nUninstalling torchaudio-2.2.1+cu121:\n  Successfully uninstalled torchaudio-2.2.1+cu121\nFound existing installation: transformers 4.41.2\nUninstalling transformers-4.41.2:\n  Successfully uninstalled transformers-4.41.2\nFound existing installation: peft 0.10.0\nUninstalling peft-0.10.0:\n  Successfully uninstalled peft-0.10.0\nFound existing installation: bitsandbytes 0.43.0\nUninstalling bitsandbytes-0.43.0:\n  Successfully uninstalled bitsandbytes-0.43.0\nFiles removed: 90\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# 2. Install NumPy FIRST with clean environment\n!pip install -q --ignore-installed numpy==1.26.4\n\n# Force reload numpy if it was previously imported\nif 'numpy' in sys.modules:\n    del sys.modules['numpy']\n\n# 3. Now import numpy FIRST before anything else\nimport numpy as np\nprint(f\"NumPy version after clean install: {np.__version__}\")\n\n# 4. Install PyTorch with CUDA 12.1 (Kaggle's version)\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# 5. Install transformer-related packages with compatible versions\n!pip install -q transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install -q bitsandbytes==0.43.0 einops==0.7.0\n\n# 6. Handle gymnasium separately to avoid conflicts\n!pip install -q gymnasium==0.29.0 --no-deps\n\n# 7. Verify installations\nimport subprocess\nimport psutil\nimport torch\nimport torchvision\n\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")  # Should show 1.26.4\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Torchvision: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"\\n=== CUDA Information ===\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n\n# 8. Now import transformer-related packages\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    pipeline\n)\n\nprint(\"\\n=== Transformer Packages Loaded Successfully ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:48:46.408495Z","iopub.execute_input":"2025-05-22T00:48:46.408860Z","iopub.status.idle":"2025-05-22T00:50:08.795568Z","shell.execute_reply.started":"2025-05-22T00:48:46.408831Z","shell.execute_reply":"2025-05-22T00:50:08.794438Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\ndatasets 2.18.0 requires fsspec[http]<=2024.2.0,>=2023.1.0, but you have fsspec 2025.5.0 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNumPy version after clean install: 1.26.4\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.2.1+cu121\nTorchvision: 0.17.1+cu121\nCUDA available: False\n\n=== Transformer Packages Loaded Successfully ===\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Cell 2: Model Loading\n# =====================\n\n# Define MODEL_NAME at the top of the cell (should match what is used in Cell 1)\nMODEL_NAME = \"gpt2\"  # Change to \"meta-llama/Llama-2-7b-chat-hf\" for Llama\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics for the environment\"\"\"\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    \"\"\"Load model with improved error handling and phi-1.5 specific settings\"\"\"\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    # Phi-1.5 specific configuration\n    trust_remote_code = True  # Required for phi-1.5\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    # Quantization config for memory efficiency\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch_dtype\n    )\n    \n    try:\n        print(\"Attempting quantized load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=trust_remote_code,\n            device_map=\"auto\",\n            torch_dtype=torch_dtype\n        )\n        \n        print(\"\\nâœ… Model loaded successfully!\")\n        print_memory()\n        return model\n        \n    except Exception as e:\n        print(f\"\\nâŒ Model loading failed: {str(e)}\")\n        print(\"Attempting standard load without quantization...\")\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                torch_dtype=torch_dtype\n            )\n            print(\"\\nâœ… Model loaded successfully without quantization!\")\n            print_memory()\n            return model\n        except Exception as e:\n            print(f\"\\nâŒ Standard load failed: {str(e)}\")\n            print(\"Attempting CPU-only fallback...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"cpu\",\n                torch_dtype=torch.float32\n            )\n            print(\"\\nâœ… Model loaded on CPU\")\n            print_memory()\n            return model\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:50:08.798795Z","iopub.execute_input":"2025-05-22T00:50:08.799113Z","iopub.status.idle":"2025-05-22T00:50:09.504479Z","shell.execute_reply.started":"2025-05-22T00:50:08.799083Z","shell.execute_reply":"2025-05-22T00:50:09.503599Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 15.6% (4.5/31.4GB)\nAttempting quantized load...\n\nâŒ Model loading failed: No GPU found. A GPU is needed for quantization.\nAttempting standard load without quantization...\n\nâœ… Model loaded successfully without quantization!\nRAM: 16.1% (4.6/31.4GB)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup\n# =======================\n\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:50:09.505299Z","iopub.execute_input":"2025-05-22T00:50:09.505556Z","iopub.status.idle":"2025-05-22T00:50:09.759908Z","shell.execute_reply.started":"2025-05-22T00:50:09.505538Z","shell.execute_reply":"2025-05-22T00:50:09.758994Z"}},"outputs":[{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Cell 4: Robust Data Preparation - Fixed Version\n# =============================================\n\n# 0. Set critical environment variables first\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n# 1. Dataset preparation with multiple fallbacks\ndef prepare_dataset(file_path=\"/kaggle/input/database_0520.jsonl\", max_samples=1000):\n    \"\"\"Prepare dataset with robust error handling\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"âŒ Path not found: {file_path}\")\n            \n        # Load JSONL file directly using datasets library\n        dataset = load_dataset('json', data_files=file_path, split='train[:{}]'.format(max_samples))\n        \n        # If the dataset has multiple columns, we just want the text\n        if 'text' not in dataset.features:\n            # Try to find the first text-like column\n            text_columns = [col for col in dataset.features if any(t in col.lower() for t in ['text', 'content', 'body'])]\n            if text_columns:\n                dataset = dataset.rename_column(text_columns[0], 'text')\n            else:\n                # If no text column found, combine all string columns\n                string_columns = [col for col in dataset.features if dataset.features[col].dtype == 'string']\n                if string_columns:\n                    def combine_columns(examples):\n                        return {'text': ' '.join(str(examples[col]) for col in string_columns)}\n                    dataset = dataset.map(combine_columns)\n                else:\n                    raise ValueError(\"No text columns found in dataset\")\n        \n        print(f\"âœ… Loaded dataset with {len(dataset)} samples\")\n        return dataset\n        \n    except Exception as e:\n        print(f\"\\nâŒ Dataset preparation failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\"Sample text \" + str(i) for i in range(10)]})\n\n# 2. Tokenization function\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with explicit numpy workarounds\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=160,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        # Convert to lists explicitly\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except RuntimeError as e:\n        if \"Numpy is not available\" in str(e):\n            # Fallback using pure Python\n            return {\n                \"input_ids\": [[0]*512],\n                \"attention_mask\": [[1]*512],\n                \"labels\": [[0]*512]\n            }\n        raise\n\ntry:\n    print(\"\\n=== Starting Processing ===\")\n    dataset = prepare_dataset()\n    \n    # Small batch test first\n    test_batch = dataset.select(range(2))\n    test_tokenized = test_batch.map(safe_tokenize, batched=True)\n    \n    # If test succeeds, process full dataset\n    tokenized_dataset = dataset.map(safe_tokenize, batched=True, batch_size=4)\n    tokenized_dataset.set_format(type='torch')\n    \n    print(\"âœ… Processing completed successfully!\")\n    \nexcept Exception as e:\n    print(f\"\\nâŒ Error: {str(e)}\")\n    print(\"Creating minimal fallback dataset...\")\n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": [[0,1,2,3]],\n        \"attention_mask\": [[1,1,1,1]],\n        \"labels\": [[0,1,2,3]]\n    })\n    tokenized_dataset.set_format(type='torch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:50:09.760757Z","iopub.execute_input":"2025-05-22T00:50:09.760976Z","iopub.status.idle":"2025-05-22T00:50:09.896079Z","shell.execute_reply.started":"2025-05-22T00:50:09.760959Z","shell.execute_reply":"2025-05-22T00:50:09.895145Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Processing ===\n\nâŒ Dataset preparation failed: âŒ Path not found: /kaggle/input/database_0520.jsonl\nCreating minimal fallback dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7f7efd869c40af898f674f07c9b17d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39e82cb047248cfa407034d6259e99b"}},"metadata":{}},{"name":"stdout","text":"âœ… Processing completed successfully!\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Cell 5: Training Configuration\n# =============================\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\n\n# LoRA configuration\nfrom peft import LoraConfig\n\npeft_config = LoraConfig(\n    r=16,  \n    lora_alpha=32,\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # GPT-2 compatible modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    fan_in_fan_out=True\n)\n\n# Training arguments optimized for Kaggle\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/phi1.5-lora-results\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,  # Reduced for Kaggle\n    learning_rate=2e-5,\n    optim=\"adamw_torch\",\n    logging_steps=10,\n    save_steps=500,\n    fp16=torch.cuda.is_available(),\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\"\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:50:09.897098Z","iopub.execute_input":"2025-05-22T00:50:09.897362Z","iopub.status.idle":"2025-05-22T00:50:10.004834Z","shell.execute_reply.started":"2025-05-22T00:50:09.897343Z","shell.execute_reply":"2025-05-22T00:50:10.004021Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Cell 6: Training Execution\n# =========================\n\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:50:10.005731Z","iopub.execute_input":"2025-05-22T00:50:10.006063Z","iopub.status.idle":"2025-05-22T00:50:23.413001Z","shell.execute_reply.started":"2025-05-22T00:50:10.006037Z","shell.execute_reply":"2025-05-22T00:50:23.411917Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nRAM: 16.2% (4.6/31.4GB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:05, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Cell 7: Enhanced Model Saving with Shard Support - FIXED VERSION\n# ===============================================\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[object] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\nğŸ’¾ Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"ğŸ’½ Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"ğŸ”¤ Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"ğŸ“ Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"âš ï¸ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"âš ï¸ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\nâš ï¸ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\nğŸ” Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir\n\n# Cell 8: Robust Model Loading and Testing with PEFT support - FIXED VERSION\n# ========================================================\n\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 160,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\nğŸ” Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\nğŸ“‚ Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\nğŸ”„ Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\nğŸ”„ Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"âš ï¸ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\nğŸ‰ Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline\n        print(\"\\nğŸš€ Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            device=0 if torch.cuda.is_available() else -1\n        )\n        \n        # Run tests\n        print(\"\\nğŸ§ª Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\nğŸ”¹ Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"ğŸ’¬ Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\nâŒ Critical error loading model: {str(e)}\")\n        print(\"\\nğŸ› ï¸ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:55:54.929930Z","iopub.execute_input":"2025-05-22T00:55:54.930353Z","iopub.status.idle":"2025-05-22T00:55:54.951226Z","shell.execute_reply.started":"2025-05-22T00:55:54.930324Z","shell.execute_reply":"2025-05-22T00:55:54.950179Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    model_path = \"/kaggle/working/gpt2-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    load_and_test_model(model_path, is_peft_model=True)\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \"What is software wallet, and what's the difference between hardware and software wallet? \",\n        \"What is PoW? \",\n        \"Explain PoW in 1 sentence. \",\n        \"Describe the key features of PoW using 3 words. \",\n        \"What is PoM? Is it something related to cryptography? \",\n        \"What is a cryptographic product? \",\n        \"What is P2P? \",\n        \"What is block chain? \",\n        \"What is public key, and what's the difference between private and public key? \"\n    ]\n    load_and_test_model(model_path, test_prompts=custom_prompts, is_peft_model=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:55:54.952664Z","iopub.execute_input":"2025-05-22T00:55:54.952960Z","iopub.status.idle":"2025-05-22T00:55:56.137339Z","shell.execute_reply.started":"2025-05-22T00:55:54.952940Z","shell.execute_reply":"2025-05-22T00:55:56.136018Z"}},"outputs":[{"name":"stdout","text":"\nğŸ’¾ Saving model artifacts to: /kaggle/working/gpt2-lora-trained\nğŸ’½ Saving model and adapter...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ğŸ”¤ Saving tokenizer...\nğŸ“ Saving training arguments...\n\nğŸ” Verifying saved files:\n- README.md (4.96 KB)\n- training_args.json (3.86 KB)\n- merges.txt (445.62 KB)\n- tokenizer_config.json (0.49 KB)\n- adapter_config.json (0.66 KB)\n- config.json (0.89 KB)\n- model.safetensors (486107.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- adapter_model.safetensors (0.04 KB)\n- vocab.json (779.45 KB)\n- generation_config.json (0.12 KB)\n\nğŸ” Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\nğŸ“‚ Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.66 KB)\n- adapter_model.safetensors (0.04 KB)\n- config.json (0.89 KB)\n- generation_config.json (0.12 KB)\n- merges.txt (445.62 KB)\n- model.safetensors (486107.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\nğŸ”„ Loading tokenizer...\n\nğŸ”„ Loading model...\nFound adapter files: ['adapter_config.json', 'adapter_model.safetensors']\n\nâŒ Critical error loading model: name 'PeftModel' is not defined\n\nğŸ› ï¸ Debugging info:\n- Path: /kaggle/working/gpt2-lora-trained\n- Directory exists: True\n- Contents: ['README.md', 'training_args.json', 'merges.txt', 'tokenizer_config.json', 'adapter_config.json', 'config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'adapter_model.safetensors', 'vocab.json', 'generation_config.json']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/81389042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Load with explicit path and PEFT flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mload_and_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Test with custom prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1808902522.py\u001b[0m in \u001b[0;36mload_and_test_model\u001b[0;34m(model_path, max_length, test_prompts, is_peft_model)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# Then load the PEFT adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 model = PeftModel.from_pretrained(\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'PeftModel' is not defined"],"ename":"NameError","evalue":"name 'PeftModel' is not defined","output_type":"error"}],"execution_count":48},{"cell_type":"code","source":"\nnotebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:55:56.137811Z","iopub.status.idle":"2025-05-22T00:55:56.138071Z","shell.execute_reply.started":"2025-05-22T00:55:56.137949Z","shell.execute_reply":"2025-05-22T00:55:56.137960Z"}},"outputs":[],"execution_count":null}]}