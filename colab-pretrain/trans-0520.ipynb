{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install required packages\n!pip install torch transformers sentence-transformers psycopg2-binary pgvector tqdm scikit-learn\n!sudo apt-get update && apt-get install -y postgresql postgresql-contrib postgresql-14-pgvector\n!service postgresql start\n!sudo -u postgres psql -c \"CREATE EXTENSION IF NOT EXISTS vector;\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:41:09.238146Z","iopub.execute_input":"2025-05-20T20:41:09.239553Z","iopub.status.idle":"2025-05-20T20:41:21.695638Z","shell.execute_reply.started":"2025-05-20T20:41:09.239432Z","shell.execute_reply":"2025-05-20T20:41:21.694210Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: psycopg2-binary in /usr/local/lib/python3.11/dist-packages (2.9.10)\nRequirement already satisfied: pgvector in /usr/local/lib/python3.11/dist-packages (0.4.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n0% [Working]","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\nHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\nHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \nHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease               \nHit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease               \nHit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \nHit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\nHit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nHit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nE: Unable to locate package postgresql-14-pgvector\n * Starting PostgreSQL 14 database server\n   ...done.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"ERROR:  could not open extension control file \"/usr/share/postgresql/14/extension/vector.control\": No such file or directory\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 2: Configure environment\nimport os\nos.environ[\"POSTGRES_URL\"] = \"postgres://postgres@localhost/postgres\"\n\n# Cell 3: Import packages\nimport numpy as np\nimport sqlite3\nimport logging\nimport hashlib\nimport psycopg2\nimport re\nimport pandas as pd\nimport torch\nfrom typing import List, Dict\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pgvector.psycopg2 import register_vector\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.info(\"All packages imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:41:21.698337Z","iopub.execute_input":"2025-05-20T20:41:21.698768Z","iopub.status.idle":"2025-05-20T20:41:21.710153Z","shell.execute_reply.started":"2025-05-20T20:41:21.698706Z","shell.execute_reply":"2025-05-20T20:41:21.708747Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\nimport logging\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n# Then your regular imports:\nimport torch\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:41:21.711322Z","iopub.execute_input":"2025-05-20T20:41:21.711658Z","iopub.status.idle":"2025-05-20T20:41:21.742219Z","shell.execute_reply.started":"2025-05-20T20:41:21.711628Z","shell.execute_reply":"2025-05-20T20:41:21.740660Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 4: Database connection verification\ntry:\n    pg_conn = psycopg2.connect(os.environ[\"POSTGRES_URL\"])\n    register_vector(pg_conn)\n    pg_conn.close()\n    logger.info(\"PostgreSQL connection successful!\")\nexcept Exception as e:\n    logger.error(f\"PostgreSQL connection failed: {str(e)}\")\n\nclass QnASystem:\n    def __init__(self, db_path: str = \"qna.db\", model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n        self.db_path = db_path\n        self.model_name = model_name\n        self.conn = None\n        self.model = None\n        self.tokenizer = None\n        self.embedding_dim = 768\n        self._initialize_components()\n\n    def _initialize_components(self):\n        \"\"\"Initialize database and ML components\"\"\"\n        try:\n            # Configure SQLite for bulk operations\n            self.conn = sqlite3.connect(self.db_path)\n            self.conn.execute(\"PRAGMA journal_mode = WAL\")\n            self.conn.execute(\"PRAGMA cache_size = -10000\")  # 10MB cache\n            self._initialize_db()\n            \n            # Initialize ML model\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            self.model = AutoModel.from_pretrained(self.model_name)\n            if torch.cuda.is_available():\n                self.model = self.model.to('cuda')\n            logger.info(\"System initialized successfully\")\n        except Exception as e:\n            logger.error(f\"Initialization failed: {str(e)}\")\n            raise\n\n    # Modified _initialize_db method\n    def _initialize_db(self):\n        \"\"\"Create optimized database schema with idempotent checks\"\"\"\n        try:\n            with self.conn:\n                # Create tables\n                self.conn.executescript(\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_pairs (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        question TEXT NOT NULL CHECK(length(question) BETWEEN 10 AND 1000),\n                        answer TEXT NOT NULL CHECK(length(answer) BETWEEN 20 AND 5000),\n                        category TEXT CHECK(length(category) <= 50),\n                        keywords TEXT CHECK(length(keywords) <= 200),\n                        question_hash TEXT UNIQUE,\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        last_accessed TIMESTAMP,\n                        usage_count INTEGER DEFAULT 0\n                    );\n    \n                    CREATE TABLE IF NOT EXISTS qna_embeddings (\n                        qna_id INTEGER PRIMARY KEY,\n                        question_vector BLOB NOT NULL,\n                        answer_vector BLOB NOT NULL,\n                        FOREIGN KEY (qna_id) REFERENCES qna_pairs(id)\n                    );\n                \"\"\")\n                \n                # Create indexes with existence checks\n                self._create_index_if_not_exists(\n                    \"idx_qna_search\", \n                    \"qna_pairs(question_hash, category)\"\n                )\n                self._create_index_if_not_exists(\n                    \"idx_embedding_search\", \n                    \"qna_embeddings(qna_id)\"\n                )\n                \n            logger.info(\"Database initialized\")\n        except sqlite3.Error as e:\n            logger.error(f\"Database error: {str(e)}\")\n            raise\n    \n    def _create_index_if_not_exists(self, index_name: str, columns: str):\n        \"\"\"Helper function for idempotent index creation\"\"\"\n        try:\n            self.conn.execute(f\"\"\"\n                CREATE INDEX {index_name} \n                ON {columns}\n            \"\"\")\n        except sqlite3.OperationalError as e:\n            if \"already exists\" in str(e):\n                logger.debug(f\"Index {index_name} already exists\")\n            else:\n                raise\n\n    def _text_to_vector(self, text: str) -> np.ndarray:\n        \"\"\"Generate embeddings with GPU optimization\"\"\"\n        try:\n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n            inputs = self.tokenizer(\n                text,\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=512,\n                padding=True\n            ).to(device)\n            \n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            \n            # Efficient mean pooling\n            return outputs.last_hidden_state.mean(dim=1).cpu().numpy().squeeze()\n        except Exception as e:\n            logger.error(f\"Embedding error: {str(e)}\")\n            raise\n\n    def validate_qna(self, qna: Dict) -> bool:\n        \"\"\"Validate QnA entry quality\"\"\"\n        return all([\n            len(qna['question']) >= 10,\n            len(qna['answer']) >= 20,\n            'category' in qna,\n            cosine_similarity(\n                [self._text_to_vector(qna['question'])],\n                [self._text_to_vector(qna['answer'])]\n            )[0][0] < 0.7\n        ])\n\n    # Modified batch_insert method\n    def batch_insert(self, qna_list: List[Dict], batch_size: int = 500):\n        \"\"\"Optimized bulk insert with duplicate handling\"\"\"\n        try:\n            valid_data = [q for q in qna_list if self.validate_qna(q)]\n            logger.info(f\"Processing {len(valid_data)} valid entries\")\n            \n            with ThreadPoolExecutor(max_workers=4) as executor:\n                for batch in self._chunk_list(valid_data, batch_size):\n                    with self.conn:\n                        # Generate hashes first\n                        hashes = [\n                            hashlib.sha256(q['question'].encode()).hexdigest()\n                            for q in batch\n                        ]\n                        \n                        # Check existing hashes\n                        cursor = self.conn.execute(f\"\"\"\n                            SELECT question_hash FROM qna_pairs\n                            WHERE question_hash IN ({','.join(['?']*len(hashes))})\n                        \"\"\", hashes)\n                        existing_hashes = {row[0] for row in cursor.fetchall()}\n                        \n                        # Filter out duplicates\n                        filtered_batch = [\n                            q for q, h in zip(batch, hashes)\n                            if h not in existing_hashes\n                        ]\n                        \n                        # Insert only new QnA pairs\n                        qna_values = [(\n                            q['question'],\n                            q['answer'],\n                            q['category'],\n                            q.get('keywords', ''),\n                            hashlib.sha256(q['question'].encode()).hexdigest()\n                        ) for q in filtered_batch]\n                        \n                        self.conn.executemany(\"\"\"\n                            INSERT OR IGNORE INTO qna_pairs \n                            (question, answer, category, keywords, question_hash)\n                            VALUES (?, ?, ?, ?, ?)\n                        \"\"\", qna_values)\n    \n                        # Get IDs of newly inserted items\n                        cursor = self.conn.execute(f\"\"\"\n                            SELECT id, question_hash FROM qna_pairs\n                            WHERE question_hash IN ({','.join(['?']*len(hashes))})\n                        \"\"\", hashes)\n                        id_mapping = {row[1]: row[0] for row in cursor.fetchall()}\n                        \n                        # Generate embeddings only for new entries\n                        embeddings = []\n                        for qna in filtered_batch:\n                            qna_id = id_mapping.get(\n                                hashlib.sha256(qna['question'].encode()).hexdigest()\n                            )\n                            if qna_id:\n                                embeddings.append((\n                                    qna_id,\n                                    self._text_to_vector(qna['question']).tobytes(),\n                                    self._text_to_vector(qna['answer']).tobytes()\n                                ))\n                        \n                        # Insert embeddings\n                        if embeddings:\n                            self.conn.executemany(\"\"\"\n                                INSERT OR REPLACE INTO qna_embeddings \n                                (qna_id, question_vector, answer_vector)\n                                VALUES (?, ?, ?)\n                            \"\"\", embeddings)\n                            \n            logger.info(f\"Successfully processed {len(valid_data)} records\")\n        except Exception as e:\n            logger.error(f\"Batch insert failed: {str(e)}\")\n            self.conn.rollback()\n            raise\n\n    def _process_embeddings(self, qna: Dict, qna_id: int):\n        \"\"\"Helper for parallel embedding processing\"\"\"\n        return (\n            qna_id,\n            self._text_to_vector(qna['question']).tobytes(),\n            self._text_to_vector(qna['answer']).tobytes()\n        )\n\n    def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"Hybrid semantic search\"\"\"\n        try:\n            query_embedding = self._text_to_vector(query)\n            \n            # First-stage: ANN search\n            cursor = self.conn.execute(\"\"\"\n                SELECT qna_id, question_vector \n                FROM qna_embeddings\n                ORDER BY vector_cosine_similarity(question_vector, ?)\n                LIMIT 100\n            \"\"\", (np.array(query_embedding).tobytes(),))\n            \n            # Second-stage: reranking\n            results = []\n            for qna_id, q_vec in cursor.fetchall():\n                q_sim = cosine_similarity(\n                    [query_embedding], \n                    [np.frombuffer(q_vec)]\n                )[0][0]\n                results.append((qna_id, q_sim))\n            \n            top_ids = [x[0] for x in sorted(results, key=lambda x: x[1], reverse=True)[:top_k]]\n            return self._get_qna_by_ids(top_ids)\n        except Exception as e:\n            logger.error(f\"Search failed: {str(e)}\")\n            return []\n\n    def _get_qna_by_ids(self, ids: List[int]) -> List[Dict]:\n        \"\"\"Retrieve full records by IDs\"\"\"\n        try:\n            cursor = self.conn.execute(f\"\"\"\n                SELECT * FROM qna_pairs \n                WHERE id IN ({','.join(['?']*len(ids))})\n            \"\"\", ids)\n            columns = [col[0] for col in cursor.description]\n            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n        except sqlite3.Error as e:\n            logger.error(f\"Record retrieval failed: {str(e)}\")\n            return []\n\n    def migrate_to_postgres(self):\n        \"\"\"Partitioned migration to PostgreSQL\"\"\"\n        try:\n            pg_conn = psycopg2.connect(os.environ[\"POSTGRES_URL\"])\n            register_vector(pg_conn)\n            \n            with pg_conn.cursor() as cursor:\n                # Create partitioned table\n                cursor.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_pairs (\n                        id INT GENERATED ALWAYS AS IDENTITY,\n                        question TEXT NOT NULL,\n                        answer TEXT NOT NULL,\n                        category TEXT,\n                        keywords TEXT,\n                        question_hash TEXT UNIQUE,\n                        created_at TIMESTAMP,\n                        last_accessed TIMESTAMP,\n                        usage_count INT,\n                        PRIMARY KEY (id, category)\n                    ) PARTITION BY LIST (category);\n                \"\"\")\n                \n                # Create partitions\n                categories = self.conn.execute(\"\"\"\n                    SELECT DISTINCT category FROM qna_pairs\n                \"\"\").fetchall()\n                \n                for category in categories:\n                    cursor.execute(f\"\"\"\n                        CREATE TABLE IF NOT EXISTS qna_{category[0].lower()}\n                        PARTITION OF qna_pairs\n                        FOR VALUES IN ('{category[0]}');\n                    \"\"\")\n                \n                # Migrate data\n                data = self.conn.execute(\"\"\"\n                    SELECT question, answer, category, keywords, question_hash,\n                           created_at, last_accessed, usage_count\n                    FROM qna_pairs\n                \"\"\").fetchall()\n                \n                cursor.executemany(\"\"\"\n                    INSERT INTO qna_pairs \n                    (question, answer, category, keywords, question_hash,\n                     created_at, last_accessed, usage_count)\n                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n                \"\"\", data)\n                \n                # Create HNSW index\n                cursor.execute(\"\"\"\n                    CREATE INDEX ON qna_embeddings \n                    USING hnsw (question_vector vector_cosine_ops);\n                \"\"\")\n                \n            pg_conn.commit()\n            logger.info(\"Migration to PostgreSQL completed\")\n        except Exception as e:\n            logger.error(f\"Migration failed: {str(e)}\")\n            pg_conn.rollback()\n            raise\n\n    def _chunk_list(self, lst: List, n: int):\n        \"\"\"Utility for batch processing\"\"\"\n        for i in range(0, len(lst), n):\n            yield lst[i:i + n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:41:21.745091Z","iopub.execute_input":"2025-05-20T20:41:21.748031Z","iopub.status.idle":"2025-05-20T20:41:21.809962Z","shell.execute_reply.started":"2025-05-20T20:41:21.747951Z","shell.execute_reply":"2025-05-20T20:41:21.808870Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Cell 5: Sample QnA Data\nqna_data = [\n    {\n        \"question\": \"What is a blockchain?\",\n        \"answer\": \"Decentralized digital ledger recording transactions across networked computers. Features: Immutability, transparency, security through cryptography.\",\n        \"category\": \"Blockchain Basics\",\n        \"keywords\": \"distributed ledger, cryptography, decentralization\"\n    },\n    {\n        \"question\": \"What is proof-of-stake?\",\n        \"answer\": \"Consensus mechanism where validators stake crypto to verify transactions. Benefits: Energy efficiency vs PoW. Risks: Centralization through pooling.\",\n        \"category\": \"Consensus\",\n        \"keywords\": \"staking, validation, energy efficiency\"\n    }\n]\n\n# Cell 6: System Initialization\nqna_system = QnASystem()\nqna_system.batch_insert(qna_data)\n\n# Cell 7: Test Search\ntry:\n    test_query = \"energy efficient consensus mechanism\"\n    results = qna_system.semantic_search(test_query)\n    logger.info(\"Top result for test query:\")\n    logger.info(results[0]['answer'] if results else \"No results found\")\nexcept Exception as e:\n    logger.error(f\"Test failed: {str(e)}\")\n\n# Cell 8: PostgreSQL Migration (Optional)\n# qna_system.migrate_to_postgres()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:41:21.811077Z","iopub.execute_input":"2025-05-20T20:41:21.811377Z","iopub.status.idle":"2025-05-20T20:41:22.731278Z","shell.execute_reply.started":"2025-05-20T20:41:21.811354Z","shell.execute_reply":"2025-05-20T20:41:22.729841Z"}},"outputs":[],"execution_count":9}]}