{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install required packages\n!pip install torch transformers sentence-transformers psycopg2-binary pgvector tqdm scikit-learn\n!apt-get update && apt-get install -y postgresql postgresql-contrib\n!service postgresql start\n!sudo -u postgres psql -c \"CREATE EXTENSION IF NOT EXISTS vector;\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:43:35.667337Z","iopub.execute_input":"2025-05-20T19:43:35.667705Z","iopub.status.idle":"2025-05-20T19:43:49.693354Z","shell.execute_reply.started":"2025-05-20T19:43:35.667679Z","shell.execute_reply":"2025-05-20T19:43:49.691872Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: psycopg2-binary in /usr/local/lib/python3.11/dist-packages (2.9.10)\nRequirement already satisfied: pgvector in /usr/local/lib/python3.11/dist-packages (0.4.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n0% [Working]","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\nHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease                          \nHit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease                                    \nHit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                                          \nHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                                              \nHit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nHit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\nHit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nHit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nHit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\npostgresql is already the newest version (14+238).\npostgresql-contrib is already the newest version (14+238).\n0 upgraded, 0 newly installed, 0 to remove and 144 not upgraded.\n * Starting PostgreSQL 14 database server\n   ...done.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"ERROR:  could not open extension control file \"/usr/share/postgresql/14/extension/vector.control\": No such file or directory\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 2: Configure environment\nimport os\nos.environ[\"POSTGRES_URL\"] = \"postgres://postgres@localhost/postgres\"\n\n# Cell 3: Import packages\nimport numpy as np\nimport sqlite3\nimport logging\nimport hashlib\nimport psycopg2\nimport time\nimport pandas as pd\nimport torch\nfrom typing import List, Dict, Optional\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pgvector.psycopg2 import register_vector\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nprint(\"All packages imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:43:49.695725Z","iopub.execute_input":"2025-05-20T19:43:49.696336Z","iopub.status.idle":"2025-05-20T19:43:49.706471Z","shell.execute_reply.started":"2025-05-20T19:43:49.696297Z","shell.execute_reply":"2025-05-20T19:43:49.704929Z"}},"outputs":[{"name":"stdout","text":"All packages imported successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 4: Verify PostgreSQL connection\ntry:\n    pg_conn = psycopg2.connect(os.environ[\"POSTGRES_URL\"])\n    register_vector(pg_conn)\n    pg_conn.close()\n    logger.info(\"PostgreSQL connection successful!\")\nexcept Exception as e:\n    logger.error(f\"PostgreSQL connection failed: {str(e)}\")\n\nclass QnASystem:\n    def __init__(self, db_path: str = \"qna.db\", model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n        self.db_path = db_path\n        self.model_name = model_name\n        self.conn = None\n        self.model = None\n        self.tokenizer = None\n        self.embedding_dim = 768\n        self._initialize_components()\n\n    def _initialize_components(self):\n        \"\"\"Initialize database and ML components with error handling\"\"\"\n        try:\n            self.conn = sqlite3.connect(self.db_path)\n            self._initialize_db()\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            self.model = AutoModel.from_pretrained(self.model_name)\n            if torch.cuda.is_available():\n                self.model = self.model.to('cuda')\n            logger.info(\"System initialized successfully\")\n        except Exception as e:\n            logger.error(f\"Initialization failed: {str(e)}\")\n            raise\n\n    def _initialize_db(self):\n        \"\"\"Create optimized database schema with transaction support\"\"\"\n        try:\n            with self.conn:\n                # Create tables\n                self.conn.executescript(\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_pairs (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        question TEXT NOT NULL,\n                        answer TEXT NOT NULL,\n                        category TEXT,\n                        keywords TEXT,\n                        question_hash TEXT UNIQUE,\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        last_accessed TIMESTAMP,\n                        usage_count INTEGER DEFAULT 0\n                    );\n                \n                    CREATE TABLE IF NOT EXISTS qna_embeddings (\n                        qna_id INTEGER PRIMARY KEY,\n                        question_vector BLOB,\n                        answer_vector BLOB,\n                        keywords_vector BLOB,\n                        FOREIGN KEY (qna_id) REFERENCES qna_pairs(id)\n                    );\"\"\")\n                \n            logger.info(\"Database initialized successfully\")\n        except sqlite3.Error as e:\n            logger.error(f\"Database initialization error: {str(e)}\")\n            raise\n\n    def _text_to_vector(self, text: str) -> np.ndarray:\n        \"\"\"Generate embeddings with GPU support and fallback\"\"\"\n        try:\n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n            self.model = self.model.to(device)\n            \n            inputs = self.tokenizer(\n                text,\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=512,\n                padding=True\n            ).to(device)\n            \n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            \n            # Contextual pooling with attention weights\n            last_hidden_state = outputs.last_hidden_state\n            attention_mask = inputs.attention_mask.unsqueeze(-1)\n            vector = (last_hidden_state * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n            return vector.cpu().numpy().squeeze()\n        except Exception as e:\n            logger.error(f\"Embedding generation failed: {str(e)}\")\n            raise\n\n    def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"Hybrid search with semantic and keyword components\"\"\"\n        try:\n            query_embedding = self._text_to_vector(query)\n            keyword_matches = self.keyword_search(query, limit=top_k*3)\n            if not keyword_matches:\n                return []\n\n            ids = [str(qid) for qid, _, _ in keyword_matches]\n            cursor = self.conn.execute(f\"\"\"\n                SELECT qna_id, question_vector, answer_vector \n                FROM qna_embeddings \n                WHERE qna_id IN ({','.join(['?']*len(ids))})\n            \"\"\", ids)\n            \n            results = []\n            for qna_id, q_vec, a_vec in cursor.fetchall():\n                q_sim = cosine_similarity([query_embedding], [np.frombuffer(q_vec)])[0][0]\n                a_sim = cosine_similarity([query_embedding], [np.frombuffer(a_vec)])[0][0]\n                combined_score = 0.6*q_sim + 0.4*a_sim\n                results.append((qna_id, combined_score))\n            \n            top_ids = [x[0] for x in sorted(results, key=lambda x: x[1], reverse=True)[:top_k]]\n            return self.get_qna_by_ids(top_ids)\n        except Exception as e:\n            logger.error(f\"Search failed: {str(e)}\")\n            return []\n\n    def keyword_search(self, query: str, limit: int = 15) -> List[tuple]:\n        \"\"\"Keyword-based search using FTS5\"\"\"\n        try:\n            cursor = self.conn.execute(\"\"\"\n                SELECT rowid, question, answer \n                FROM qna_search \n                WHERE question MATCH ? \n                ORDER BY bm25(qna_search) \n                LIMIT ?\n            \"\"\", (query, limit))\n            return cursor.fetchall()\n        except sqlite3.Error as e:\n            logger.error(f\"Keyword search failed: {str(e)}\")\n            return []\n\n    def get_qna_by_ids(self, ids: List[int]) -> List[Dict]:\n        \"\"\"Retrieve full QnA records by IDs\"\"\"\n        try:\n            cursor = self.conn.execute(f\"\"\"\n                SELECT * FROM qna_pairs \n                WHERE id IN ({','.join(['?']*len(ids))})\n            \"\"\", ids)\n            columns = [col[0] for col in cursor.description]\n            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n        except sqlite3.Error as e:\n            logger.error(f\"Get QnA by IDs failed: {str(e)}\")\n            return []\n\n    def batch_insert(self, qna_list: List[Dict], batch_size: int = 100):\n        \"\"\"Optimized batch processing with transactions\"\"\"\n        try:\n            for batch in tqdm(self._chunk_list(qna_list, batch_size), desc=\"Processing batches\"):\n                with self.conn:\n                    # Insert into qna_pairs\n                    qna_values = [\n                        (\n                            q['question'],\n                            q['answer'],\n                            q.get('category'),\n                            len(q['answer'].split()),\n                            q.get('keywords', ''),\n                            self._normalize_text(q['question']),\n                            self._generate_hash(q['question'])\n                        ) for q in batch\n                    ]\n                    self.conn.executemany(\"\"\"\n                        INSERT INTO qna_pairs \n                        (question, answer, category, word_count, keywords, normalized_question, question_hash)\n                        VALUES (?, ?, ?, ?, ?, ?, ?)\n                    \"\"\", qna_values)\n\n                    # Insert into qna_search\n                    search_values = [\n                        (q['question'], q['answer'], q.get('keywords', '')) \n                        for q in batch\n                    ]\n                    self.conn.executemany(\"\"\"\n                        INSERT INTO qna_search \n                        (question, answer, keywords)\n                        VALUES (?, ?, ?)\n                    \"\"\", search_values)\n\n                    # Get inserted IDs\n                    cursor = self.conn.execute(\"\"\"\n                        SELECT id FROM qna_pairs \n                        ORDER BY id DESC LIMIT ?\n                    \"\"\", (len(batch),))\n                    inserted_ids = [row[0] for row in cursor.fetchall()][::-1]\n\n                    # Generate embeddings\n                    embeddings = []\n                    for qna_id, qna in zip(inserted_ids, batch):\n                        q_vec = self._text_to_vector(qna['question']).tobytes()\n                        a_vec = self._text_to_vector(qna['answer']).tobytes()\n                        k_vec = self._text_to_vector(qna.get('keywords', '')).tobytes()\n                        embeddings.append((qna_id, q_vec, a_vec, k_vec))\n\n                    self.conn.executemany(\"\"\"\n                        INSERT INTO qna_embeddings \n                        (qna_id, question_vector, answer_vector, keywords_vector)\n                        VALUES (?, ?, ?, ?)\n                    \"\"\", embeddings)\n        except Exception as e:\n            logger.error(f\"Batch insert failed: {str(e)}\")\n            self.conn.rollback()\n            raise\n\n    def migrate_to_postgres(self):\n        \"\"\"Database migration with full data transfer\"\"\"\n        try:\n            pg_conn = psycopg2.connect(os.environ[\"POSTGRES_URL\"])\n            register_vector(pg_conn)\n            \n            with pg_conn.cursor() as cursor, self.conn:\n                # Create PostgreSQL schema\n                cursor.execute(f\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_pairs (\n                        id INTEGER PRIMARY KEY,\n                        question TEXT NOT NULL,\n                        answer TEXT NOT NULL,\n                        category TEXT,\n                        word_count INTEGER,\n                        created_at TIMESTAMP,\n                        last_accessed TIMESTAMP,\n                        usage_count INTEGER,\n                        keywords TEXT,\n                        normalized_question TEXT,\n                        question_hash TEXT UNIQUE\n                    )\"\"\")\n                \n                cursor.execute(f\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_embeddings (\n                        qna_id INTEGER PRIMARY KEY,\n                        question_vector VECTOR({self.embedding_dim}),\n                        answer_vector VECTOR({self.embedding_dim}),\n                        keywords_vector VECTOR({self.embedding_dim})\n                    )\"\"\")\n\n                # Migrate qna_pairs\n                sqlite_data = self.conn.execute(\"\"\"\n                    SELECT id, question, answer, category, word_count, created_at,\n                           last_accessed, usage_count, keywords, normalized_question, question_hash\n                    FROM qna_pairs\n                \"\"\").fetchall()\n                \n                cursor.executemany(\"\"\"\n                    INSERT INTO qna_pairs \n                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n                \"\"\", sqlite_data)\n\n                # Migrate embeddings with vector conversion\n                embedding_data = self.conn.execute(\"\"\"\n                    SELECT qna_id, question_vector, answer_vector, keywords_vector \n                    FROM qna_embeddings\n                \"\"\").fetchall()\n                \n                converted_embeddings = []\n                for row in embedding_data:\n                    converted = (\n                        row[0],\n                        np.frombuffer(row[1]).tolist(),\n                        np.frombuffer(row[2]).tolist(),\n                        np.frombuffer(row[3]).tolist()\n                    )\n                    converted_embeddings.append(converted)\n                \n                cursor.executemany(\"\"\"\n                    INSERT INTO qna_embeddings \n                    VALUES (%s, %s, %s, %s)\n                \"\"\", converted_embeddings)\n\n                pg_conn.commit()\n            logger.info(\"Migration completed successfully\")\n        except Exception as e:\n            logger.error(f\"Migration failed: {str(e)}\")\n            if 'pg_conn' in locals():\n                pg_conn.rollback()\n            raise\n\n    # Helper methods\n    def _chunk_list(self, lst: List, n: int):\n        for i in range(0, len(lst), n):\n            yield lst[i:i + n]\n\n    def _normalize_text(self, text: str) -> str:\n        return text.lower().strip()\n\n    def _generate_hash(self, text: str) -> str:\n        return hashlib.sha256(text.encode()).hexdigest()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:43:49.708429Z","iopub.execute_input":"2025-05-20T19:43:49.708869Z","iopub.status.idle":"2025-05-20T19:43:49.772960Z","shell.execute_reply.started":"2025-05-20T19:43:49.708836Z","shell.execute_reply":"2025-05-20T19:43:49.771738Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"qna_data = [\n    {\n        \"question\": \"What is a Merkle Tree?\",\n        \"answer\": \"A Merkle tree is a binary tree with hash pointers that allows efficient and secure verification of large data structures in blockchain systems.\",\n        \"category\": \"Blockchain Fundamentals\",\n        \"keywords\": \"data structure, hash function, verification\"\n    },\n    {\n        \"question\": \"What does the output refer to in Bitcoin transactions?\",\n        \"answer\": \"The output refers to the destination address used in the Bitcoin transaction, specifying where funds are being sent.\",\n        \"category\": \"Bitcoin Transactions\",\n        \"keywords\": \"UTXO, scripting, receiver address\"\n    },\n    {\n        \"question\": \"What is confidentiality in cryptography?\",\n        \"answer\": \"Confidentiality means that transmitted messages are only received and readable by authorized parties through encryption.\",\n        \"category\": \"Cryptography\",\n        \"keywords\": \"encryption, data privacy, secure communication\"\n    },\n    {\n        \"question\": \"What is authentication in blockchain?\",\n        \"answer\": \"Authentication is the verification of the sender's identity at the receiver end using cryptographic signatures.\",\n        \"category\": \"Network Security\",\n        \"keywords\": \"digital signatures, identity verification\"\n    },\n    {\n        \"question\": \"What is the Genesis Block?\",\n        \"answer\": \"The Genesis Block is the first block in a blockchain, serving as the foundation of the entire chain structure.\",\n        \"category\": \"Blockchain Basics\",\n        \"keywords\": \"block height 0, initial block\"\n    },\n    {\n        \"question\": \"What are smart contracts?\",\n        \"answer\": \"Self-executing contracts with terms directly written into code that automatically execute when conditions are met.\",\n        \"category\": \"Smart Contracts\",\n        \"keywords\": \"automation, if-then logic, decentralized\"\n    },\n    {\n        \"question\": \"What is proof-of-work?\",\n        \"answer\": \"A consensus mechanism where miners solve cryptographic puzzles to validate transactions and create new blocks.\",\n        \"category\": \"Consensus Mechanisms\",\n        \"keywords\": \"mining, computational power, security\"\n    },\n    {\n        \"question\": \"What is cold storage?\",\n        \"answer\": \"A security method keeping cryptocurrency wallets offline to protect from internet-based attacks.\",\n        \"category\": \"Wallet Security\",\n        \"keywords\": \"offline storage, hardware wallets\"\n    },\n    {\n        \"question\": \"What is Bitcoin mining?\",\n        \"answer\": \"The process of validating transactions and securing the network through computational work rewarded with new BTC.\",\n        \"category\": \"Mining\",\n        \"keywords\": \"block reward, hashing, ASICs\"\n    },\n    {\n        \"question\": \"What is a hard fork?\",\n        \"answer\": \"A permanent divergence in blockchain creating two separate networks with shared history but different protocols.\",\n        \"category\": \"Blockchain Governance\",\n        \"keywords\": \"protocol change, chain split\"\n    },\n    {\n        \"question\": \"What is a 51% attack?\",\n        \"answer\": \"When a single entity controls majority of network hashing power, enabling transaction manipulation.\",\n        \"category\": \"Network Security\",\n        \"keywords\": \"double-spend, consensus attack\"\n    },\n    {\n        \"question\": \"What is SHA-256?\",\n        \"answer\": \"The cryptographic hash function used in Bitcoin mining and blockchain security protocols.\",\n        \"category\": \"Cryptography\",\n        \"keywords\": \"hashing algorithm, mining\"\n    },\n    {\n        \"question\": \"What is a decentralized exchange (DEX)?\",\n        \"answer\": \"Peer-to-peer trading platform operating without central authority using smart contracts.\",\n        \"category\": \"Trading\",\n        \"keywords\": \"P2P, non-custodial\"\n    },\n    {\n        \"question\": \"What is yield farming?\",\n        \"answer\": \"Generating returns by providing liquidity to DeFi protocols through token staking.\",\n        \"category\": \"DeFi\",\n        \"keywords\": \"liquidity pools, APY\"\n    },\n    {\n        \"question\": \"What is the Lightning Network?\",\n        \"answer\": \"Layer-2 solution enabling fast, low-cost Bitcoin transactions through payment channels.\",\n        \"category\": \"Scaling Solutions\",\n        \"keywords\": \"micropayments, off-chain\"\n    },\n    {\n        \"question\": \"What is an NFT?\",\n        \"answer\": \"Non-fungible token representing unique digital ownership on blockchain networks.\",\n        \"category\": \"Digital Assets\",\n        \"keywords\": \"collectibles, digital art\"\n    },\n    {\n        \"question\": \"What is staking?\",\n        \"answer\": \"Locking cryptocurrency to support network operations and earn rewards in proof-of-stake systems.\",\n        \"category\": \"Consensus Mechanisms\",\n        \"keywords\": \"validation, passive income\"\n    },\n    {\n        \"question\": \"What is Web3?\",\n        \"answer\": \"Decentralized internet paradigm using blockchain and token-based economics.\",\n        \"category\": \"Blockchain Ecosystem\",\n        \"keywords\": \"dApps, decentralized web\"\n    },\n    {\n        \"question\": \"What is gas fee?\",\n        \"answer\": \"Payment required to execute transactions or smart contracts on Ethereum network.\",\n        \"category\": \"Ethereum\",\n        \"keywords\": \"transaction cost, Gwei\"\n    },\n    {\n        \"question\": \"What is a DAO?\",\n        \"answer\": \"Decentralized Autonomous Organization governed by smart contracts and member voting.\",\n        \"category\": \"Governance\",\n        \"keywords\": \"smart contracts, community-led\"\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:43:49.775357Z","iopub.execute_input":"2025-05-20T19:43:49.775732Z","iopub.status.idle":"2025-05-20T19:43:49.793140Z","shell.execute_reply.started":"2025-05-20T19:43:49.775705Z","shell.execute_reply":"2025-05-20T19:43:49.791584Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Cell 4: User QnA Data Input (BEFORE system initialization)\nqna_data = [\n    {\n        \"question\": \"What is Bitcoin?\",\n        \"answer\": \"A decentralized digital currency...\",\n        \"category\": \"Cryptocurrency Basics\",\n        \"keywords\": \"blockchain, digital currency\"\n    },\n    # Add your custom QnAs here\n]\n\n# Cell 5: System Initialization & Data Insertion\nqna_system = QnASystem()  # Initialize AFTER data input\nqna_system.batch_insert(qna_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:43:49.794356Z","iopub.execute_input":"2025-05-20T19:43:49.794778Z","iopub.status.idle":"2025-05-20T19:43:54.524777Z","shell.execute_reply.started":"2025-05-20T19:43:49.794721Z","shell.execute_reply":"2025-05-20T19:43:54.523660Z"}},"outputs":[{"name":"stderr","text":"Processing batches: 1it [00:04,  4.70s/it]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"logger.info(\"System initialized successfully!\")\n\n# Cell 6 (Revised)\ntry:\n    test_qna = {\n        \"question\": \"What is Kaggle?\",\n        \"answer\": \"A data science competition platform\",\n        \"keywords\": \"platform\"\n    }\n    \n    with qna_system.conn:\n        qna_system.batch_insert([test_qna])\n    \n    results = qna_system.semantic_search(\"data science platform\")\n    logger.info(f\"Test results: {results[0]['answer'] if results else 'No matches'}\")\nexcept Exception as e:\n    logger.error(f\"Test failed: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:43:54.525704Z","iopub.execute_input":"2025-05-20T19:43:54.526100Z","iopub.status.idle":"2025-05-20T19:43:54.537966Z","shell.execute_reply.started":"2025-05-20T19:43:54.526068Z","shell.execute_reply":"2025-05-20T19:43:54.536692Z"}},"outputs":[{"name":"stderr","text":"Processing batches: 0it [00:00, ?it/s]\n","output_type":"stream"}],"execution_count":19}]}