{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Enhanced database schema with proper indexing and normalization\ndef _initialize_db(self):\n    \"\"\"Create optimized database schema\"\"\"\n    self.conn.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS qna_pairs (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        question TEXT NOT NULL,\n        answer TEXT NOT NULL,\n        category TEXT,\n        word_count INTEGER,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        last_accessed TIMESTAMP,\n        usage_count INTEGER DEFAULT 0,\n        keywords TEXT,\n        normalized_question TEXT,\n        question_hash TEXT UNIQUE  -- For duplicate detection\n    )\"\"\")\n    \n    # Vector embeddings table with proper indexing\n    self.conn.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS qna_embeddings (\n        qna_id INTEGER PRIMARY KEY,\n        question_vector BLOB,\n        answer_vector BLOB,\n        keywords_vector BLOB,\n        FOREIGN KEY (qna_id) REFERENCES qna_pairs(id)\n    )\"\"\")\n    \n    # Create optimized indexes\n    self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_category ON qna_pairs(category)\")\n    self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_keywords ON qna_pairs(keywords)\")\n    self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_normalized ON qna_pairs(normalized_question)\")\n    \n    # Full-text search with proper tokenizer\n    self.conn.execute(\"\"\"\n    CREATE VIRTUAL TABLE IF NOT EXISTS qna_search \n    USING fts5(question, answer, keywords, tokenize='porter unicode61')\n    \"\"\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:\n    \"\"\"Enhanced semantic search with RAG\"\"\"\n    # Generate query embedding\n    query_embedding = self._text_to_vector(query)\n    \n    # First perform keyword pre-filtering\n    keyword_matches = self.keyword_search(query, limit=top_k*3)\n    \n    if not keyword_matches:\n        return []\n    \n    # Get embeddings for pre-filtered results\n    ids = [str(qid) for qid, _, _ in keyword_matches]\n    cursor = self.conn.execute(f\"\"\"\n    SELECT qna_id, question_vector, answer_vector \n    FROM qna_embeddings \n    WHERE qna_id IN ({','.join(['?']*len(ids))})\n    \"\"\", ids)\n    \n    # Calculate similarities\n    results = []\n    for qna_id, q_vec, a_vec in cursor.fetchall():\n        q_sim = cosine_similarity([query_embedding], [self._vector_to_array(q_vec)])[0][0]\n        a_sim = cosine_similarity([query_embedding], [self._vector_to_array(a_vec)])[0][0]\n        combined_score = 0.6*q_sim + 0.4*a_sim\n        results.append((qna_id, combined_score))\n    \n    # Get top results with full data\n    top_ids = [x[0] for x in sorted(results, key=lambda x: x[1], reverse=True)[:top_k]]\n    return self.get_qna_by_ids(top_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _text_to_vector(self, text: str) -> np.ndarray:\n    \"\"\"GPU-accelerated embedding generation\"\"\"\n    if torch.cuda.is_available():\n        self.model = self.model.to('cuda')\n    \n    inputs = self.tokenizer(\n        text, \n        return_tensors=\"pt\", \n        truncation=True, \n        max_length=512,\n        padding=True\n    )\n    \n    if torch.cuda.is_available():\n        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = self.model(**inputs)\n    \n    # Average pooling\n    last_hidden_state = outputs.last_hidden_state\n    vector = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n    return vector","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_insert(self, qna_list: List[Dict], batch_size: int = 100):\n    \"\"\"Optimized batch insert with progress tracking\"\"\"\n    for batch in tqdm(self._chunk_list(qna_list, batch_size), desc=\"Processing batches\"):\n        # Process embeddings in parallel\n        with ThreadPoolExecutor() as executor:\n            embeddings = list(executor.map(self._process_single_qna, batch))\n        \n        # Bulk insert\n        with self.conn:\n            self.conn.executemany(\"\"\"\n            INSERT INTO qna_pairs \n            (question, answer, category, word_count, keywords, normalized_question, question_hash)\n            VALUES (?, ?, ?, ?, ?, ?, ?)\n            \"\"\", [(q['question'], q['answer'], q.get('category'), \n                  len(q['answer'].split()), q['keywords'], \n                  self._normalize_text(q['question']),\n                  self._generate_hash(q['question']) for q in batch])\n            \n            # Insert embeddings\n            self.conn.executemany(\"\"\"\n            INSERT INTO qna_embeddings \n            (qna_id, question_vector, answer_vector, keywords_vector)\n            VALUES (?, ?, ?, ?)\n            \"\"\", embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def migrate_to_postgres(self):\n    \"\"\"Migration path to PostgreSQL\"\"\"\n    import psycopg2\n    from pgvector.psycopg2 import register_vector\n    \n    # Connect to PostgreSQL\n    pg_conn = psycopg2.connect(os.getenv(\"POSTGRES_URL\"))\n    register_vector(pg_conn)\n    \n    # Create optimized schema\n    with pg_conn.cursor() as cursor:\n        cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS qna_pairs (\n            id SERIAL PRIMARY KEY,\n            question TEXT NOT NULL,\n            answer TEXT NOT NULL,\n            category TEXT,\n            word_count INTEGER,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            last_accessed TIMESTAMP,\n            usage_count INTEGER DEFAULT 0,\n            keywords TEXT,\n            normalized_question TEXT,\n            question_hash TEXT UNIQUE\n        )\"\"\")\n        \n        cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS qna_embeddings (\n            qna_id INTEGER PRIMARY KEY REFERENCES qna_pairs(id),\n            question_vector VECTOR(1024),\n            answer_vector VECTOR(1024),\n            keywords_vector VECTOR(1024)\n        )\"\"\")\n        \n        # Create specialized indexes\n        cursor.execute(\"CREATE INDEX ON qna_pairs USING GIN(to_tsvector('english', question))\")\n        cursor.execute(\"CREATE INDEX ON qna_pairs USING GIN(to_tsvector('english', answer))\")\n        cursor.execute(\"CREATE INDEX ON qna_embeddings USING ivfflat (question_vector vector_l2_ops)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_domain_coverage(self, domain_terms: List[str]) -> Dict:\n    \"\"\"Evaluate how well the model covers domain-specific terms\"\"\"\n    vocab = set(self.tokenizer.get_vocab().keys())\n    missing = [term for term in domain_terms if term.lower() not in vocab]\n    \n    return {\n        \"total_terms\": len(domain_terms),\n        \"covered_terms\": len(domain_terms) - len(missing),\n        \"coverage_percentage\": (len(domain_terms) - len(missing)) / len(domain_terms) * 100,\n        \"missing_terms\": missing[:20]  # Show first 20 missing terms\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def benchmark_search(self, test_queries: List[str], iterations: int = 10):\n    \"\"\"Benchmark search performance\"\"\"\n    results = []\n    for query in test_queries:\n        times = []\n        for _ in range(iterations):\n            start = time.time()\n            self.semantic_search(query)\n            times.append(time.time() - start)\n        \n        results.append({\n            \"query\": query,\n            \"avg_time\": sum(times)/len(times),\n            \"min_time\": min(times),\n            \"max_time\": max(times)\n        })\n    \n    return pd.DataFrame(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_answer_quality(self, test_set: List[Dict]) -> Dict:\n    \"\"\"Evaluate answer quality against a test set\"\"\"\n    correct = 0\n    for item in test_set:\n        result = self.semantic_search(item[\"question\"], top_k=1)\n        if result and result[0][\"answer\"] == item[\"expected_answer\"]:\n            correct += 1\n    \n    return {\n        \"total_questions\": len(test_set),\n        \"correct_answers\": correct,\n        \"accuracy\": correct / len(test_set) * 100\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}