{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install required packages\n!pip install torch transformers sentence-transformers psycopg2-binary pgvector tqdm scikit-learn\n!apt-get update && apt-get install -y postgresql postgresql-contrib\n!service postgresql start\n!sudo -u postgres psql -c \"CREATE EXTENSION IF NOT EXISTS vector;\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:30:03.644174Z","iopub.execute_input":"2025-05-20T19:30:03.645866Z","iopub.status.idle":"2025-05-20T19:32:31.155357Z","shell.execute_reply.started":"2025-05-20T19:30:03.645815Z","shell.execute_reply":"2025-05-20T19:32:31.152842Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nCollecting psycopg2-binary\n  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: pgvector in /usr/local/lib/python3.11/dist-packages (0.4.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: psycopg2-binary, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 psycopg2-binary-2.9.10\nGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease                                              \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]                           \nGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]                             \nGet:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]                                \nGet:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]                           \nHit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\nHit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nGet:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\nGet:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\nGet:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,958 kB]\nGet:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,255 kB]\nGet:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,729 kB]                    \nGet:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]        \nGet:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]          \nGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,540 kB]\nGet:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nGet:22 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [77.3 kB]    \nFetched 31.9 MB in 7s (4,446 kB/s)                                                                  \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n  logrotate netbase postgresql-14 postgresql-client-14 postgresql-client-common\n  postgresql-common ssl-cert sysstat\nSuggested packages:\n  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\nThe following NEW packages will be installed:\n  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n  logrotate netbase postgresql postgresql-14 postgresql-client-14\n  postgresql-client-common postgresql-common postgresql-contrib ssl-cert\n  sysstat\n0 upgraded, 14 newly installed, 0 to remove and 144 not upgraded.\nNeed to get 18.4 MB of archives.\nAfter this operation, 52.0 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-xs-perl amd64 4.030-1build3 [87.2 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.17-0ubuntu0.22.04.1 [1,246 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.17-0ubuntu0.22.04.1 [16.2 MB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\nFetched 18.4 MB in 1s (18.4 MB/s) \nPreconfiguring packages ...\nSelecting previously unselected package logrotate.\n(Reading database ... 129184 files and directories currently installed.)\nPreparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\nUnpacking logrotate (3.19.0-1ubuntu1.1) ...\nSelecting previously unselected package netbase.\nPreparing to unpack .../01-netbase_6.3_all.deb ...\nUnpacking netbase (6.3) ...\nSelecting previously unselected package libcommon-sense-perl:amd64.\nPreparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\nUnpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\nSelecting previously unselected package libjson-perl.\nPreparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\nUnpacking libjson-perl (4.04000-1) ...\nSelecting previously unselected package libtypes-serialiser-perl.\nPreparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\nUnpacking libtypes-serialiser-perl (1.01-1) ...\nSelecting previously unselected package libjson-xs-perl.\nPreparing to unpack .../05-libjson-xs-perl_4.030-1build3_amd64.deb ...\nUnpacking libjson-xs-perl (4.030-1build3) ...\nSelecting previously unselected package postgresql-client-common.\nPreparing to unpack .../06-postgresql-client-common_238_all.deb ...\nUnpacking postgresql-client-common (238) ...\nSelecting previously unselected package postgresql-client-14.\nPreparing to unpack .../07-postgresql-client-14_14.17-0ubuntu0.22.04.1_amd64.deb ...\nUnpacking postgresql-client-14 (14.17-0ubuntu0.22.04.1) ...\nSelecting previously unselected package ssl-cert.\nPreparing to unpack .../08-ssl-cert_1.1.2_all.deb ...\nUnpacking ssl-cert (1.1.2) ...\nSelecting previously unselected package postgresql-common.\nPreparing to unpack .../09-postgresql-common_238_all.deb ...\nAdding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\nUnpacking postgresql-common (238) ...\nSelecting previously unselected package postgresql-14.\nPreparing to unpack .../10-postgresql-14_14.17-0ubuntu0.22.04.1_amd64.deb ...\nUnpacking postgresql-14 (14.17-0ubuntu0.22.04.1) ...\nSelecting previously unselected package postgresql.\nPreparing to unpack .../11-postgresql_14+238_all.deb ...\nUnpacking postgresql (14+238) ...\nSelecting previously unselected package postgresql-contrib.\nPreparing to unpack .../12-postgresql-contrib_14+238_all.deb ...\nUnpacking postgresql-contrib (14+238) ...\nSelecting previously unselected package sysstat.\nPreparing to unpack .../13-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\nUnpacking sysstat (12.5.2-2ubuntu0.2) ...\nSetting up logrotate (3.19.0-1ubuntu1.1) ...\nCreated symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\nSetting up libcommon-sense-perl:amd64 (3.75-2build1) ...\nSetting up ssl-cert (1.1.2) ...\nSetting up libtypes-serialiser-perl (1.01-1) ...\nSetting up libjson-perl (4.04000-1) ...\nSetting up netbase (6.3) ...\nSetting up sysstat (12.5.2-2ubuntu0.2) ...\n\nCreating config file /etc/default/sysstat with new version\nupdate-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\nCreated symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\nCreated symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\nCreated symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\nSetting up postgresql-client-common (238) ...\nSetting up libjson-xs-perl (4.030-1build3) ...\nSetting up postgresql-client-14 (14.17-0ubuntu0.22.04.1) ...\nupdate-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\nSetting up postgresql-common (238) ...\nAdding user postgres to group ssl-cert\n\nCreating config file /etc/postgresql-common/createcluster.conf with new version\nBuilding PostgreSQL dictionaries from installed myspell/hunspell packages...\nRemoving obsolete dictionary files:\nCreated symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\nSetting up postgresql-14 (14.17-0ubuntu0.22.04.1) ...\nCreating new PostgreSQL cluster 14/main ...\n/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\nThe files belonging to this database system will be owned by user \"postgres\".\nThis user must also own the server process.\n\nThe database cluster will be initialized with locale \"en_US.UTF-8\".\nThe default database encoding has accordingly been set to \"UTF8\".\nThe default text search configuration will be set to \"english\".\n\nData page checksums are disabled.\n\nfixing permissions on existing directory /var/lib/postgresql/14/main ... ok\ncreating subdirectories ... ok\nselecting dynamic shared memory implementation ... posix\nselecting default max_connections ... 100\nselecting default shared_buffers ... 128MB\nselecting default time zone ... Etc/UTC\ncreating configuration files ... ok\nrunning bootstrap script ... ok\nperforming post-bootstrap initialization ... ok\nsyncing data to disk ... ok\nupdate-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\ninvoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of start.\nSetting up postgresql-contrib (14+238) ...\nSetting up postgresql (14+238) ...\nProcessing triggers for man-db (2.10.2-1) ...\n * Starting PostgreSQL 14 database server\n   ...done.\nERROR:  could not open extension control file \"/usr/share/postgresql/14/extension/vector.control\": No such file or directory\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Cell 2: Configure environment\nimport os\nos.environ[\"POSTGRES_URL\"] = \"postgres://postgres@localhost/postgres\"\n\n# Cell 3: Import packages\nimport numpy as np\nimport sqlite3\nimport logging\nimport hashlib\nimport psycopg2\nimport time\nimport pandas as pd\nimport torch\nfrom typing import List, Dict, Optional\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pgvector.psycopg2 import register_vector\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nprint(\"All packages imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:32:31.159219Z","iopub.execute_input":"2025-05-20T19:32:31.159687Z","iopub.status.idle":"2025-05-20T19:32:31.171298Z","shell.execute_reply.started":"2025-05-20T19:32:31.159644Z","shell.execute_reply":"2025-05-20T19:32:31.170013Z"}},"outputs":[{"name":"stdout","text":"All packages imported successfully!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 4: Verify PostgreSQL connection\ntry:\n    pg_conn = psycopg2.connect(os.environ[\"POSTGRES_URL\"])\n    register_vector(pg_conn)\n    pg_conn.close()\n    logger.info(\"PostgreSQL connection successful!\")\nexcept Exception as e:\n    logger.error(f\"PostgreSQL connection failed: {str(e)}\")\n\nclass QnASystem:\n    def __init__(self, db_path: str = \"qna.db\", model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n        self.db_path = db_path\n        self.model_name = model_name\n        self.conn = None\n        self.model = None\n        self.tokenizer = None\n        self.embedding_dim = 768\n        self._initialize_components()\n\n    def _initialize_components(self):\n        \"\"\"Initialize database and ML components with error handling\"\"\"\n        try:\n            self.conn = sqlite3.connect(self.db_path)\n            self._initialize_db()\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            self.model = AutoModel.from_pretrained(self.model_name)\n            if torch.cuda.is_available():\n                self.model = self.model.to('cuda')\n            logger.info(\"System initialized successfully\")\n        except Exception as e:\n            logger.error(f\"Initialization failed: {str(e)}\")\n            raise\n\n    def _initialize_db(self):\n        \"\"\"Create optimized database schema with transaction support\"\"\"\n        try:\n            with self.conn:\n                # Create tables\n                self.conn.executescript(\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_pairs (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        question TEXT NOT NULL,\n                        answer TEXT NOT NULL,\n                        category TEXT,\n                        word_count INTEGER,\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        last_accessed TIMESTAMP,\n                        usage_count INTEGER DEFAULT 0,\n                        keywords TEXT,\n                        normalized_question TEXT,\n                        question_hash TEXT UNIQUE\n                    );\n                    \n                    CREATE TABLE IF NOT EXISTS qna_embeddings (\n                        qna_id INTEGER PRIMARY KEY,\n                        question_vector BLOB,\n                        answer_vector BLOB,\n                        keywords_vector BLOB,\n                        FOREIGN KEY (qna_id) REFERENCES qna_pairs(id)\n                    );\n                    \n                    CREATE INDEX IF NOT EXISTS idx_category ON qna_pairs(category);\n                    CREATE INDEX IF NOT EXISTS idx_keywords ON qna_pairs(keywords);\n                    \n                    CREATE VIRTUAL TABLE IF NOT EXISTS qna_search \n                    USING fts5(question, answer, keywords, tokenize='porter unicode61');\n                \"\"\")\n            logger.info(\"Database initialized successfully\")\n        except sqlite3.Error as e:\n            logger.error(f\"Database initialization error: {str(e)}\")\n            raise\n\n    def _text_to_vector(self, text: str) -> np.ndarray:\n        \"\"\"Generate embeddings with GPU support and fallback\"\"\"\n        try:\n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n            self.model = self.model.to(device)\n            \n            inputs = self.tokenizer(\n                text,\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=512,\n                padding=True\n            ).to(device)\n            \n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            \n            # Contextual pooling with attention weights\n            last_hidden_state = outputs.last_hidden_state\n            attention_mask = inputs.attention_mask.unsqueeze(-1)\n            vector = (last_hidden_state * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n            return vector.cpu().numpy().squeeze()\n        except Exception as e:\n            logger.error(f\"Embedding generation failed: {str(e)}\")\n            raise\n\n    def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"Hybrid search with semantic and keyword components\"\"\"\n        try:\n            query_embedding = self._text_to_vector(query)\n            keyword_matches = self.keyword_search(query, limit=top_k*3)\n            if not keyword_matches:\n                return []\n\n            ids = [str(qid) for qid, _, _ in keyword_matches]\n            cursor = self.conn.execute(f\"\"\"\n                SELECT qna_id, question_vector, answer_vector \n                FROM qna_embeddings \n                WHERE qna_id IN ({','.join(['?']*len(ids))})\n            \"\"\", ids)\n            \n            results = []\n            for qna_id, q_vec, a_vec in cursor.fetchall():\n                q_sim = cosine_similarity([query_embedding], [np.frombuffer(q_vec)])[0][0]\n                a_sim = cosine_similarity([query_embedding], [np.frombuffer(a_vec)])[0][0]\n                combined_score = 0.6*q_sim + 0.4*a_sim\n                results.append((qna_id, combined_score))\n            \n            top_ids = [x[0] for x in sorted(results, key=lambda x: x[1], reverse=True)[:top_k]]\n            return self.get_qna_by_ids(top_ids)\n        except Exception as e:\n            logger.error(f\"Search failed: {str(e)}\")\n            return []\n\n    def keyword_search(self, query: str, limit: int = 15) -> List[tuple]:\n        \"\"\"Keyword-based search using FTS5\"\"\"\n        try:\n            cursor = self.conn.execute(\"\"\"\n                SELECT rowid, question, answer \n                FROM qna_search \n                WHERE question MATCH ? \n                ORDER BY bm25(qna_search) \n                LIMIT ?\n            \"\"\", (query, limit))\n            return cursor.fetchall()\n        except sqlite3.Error as e:\n            logger.error(f\"Keyword search failed: {str(e)}\")\n            return []\n\n    def get_qna_by_ids(self, ids: List[int]) -> List[Dict]:\n        \"\"\"Retrieve full QnA records by IDs\"\"\"\n        try:\n            cursor = self.conn.execute(f\"\"\"\n                SELECT * FROM qna_pairs \n                WHERE id IN ({','.join(['?']*len(ids))})\n            \"\"\", ids)\n            columns = [col[0] for col in cursor.description]\n            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n        except sqlite3.Error as e:\n            logger.error(f\"Get QnA by IDs failed: {str(e)}\")\n            return []\n\n    def batch_insert(self, qna_list: List[Dict], batch_size: int = 100):\n        \"\"\"Optimized batch processing with transactions\"\"\"\n        try:\n            for batch in tqdm(self._chunk_list(qna_list, batch_size), desc=\"Processing batches\"):\n                with self.conn:\n                    # Insert into qna_pairs\n                    qna_values = [\n                        (\n                            q['question'],\n                            q['answer'],\n                            q.get('category'),\n                            len(q['answer'].split()),\n                            q.get('keywords', ''),\n                            self._normalize_text(q['question']),\n                            self._generate_hash(q['question'])\n                        ) for q in batch\n                    ]\n                    self.conn.executemany(\"\"\"\n                        INSERT INTO qna_pairs \n                        (question, answer, category, word_count, keywords, normalized_question, question_hash)\n                        VALUES (?, ?, ?, ?, ?, ?, ?)\n                    \"\"\", qna_values)\n\n                    # Insert into qna_search\n                    search_values = [\n                        (q['question'], q['answer'], q.get('keywords', '')) \n                        for q in batch\n                    ]\n                    self.conn.executemany(\"\"\"\n                        INSERT INTO qna_search \n                        (question, answer, keywords)\n                        VALUES (?, ?, ?)\n                    \"\"\", search_values)\n\n                    # Get inserted IDs\n                    cursor = self.conn.execute(\"\"\"\n                        SELECT id FROM qna_pairs \n                        ORDER BY id DESC LIMIT ?\n                    \"\"\", (len(batch),))\n                    inserted_ids = [row[0] for row in cursor.fetchall()][::-1]\n\n                    # Generate embeddings\n                    embeddings = []\n                    for qna_id, qna in zip(inserted_ids, batch):\n                        q_vec = self._text_to_vector(qna['question']).tobytes()\n                        a_vec = self._text_to_vector(qna['answer']).tobytes()\n                        k_vec = self._text_to_vector(qna.get('keywords', '')).tobytes()\n                        embeddings.append((qna_id, q_vec, a_vec, k_vec))\n\n                    self.conn.executemany(\"\"\"\n                        INSERT INTO qna_embeddings \n                        (qna_id, question_vector, answer_vector, keywords_vector)\n                        VALUES (?, ?, ?, ?)\n                    \"\"\", embeddings)\n        except Exception as e:\n            logger.error(f\"Batch insert failed: {str(e)}\")\n            self.conn.rollback()\n            raise\n\n    def migrate_to_postgres(self):\n        \"\"\"Database migration with full data transfer\"\"\"\n        try:\n            pg_conn = psycopg2.connect(os.environ[\"POSTGRES_URL\"])\n            register_vector(pg_conn)\n            \n            with pg_conn.cursor() as cursor, self.conn:\n                # Create PostgreSQL schema\n                cursor.execute(f\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_pairs (\n                        id INTEGER PRIMARY KEY,\n                        question TEXT NOT NULL,\n                        answer TEXT NOT NULL,\n                        category TEXT,\n                        word_count INTEGER,\n                        created_at TIMESTAMP,\n                        last_accessed TIMESTAMP,\n                        usage_count INTEGER,\n                        keywords TEXT,\n                        normalized_question TEXT,\n                        question_hash TEXT UNIQUE\n                    )\"\"\")\n                \n                cursor.execute(f\"\"\"\n                    CREATE TABLE IF NOT EXISTS qna_embeddings (\n                        qna_id INTEGER PRIMARY KEY,\n                        question_vector VECTOR({self.embedding_dim}),\n                        answer_vector VECTOR({self.embedding_dim}),\n                        keywords_vector VECTOR({self.embedding_dim})\n                    )\"\"\")\n\n                # Migrate qna_pairs\n                sqlite_data = self.conn.execute(\"\"\"\n                    SELECT id, question, answer, category, word_count, created_at,\n                           last_accessed, usage_count, keywords, normalized_question, question_hash\n                    FROM qna_pairs\n                \"\"\").fetchall()\n                \n                cursor.executemany(\"\"\"\n                    INSERT INTO qna_pairs \n                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n                \"\"\", sqlite_data)\n\n                # Migrate embeddings with vector conversion\n                embedding_data = self.conn.execute(\"\"\"\n                    SELECT qna_id, question_vector, answer_vector, keywords_vector \n                    FROM qna_embeddings\n                \"\"\").fetchall()\n                \n                converted_embeddings = []\n                for row in embedding_data:\n                    converted = (\n                        row[0],\n                        np.frombuffer(row[1]).tolist(),\n                        np.frombuffer(row[2]).tolist(),\n                        np.frombuffer(row[3]).tolist()\n                    )\n                    converted_embeddings.append(converted)\n                \n                cursor.executemany(\"\"\"\n                    INSERT INTO qna_embeddings \n                    VALUES (%s, %s, %s, %s)\n                \"\"\", converted_embeddings)\n\n                pg_conn.commit()\n            logger.info(\"Migration completed successfully\")\n        except Exception as e:\n            logger.error(f\"Migration failed: {str(e)}\")\n            if 'pg_conn' in locals():\n                pg_conn.rollback()\n            raise\n\n    # Helper methods\n    def _chunk_list(self, lst: List, n: int):\n        for i in range(0, len(lst), n):\n            yield lst[i:i + n]\n\n    def _normalize_text(self, text: str) -> str:\n        return text.lower().strip()\n\n    def _generate_hash(self, text: str) -> str:\n        return hashlib.sha256(text.encode()).hexdigest()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:32:31.172376Z","iopub.execute_input":"2025-05-20T19:32:31.172858Z","iopub.status.idle":"2025-05-20T19:32:31.242575Z","shell.execute_reply.started":"2025-05-20T19:32:31.172818Z","shell.execute_reply":"2025-05-20T19:32:31.241191Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Cell 5: Initialize system\nqna_system = QnASystem()\nlogger.info(\"System initialized successfully!\")\n\n# Cell 6: Test functionality\ntry:\n    test_data = [{\n        \"question\": \"What is Kaggle?\",\n        \"answer\": \"A data science competition platform\",\n        \"keywords\": \"platform\"\n    }]\n    qna_system.batch_insert(test_data)\n    results = qna_system.semantic_search(\"data science platform\")\n    logger.info(f\"Test search results: {results}\")\nexcept Exception as e:\n    logger.error(f\"Initial test failed: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:32:31.244805Z","iopub.execute_input":"2025-05-20T19:32:31.245185Z","iopub.status.idle":"2025-05-20T19:33:04.569959Z","shell.execute_reply.started":"2025-05-20T19:32:31.245153Z","shell.execute_reply":"2025-05-20T19:33:04.568924Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae0ab31bbb948a2ad7f61f8788d4183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bdf76a7f658490c9162edb1de99eb18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"722cb18c67f24a48a3abec2015114b89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46e1d63634d84fd88eb04db0717b584f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82c4b01741b477583b8cafe833c35f0"}},"metadata":{}},{"name":"stderr","text":"2025-05-20 19:32:43.019183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747769563.322478      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747769563.407733      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41a70e8d5b74e6480faf65c2ffeb187"}},"metadata":{}},{"name":"stderr","text":"Processing batches: 1it [00:00,  2.61it/s]\n","output_type":"stream"}],"execution_count":12}]}