{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11876335,"sourceType":"datasetVersion","datasetId":7463918}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install required packages (simplified)\n!pip install torch transformers sentence-transformers tqdm scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:41:17.894530Z","iopub.execute_input":"2025-05-20T21:41:17.895521Z","iopub.status.idle":"2025-05-20T21:41:22.291521Z","shell.execute_reply.started":"2025-05-20T21:41:17.895490Z","shell.execute_reply":"2025-05-20T21:41:22.290224Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 2: Configure environment and logging\nimport os\nimport logging\n\n# Suppress unnecessary warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Reduce TensorFlow logging\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n# Configure main logger\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\nlogger.info(\"Logging configured successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:41:22.293857Z","iopub.execute_input":"2025-05-20T21:41:22.294299Z","iopub.status.idle":"2025-05-20T21:41:22.305095Z","shell.execute_reply.started":"2025-05-20T21:41:22.294261Z","shell.execute_reply":"2025-05-20T21:41:22.303493Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Cell 3: Import packages with verification\nimport numpy as np\nimport sqlite3\nimport hashlib\nimport torch\nfrom typing import List, Dict\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nlogger.info(\"Core packages imported successfully\")\n\n# Verify CUDA availability\nif torch.cuda.is_available():\n    logger.info(f\"CUDA enabled using {torch.cuda.get_device_name(0)}\")\nelse:\n    logger.warning(\"CUDA not available, using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:41:22.306258Z","iopub.execute_input":"2025-05-20T21:41:22.306696Z","iopub.status.idle":"2025-05-20T21:41:22.327584Z","shell.execute_reply.started":"2025-05-20T21:41:22.306662Z","shell.execute_reply":"2025-05-20T21:41:22.326506Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Cell 4: QnA System Class (enhanced)\nclass QnASystem:\n    def __init__(self, db_path: str = \"/kaggle/working/qna_db.sqlite\"):\n        self.db_path = db_path\n        self.conn = None\n        self.model = None\n        self.tokenizer = None\n        self._initialize_components()\n        self._verify_system_health()\n\n    def _initialize_components(self):\n        \"\"\"Initialize database and ML components with verification\"\"\"\n        try:\n            # Database setup with check_same_thread=False for notebook environments\n            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n            self._initialize_db()\n            logger.info(\"Database component initialized\")\n            \n            # Model setup\n            self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n            self.model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n            if torch.cuda.is_available():\n                self.model = self.model.to('cuda')\n            logger.info(\"Model loaded successfully\")\n            \n            # Test embedding generation\n            test_embed = self._text_to_vector(\"test\")\n            logger.debug(f\"Test embedding shape: {test_embed.shape}\")\n            \n        except Exception as e:\n            logger.error(f\"Initialization failed: {str(e)}\")\n            raise\n\n    def _verify_system_health(self):\n        \"\"\"Perform system health checks\"\"\"\n        cursor = self.conn.execute(\"SELECT count(*) FROM sqlite_master\")\n        if cursor.fetchone()[0] < 1:\n            raise RuntimeError(\"Database tables not initialized properly\")\n            \n        test_text = \"system health check\"\n        emb = self._text_to_vector(test_text)\n        if emb.shape != (768,):\n            raise RuntimeError(f\"Invalid embedding shape: {emb.shape}\")\n            \n        logger.info(\"System health verification passed\")\n\n    def _initialize_db(self):\n        \"\"\"Create database schema with error recovery\"\"\"\n        try:\n            self.conn.executescript(\"\"\"\n                CREATE TABLE IF NOT EXISTS qna_pairs (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    question TEXT UNIQUE NOT NULL,\n                    answer TEXT NOT NULL,\n                    category TEXT,\n                    keywords TEXT,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                );\n                CREATE TABLE IF NOT EXISTS qna_embeddings (\n                    qna_id INTEGER PRIMARY KEY,\n                    question_vector BLOB NOT NULL,\n                    FOREIGN KEY(qna_id) REFERENCES qna_pairs(id)\n                );\n            \"\"\")\n        except sqlite3.Error as e:\n            logger.error(f\"Database error: {str(e)}\")\n            raise\n\n    def _text_to_vector(self, text: str) -> np.ndarray:\n        \"\"\"Convert text to embedding vector with proper pooling\"\"\"\n        try:\n            inputs = self.tokenizer(\n                text, \n                return_tensors=\"pt\", \n                padding=True, \n                truncation=True, \n                max_length=512\n            )\n            \n            if torch.cuda.is_available():\n                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            \n            # Mean pooling implementation\n            token_embeddings = outputs.last_hidden_state\n            attention_mask = inputs['attention_mask']\n            input_mask_expanded = (\n                attention_mask\n                .unsqueeze(-1)\n                .expand(token_embeddings.size())\n                .float()\n            )\n            embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n            embeddings = embeddings / sum_mask\n            \n            return embeddings.cpu().numpy().squeeze()\n        \n        except Exception as e:\n            logger.error(f\"Vectorization failed: {str(e)}\")\n            raise\n\n    def _get_embeddings(self) -> Dict[int, np.ndarray]:\n            \"\"\"Retrieve all stored embeddings from database\"\"\"\n            try:\n                cursor = self.conn.execute(\"\"\"\n                    SELECT qna_id, question_vector \n                    FROM qna_embeddings\n                \"\"\")\n                \n                embeddings = {}\n                for qna_id, vec_blob in cursor.fetchall():\n                    embeddings[qna_id] = np.frombuffer(vec_blob, dtype=np.float32)\n                \n                logger.debug(f\"Loaded {len(embeddings)} embeddings from database\")\n                return embeddings\n                \n            except sqlite3.Error as e:\n                logger.error(f\"Failed to load embeddings: {str(e)}\")\n                raise\n\n    def _preload_embeddings(self):\n        \"\"\"Load all embeddings into memory for fast access\"\"\"\n        try:\n            cursor = self.conn.execute(\"SELECT qna_id, question_vector FROM qna_embeddings\")\n            results = cursor.fetchall()\n            \n            self.embedding_ids = []\n            vectors = []\n            for qna_id, vec_blob in results:\n                self.embedding_ids.append(qna_id)\n                vectors.append(np.frombuffer(vec_blob, dtype=np.float32))\n            \n            self.embedding_matrix = np.array(vectors)\n            logger.info(f\"Loaded {len(self.embedding_ids)} embeddings into memory\")\n\n        except sqlite3.Error as e:\n            logger.error(f\"Embedding preload failed: {str(e)}\")\n            raise\n\n    def _batch_text_to_vectors(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Convert batch of texts to embeddings with GPU optimization\"\"\"\n        try:\n            inputs = self.tokenizer(\n                texts, \n                return_tensors=\"pt\", \n                padding=True, \n                truncation=True, \n                max_length=512\n            )\n            \n            if torch.cuda.is_available():\n                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                outputs = self.model(**inputs)\n            \n            # Optimized mean pooling\n            attention_mask = inputs['attention_mask']\n            last_hidden = outputs.last_hidden_state.masked_fill(\n                ~attention_mask[..., None].bool(), 0.0\n            )\n            embeddings = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n            \n            return embeddings.cpu().numpy().astype(np.float32)\n        \n        except Exception as e:\n            logger.error(f\"Batch vectorization failed: {str(e)}\")\n            raise\n    \n    # Modified data ingestion method\n    def ingest_batch(self, qna_batch: List[Dict], batch_size: int = 500):\n        \"\"\"Optimized batch ingestion with embeddings\"\"\"\n        try:\n            with self.conn:\n                # Insert QnA pairs\n                self.conn.executemany(\n                    \"\"\"INSERT OR IGNORE INTO qna_pairs \n                    (question, answer, category, keywords)\n                    VALUES (?, ?, ?, ?)\"\"\",\n                    [(q[\"question\"], q[\"answer\"], q[\"category\"], q[\"keywords\"]) \n                     for q in qna_batch]\n                )\n\n                # Get inserted IDs\n                questions = [q[\"question\"] for q in qna_batch]\n                cursor = self.conn.execute(\n                    \"SELECT id, question FROM qna_pairs WHERE question IN (%s)\" \n                    % \",\".join(\"?\"*len(questions)), questions)\n                id_map = {row[1]: row[0] for row in cursor.fetchall()}\n\n            # Generate embeddings in batches\n            embedding_data = []\n            for i in range(0, len(qna_batch), batch_size):\n                batch = qna_batch[i:i+batch_size]\n                texts = [q[\"question\"] for q in batch]\n                vectors = self._batch_text_to_vectors(texts)\n                \n                for q, vec in zip(batch, vectors):\n                    if q[\"question\"] in id_map:\n                        embedding_data.append((\n                            id_map[q[\"question\"]],\n                            vec.tobytes()\n                        ))\n\n            # Insert embeddings\n            with self.conn:\n                self.conn.executemany(\n                    \"INSERT OR REPLACE INTO qna_embeddings VALUES (?, ?)\",\n                    embedding_data\n                )\n            \n            # Update in-memory embeddings\n            self._preload_embeddings()\n\n            logger.info(f\"Ingested {len(qna_batch)} QnAs with {len(embedding_data)} embeddings\")\n\n        except Exception as e:\n            logger.error(f\"Batch ingestion failed: {str(e)}\")\n            raise\n\n    def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"Optimized search using preloaded embeddings\"\"\"\n        try:\n            # Convert query once\n            query_vector = self._text_to_vector(query)\n            \n            # Use preloaded matrix\n            similarities = cosine_similarity(\n                [query_vector], \n                self.embedding_matrix\n            ).squeeze()\n\n            # Get top indices\n            top_indices = np.argpartition(similarities, -top_k)[-top_k:]\n            top_indices = top_indices[np.argsort(similarities[top_indices])][::-1]\n\n            # Retrieve results\n            results = []\n            for idx in top_indices:\n                qna_id = self.embedding_ids[idx]\n                cursor = self.conn.execute(\n                    \"SELECT question, answer, category FROM qna_pairs WHERE id = ?\",\n                    (qna_id,)\n                )\n                question, answer, category = cursor.fetchone()\n                results.append({\n                    \"qna_id\": qna_id,\n                    \"question\": question,\n                    \"answer\": answer,\n                    \"category\": category,\n                    \"similarity\": float(similarities[idx])\n                })\n\n            return results\n\n        except Exception as e:\n            logger.error(f\"Search error: {str(e)}\")\n            return []\n\n    def close(self):\n        \"\"\"Close database connection\"\"\"\n        if self.conn:\n            self.conn.close()\n            self.conn = None\n            logger.info(\"Database connection closed\")\n            \n    def __del__(self):\n        \"\"\"Cleanup resources\"\"\"\n        self.close()\n        logger.info(\"System shutdown complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:41:22.330164Z","iopub.execute_input":"2025-05-20T21:41:22.331143Z","iopub.status.idle":"2025-05-20T21:41:22.367181Z","shell.execute_reply.started":"2025-05-20T21:41:22.331107Z","shell.execute_reply":"2025-05-20T21:41:22.365860Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Cell 5: Data ingestion with verification (fixed)\nqna_data = [\n    {\n        \"question\": \"What is blockchain?\",\n        \"answer\": \"A decentralized digital ledger technology...\",\n        \"category\": \"Blockchain Basics\",\n        \"keywords\": \"distributed ledger, cryptography\"\n    }\n]\n\n# Close any existing connection\nif 'system' in locals():\n    system.conn.close()\n    del system\n\nsystem = QnASystem()\n\n# Cell 5: Batch Data Ingestion\ntry:\n    logger.info(\"Starting bulk data ingestion...\")\n    system = QnASystem()\n    \n    # Example batch loading - replace with your actual data\n    large_dataset = [...]  # Your 10k+ QnA pairs\n    \n    batch_size = 500  # Adjust based on available memory\n    for i in range(0, len(large_dataset), batch_size):\n        batch = large_dataset[i:i+batch_size]\n        system.ingest_batch(batch)\n        logger.info(f\"Ingested {i+batch_size} items\")\n        \n    logger.info(\"Bulk ingestion completed successfully\")\n\nexcept Exception as e:\n    logger.error(f\"Ingestion failed: {str(e)}\")\n    raise\nfinally:\n    system.conn.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:42:19.337986Z","iopub.execute_input":"2025-05-20T21:42:19.338916Z","iopub.status.idle":"2025-05-20T21:42:20.379830Z","shell.execute_reply.started":"2025-05-20T21:42:19.338880Z","shell.execute_reply":"2025-05-20T21:42:20.378264Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_8568/912355515.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlarge_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ingested {i+batch_size} items\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_8568/2350130958.py\u001b[0m in \u001b[0;36mingest_batch\u001b[0;34m(self, qna_batch, batch_size)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     VALUES (?, ?, ?, ?)\"\"\",\n\u001b[0;32m--> 183\u001b[0;31m                     [(q[\"question\"], q[\"answer\"], q[\"category\"], q[\"keywords\"]) \n\u001b[0m\u001b[1;32m    184\u001b[0m                      for q in qna_batch]\n\u001b[1;32m    185\u001b[0m                 )\n","\u001b[0;32m/tmp/ipykernel_8568/2350130958.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     VALUES (?, ?, ?, ?)\"\"\",\n\u001b[0;32m--> 183\u001b[0;31m                     [(q[\"question\"], q[\"answer\"], q[\"category\"], q[\"keywords\"]) \n\u001b[0m\u001b[1;32m    184\u001b[0m                      for q in qna_batch]\n\u001b[1;32m    185\u001b[0m                 )\n","\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not subscriptable"],"ename":"TypeError","evalue":"'ellipsis' object is not subscriptable","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"# Cell 6: Enhanced search test (fixed)\ntry:\n    logger.info(\"\\n=== Testing Search Functionality ===\")\n    test_query = \"distributed ledger technology\"\n    \n    # Create new connection for search\n   # In your search test cell:\n    with sqlite3.connect(\"/kaggle/working/qna_db.sqlite\", check_same_thread=False) as temp_conn:\n        system.conn = temp_conn\n        results = system.semantic_search(test_query)\n    \n    if not results:\n        logger.warning(\"No results found for test query\")\n    else:\n        logger.info(\"Top 3 results:\")\n        for i, result in enumerate(results[:3], 1):\n            logger.info(f\"{i}. {result['answer']}\")\n            \nexcept Exception as e:\n    logger.error(f\"Search test failed: {str(e)}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:42:20.381040Z","iopub.status.idle":"2025-05-20T21:42:20.381448Z","shell.execute_reply.started":"2025-05-20T21:42:20.381279Z","shell.execute_reply":"2025-05-20T21:42:20.381296Z"}},"outputs":[],"execution_count":null}]}