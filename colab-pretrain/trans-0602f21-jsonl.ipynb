{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:04:13.620400Z","iopub.execute_input":"2025-06-02T21:04:13.625060Z","iopub.status.idle":"2025-06-02T21:04:13.643824Z","shell.execute_reply.started":"2025-06-02T21:04:13.624940Z","shell.execute_reply":"2025-06-02T21:04:13.642347Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cell 1: Complete Environment Setup\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Clear pip cache\n!pip cache purge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:04:13.646473Z","iopub.execute_input":"2025-06-02T21:04:13.646829Z","iopub.status.idle":"2025-06-02T21:04:18.053215Z","shell.execute_reply.started":"2025-06-02T21:04:13.646803Z","shell.execute_reply":"2025-06-02T21:04:18.051825Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1+cu121\nUninstalling torch-2.2.1+cu121:\n  Successfully uninstalled torch-2.2.1+cu121\nFound existing installation: torchvision 0.17.1+cu121\nUninstalling torchvision-0.17.1+cu121:\n  Successfully uninstalled torchvision-0.17.1+cu121\nFound existing installation: torchaudio 2.2.1+cu121\nUninstalling torchaudio-2.2.1+cu121:\n  Successfully uninstalled torchaudio-2.2.1+cu121\nFound existing installation: transformers 4.41.2\nUninstalling transformers-4.41.2:\n  Successfully uninstalled transformers-4.41.2\nFound existing installation: peft 0.10.0\nUninstalling peft-0.10.0:\n  Successfully uninstalled peft-0.10.0\nFound existing installation: bitsandbytes 0.43.0\nUninstalling bitsandbytes-0.43.0:\n  Successfully uninstalled bitsandbytes-0.43.0\nFiles removed: 84\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Install NumPy FIRST with clean environment\n!pip install -q --ignore-installed numpy==1.26.4\n\n# Install PyTorch with CUDA 12.1 (Kaggle's version)\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# Install transformer-related packages with compatible versions\n!pip install -q transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install -q bitsandbytes==0.43.0 einops==0.7.0\n\n# Handle gymnasium separately to avoid conflicts\n!pip install -q gymnasium==0.29.0 --no-deps\n!pip install -U bitsandbytes  # Ensure latest version\n!pip install -U transformers accelerate peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:04:18.055798Z","iopub.execute_input":"2025-06-02T21:04:18.056144Z","iopub.status.idle":"2025-06-02T21:06:20.140898Z","shell.execute_reply.started":"2025-06-02T21:04:18.056106Z","shell.execute_reply":"2025-06-02T21:06:20.139282Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.43.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.2.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2024.2.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3,>=2.2->bitsandbytes)\n  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.2->bitsandbytes) (12.9.41)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.2->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hUsing cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: nvidia-cuda-runtime-cu12, bitsandbytes\n  Attempting uninstall: bitsandbytes\n    Found existing installation: bitsandbytes 0.43.0\n    Uninstalling bitsandbytes-0.43.0:\n      Successfully uninstalled bitsandbytes-0.43.0\n\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed bitsandbytes-0.46.0 nvidia-cuda-runtime-cu12\n\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.41.2)\nCollecting transformers\n  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (0.29.1)\nCollecting accelerate\n  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.10.0)\nCollecting peft\n  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.2.1+cu121)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.9.41)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\nDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hUsing cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: nvidia-cuda-runtime-cu12, tokenizers, transformers, accelerate, peft\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.29.1\n    Uninstalling accelerate-0.29.1:\n      Successfully uninstalled accelerate-0.29.1\n  Attempting uninstall: peft\n\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m    Found existing installation: peft 0.10.0\n    Uninstalling peft-0.10.0:\n      Successfully uninstalled peft-0.10.0\n\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed accelerate-1.7.0 nvidia-cuda-runtime-cu12 peft-0.15.2 tokenizers-0.21.1 transformers-4.52.4\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import sys\nimport os\nimport json\nimport numpy as np\nimport psutil\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Dict, List, Tuple\nfrom collections import defaultdict\nfrom datasets import Dataset, load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    pipeline,\n    GenerationConfig,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom sentence_transformers import util as semantic_util","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:20.143021Z","iopub.execute_input":"2025-06-02T21:06:20.143466Z","iopub.status.idle":"2025-06-02T21:06:20.152506Z","shell.execute_reply.started":"2025-06-02T21:06:20.143427Z","shell.execute_reply":"2025-06-02T21:06:20.151004Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Model Loading\n\nMODEL_NAME = \"gpt2\"  # Default to GPT-2 for Kaggle compatibility\n\ndef load_model(model_name: str):\n    \"\"\"Robust model loading with CPU/GPU handling\"\"\"\n    print(f\"\\n=== Loading {model_name} ===\")\n    \n    # Configure tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Handle quantization based on GPU availability\n    if torch.cuda.is_available():\n        print(\"Configuring for GPU with 4-bit quantization\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n    else:\n        print(\"Configuring for CPU without quantization\")\n        bnb_config = None\n    \n    # Model loading with fallbacks\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        print(\"✅ Model loaded successfully\")\n        print_memory()\n        return model, tokenizer\n    except Exception as e:\n        print(f\"❌ Error loading model: {e}\")\n        print(\"Attempting CPU fallback...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=\"cpu\",\n            torch_dtype=torch.float32\n        )\n        print(\"✅ Model loaded on CPU\")\n        return model, tokenizer\n\n# Load model and tokenizer\nmodel, tokenizer = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:20.155531Z","iopub.execute_input":"2025-06-02T21:06:20.155901Z","iopub.status.idle":"2025-06-02T21:06:26.212969Z","shell.execute_reply.started":"2025-06-02T21:06:20.155876Z","shell.execute_reply":"2025-06-02T21:06:26.210641Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading gpt2 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7228cc9b310b49438bfe1a14c3bbf6a5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ce661075c245eb889859028d4764a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba4a0ed045f4dad8bb0fa5041a1179a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a44524132df47bc88e28b281d0496cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a2ebf0fcf74f0ab51f08b4bf7e2f0c"}},"metadata":{}},{"name":"stdout","text":"Configuring for CPU without quantization\n❌ Error loading model: Failed to import transformers.models.mega.configuration_mega because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mega.configuration_mega'\nAttempting CPU fallback...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m \u001b[0mHowever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mable\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfind\u001b[0m \u001b[0ma\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0minstallation\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m\"TF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mare\u001b[0m \u001b[0motherwise\u001b[0m \u001b[0midentically\u001b[0m \u001b[0mnamed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mour\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.mega.configuration_mega'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_112/2854792492.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;31m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwargs_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mconfig_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;34m-\u001b[0m \u001b[0mconfig_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    749\u001b[0m     \u001b[0;31m\"\u001b[0m \u001b[0mA\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mto\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mload\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maccessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m \u001b[0;31m# docstyle-ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m TORCHVISION_IMPORT_ERROR = \"\"\"\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m\"TF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mare\u001b[0m \u001b[0motherwise\u001b[0m \u001b[0midentically\u001b[0m \u001b[0mnamed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mour\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m \u001b[0mmeans\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mTF\u001b[0m \u001b[0mequivalent\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0myou\u001b[0m \u001b[0mtried\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;34m\"TF{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mTensorFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplease\u001b[0m \u001b[0muse\u001b[0m \u001b[0mTF\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;31m!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.mega.configuration_mega because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mega.configuration_mega'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m \u001b[0mHowever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mable\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfind\u001b[0m \u001b[0ma\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0minstallation\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m\"TF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mare\u001b[0m \u001b[0motherwise\u001b[0m \u001b[0midentically\u001b[0m \u001b[0mnamed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mour\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.mega.configuration_mega'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_112/2854792492.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Load model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_112/2854792492.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"❌ Error loading model: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting CPU fallback...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m             )\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;31m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwargs_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"torch_dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m\"\u001b[0m \u001b[0mA\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mto\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mload\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maccessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mconfig_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;34m-\u001b[0m \u001b[0mconfig_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmap\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \"\"\"\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    749\u001b[0m     \u001b[0;31m\"\u001b[0m \u001b[0mA\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mto\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mload\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maccessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m \u001b[0;31m# docstyle-ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m TORCHVISION_IMPORT_ERROR = \"\"\"\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mTorchvision\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mCheckout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstructions\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0mHowever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mable\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfind\u001b[0m \u001b[0ma\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0minstallation\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m\"TF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mare\u001b[0m \u001b[0motherwise\u001b[0m \u001b[0midentically\u001b[0m \u001b[0mnamed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mour\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m \u001b[0mmeans\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mTF\u001b[0m \u001b[0mequivalent\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0myou\u001b[0m \u001b[0mtried\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;34m\"TF{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mTensorFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplease\u001b[0m \u001b[0muse\u001b[0m \u001b[0mTF\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;31m!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.mega.configuration_mega because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mega.configuration_mega'"],"ename":"RuntimeError","evalue":"Failed to import transformers.models.mega.configuration_mega because of the following error (look up to see its traceback):\nNo module named 'transformers.models.mega.configuration_mega'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# Tokenizer Setup\n# =====================\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.213808Z","iopub.status.idle":"2025-06-02T21:06:26.214205Z","shell.execute_reply.started":"2025-06-02T21:06:26.213985Z","shell.execute_reply":"2025-06-02T21:06:26.213996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Loading Tokenizer ===\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# NEW: Analyze vocabulary coverage\nfrom data_quality import analyze_vocab_coverage\nvocab_report = analyze_vocab_coverage(tokenizer)\nprint(f\"Tokenizer vocabulary coverage: {vocab_report['coverage']:.1%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.215545Z","iopub.status.idle":"2025-06-02T21:06:26.215855Z","shell.execute_reply.started":"2025-06-02T21:06:26.215723Z","shell.execute_reply":"2025-06-02T21:06:26.215735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preparation\n\ndef analyze_dataset(dataset, tokenizer) -> Dict:\n    \"\"\"Comprehensive dataset quality analysis\"\"\"\n    # Vocabulary coverage\n    vocab = set(tokenizer.get_vocab().keys())\n    dataset_tokens = set()\n    text_lengths = []\n    \n    for example in dataset:\n        tokens = tokenizer.tokenize(example['text'])\n        dataset_tokens.update(tokens)\n        text_lengths.append(len(tokens))\n    \n    coverage = len(dataset_tokens & vocab) / len(vocab)\n    \n    # Statistical analysis\n    length_stats = {\n        'mean': np.mean(text_lengths),\n        'std': np.std(text_lengths),\n        'min': min(text_lengths),\n        'max': max(text_lengths),\n        'percentiles': np.percentile(text_lengths, [25, 50, 75])\n    }\n    \n    # Topic diversity (simple heuristic)\n    topics = defaultdict(int)\n    for text in dataset['text']:\n        for term in ['blockchain', 'wallet', 'mining', 'crypto', 'token']:\n            if term in text.lower():\n                topics[term] += 1\n    \n    return {\n        'vocab_coverage': round(coverage, 4),\n        'length_stats': length_stats,\n        'topic_distribution': dict(topics),\n        'total_samples': len(dataset)\n    }\n\ndef stratified_sample(dataset, stratify_by: str = 'label', n_samples: int = None) -> Dataset:\n    \"\"\"Stratified sampling for small datasets\"\"\"\n    if n_samples is None:\n        n_samples = min(1000, len(dataset))\n    \n    if stratify_by not in dataset.features:\n        return dataset.select(range(n_samples))\n    \n    from sklearn.model_selection import train_test_split\n    import pandas as pd\n    \n    df = pd.DataFrame(dataset)\n    _, sample = train_test_split(\n        df,\n        train_size=n_samples,\n        stratify=df[stratify_by],\n        random_state=42\n    )\n    return Dataset.from_pandas(sample)\n\ndef prepare_dataset(file_path: str, max_samples: int = 1000) -> Dataset:\n    \"\"\"Robust dataset preparation with quality checks\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Dataset path not found: {file_path}\")\n            \n        dataset = load_dataset('json', data_files=file_path, split=f'train[:{max_samples}]')\n        \n        # Standardize text column\n        if 'text' not in dataset.features:\n            text_cols = [c for c in dataset.features if 'text' in c.lower()]\n            dataset = dataset.rename_column(text_cols[0], 'text') if text_cols else dataset\n        \n        # Quality analysis\n        quality_report = analyze_dataset(dataset, tokenizer)\n        print(\"Dataset Quality Report:\")\n        for k, v in quality_report.items():\n            print(f\"- {k}: {v}\")\n            \n        return dataset\n        \n    except Exception as e:\n        print(f\"Error preparing dataset: {e}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\n            \"Blockchain is a decentralized ledger technology.\",\n            \"Cryptocurrencies use public-key cryptography.\",\n            \"Proof of Work requires computational effort.\",\n            \"Hardware wallets provide secure key storage.\"\n        ]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.217064Z","iopub.status.idle":"2025-06-02T21:06:26.218064Z","shell.execute_reply.started":"2025-06-02T21:06:26.217873Z","shell.execute_reply":"2025-06-02T21:06:26.217895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Configuration\n\ndef suggest_hyperparameters(model, dataset) -> Dict:\n    \"\"\"Auto-suggest training parameters based on model and data\"\"\"\n    params = {\n        'batch_size': max(1, min(8, len(dataset) // 100)),\n        'learning_rate': 2e-5,\n        'epochs': 1 if len(dataset) < 1000 else 3,\n        'grad_accum': max(1, 32 // suggested_batch_size)\n    }\n    \n    # Adjust for model size\n    num_params = sum(p.numel() for p in model.parameters())\n    if num_params > 1e9:  # Large model\n        params['learning_rate'] /= 2\n        params['batch_size'] = max(1, params['batch_size'] // 2)\n    \n    return params\n\ndef configure_training(model) -> Tuple:\n    \"\"\"Complete training configuration with LoRA\"\"\"\n    # LoRA setup\n    peft_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,\n        learning_rate=2e-5,\n        fp16=torch.cuda.is_available(),\n        logging_steps=10,\n        save_strategy=\"steps\",\n        save_steps=500,\n        report_to=\"none\"\n    )\n    \n    # Prepare model\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n    \n    return model, training_args\n\nclass TrainingMonitor:\n    \"\"\"Real-time training monitoring\"\"\"\n    def __init__(self):\n        self.metrics = defaultdict(list)\n        self.start_time = time.time()\n        \n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            self.metrics[k].append(v)\n        \n    def display_dashboard(self):\n        clear_output(wait=True)\n        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n        \n        # Loss plot\n        axes[0].plot(self.metrics.get('loss', []))\n        axes[0].set_title(\"Training Loss\")\n        \n        # Grad norm\n        axes[1].plot(self.metrics.get('grad_norm', []))\n        axes[1].set_title(\"Gradient Norm\")\n        \n        # Hardware\n        axes[2].bar(['CPU', 'GPU', 'RAM'],\n                  [psutil.cpu_percent(),\n                   get_gpu_usage(),\n                   psutil.virtual_memory().percent])\n        axes[2].set_title(\"Hardware Usage\")\n        \n        plt.tight_layout()\n        plt.show()\n        \n    def should_stop_early(self, patience=3) -> bool:\n        \"\"\"Early stopping check\"\"\"\n        losses = self.metrics.get('loss', [])\n        if len(losses) < patience * 2:\n            return False\n        return losses[-1] > np.mean(losses[-patience*2:-patience])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.218909Z","iopub.status.idle":"2025-06-02T21:06:26.219230Z","shell.execute_reply.started":"2025-06-02T21:06:26.219080Z","shell.execute_reply":"2025-06-02T21:06:26.219094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Execution\n# =====================\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n                             'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n                             'labels': torch.stack([f['input_ids'] for f in data])}\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ndef generate_contrastive_examples(example):\n    \"\"\"Generate contrastive examples for training\"\"\"\n    # Generate negative sample by:\n    # 1. Random Q/A from different category\n    # 2. GPT-generated incorrect answer\n    # 3. Perturbed correct answer\n    return {\n        'anchor': example['answer'],\n        'positive': augment_answer(example['answer']),\n        'negative': get_negative_sample(example)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.220736Z","iopub.status.idle":"2025-06-02T21:06:26.221160Z","shell.execute_reply.started":"2025-06-02T21:06:26.220955Z","shell.execute_reply":"2025-06-02T21:06:26.220974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Saving\n# =====================\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"💽 Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n⚠️ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.223380Z","iopub.status.idle":"2025-06-02T21:06:26.223925Z","shell.execute_reply.started":"2025-06-02T21:06:26.223713Z","shell.execute_reply":"2025-06-02T21:06:26.223734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Loading and Testing\n# =====================\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 250,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline - REMOVED device parameter since we're using device_map=\"auto\"\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"💬 Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.225824Z","iopub.status.idle":"2025-06-02T21:06:26.226381Z","shell.execute_reply.started":"2025-06-02T21:06:26.226052Z","shell.execute_reply":"2025-06-02T21:06:26.226068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EnhancedModelWrapper:\n    \"\"\"Advanced wrapper for constrained generation with technical enforcement\"\"\"\n    \n    def __init__(self, model, tokenizer, knowledge_base: Optional[Dict] = None):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.required_terms = []\n        self.complete_sentences = True\n        self.technical_terms = knowledge_base or {\n            'blockchain': ['decentralized', 'immutable', 'consensus', 'ledger'],\n            'wallet': ['private key', 'public key', 'address', 'security'],\n            'PoW': ['mining', 'difficulty', 'hash', 'computational'],\n            'cryptography': ['encryption', 'signature', 'asymmetric', 'algorithm'],\n            'P2P': ['network', 'nodes', 'direct', 'decentralized']\n        }\n        self.banned_phrases = [\n            \"I don't know\", \"as an AI\", \"I'm not sure\",\n            \"I can't answer\", \"my training data\"\n        ]\n\n    def set_constraints(self, \n                      required_terms: List[str] = None,\n                      complete_sentences: bool = True,\n                      technical_focus: str = None):\n        \"\"\"Configure generation constraints\"\"\"\n        self.required_terms = required_terms or []\n        self.complete_sentences = complete_sentences\n        \n        if technical_focus:\n            self.required_terms.extend(self.technical_terms.get(technical_focus, []))\n\n    def generate(self, \n                prompt: str,\n                max_length: int = 200,\n                temperature: float = 0.7,\n                **kwargs) -> Dict:\n        \"\"\"Generate response with multiple validation layers\"\"\"\n        \n        # Create generation config\n        gen_config = GenerationConfig(\n            max_length=max_length,\n            temperature=temperature,\n            do_sample=True,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            pad_token_id=self.tokenizer.eos_token_id,\n            **kwargs\n        )\n        \n        # Generate raw output\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        outputs = self.model.generate(**inputs, generation_config=gen_config)\n        raw_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Apply processing pipeline\n        processed_text = self._process_output(prompt, raw_text)\n        \n        # Validate and score\n        validation = self._validate_response(prompt, processed_text)\n        \n        return {\n            'raw': raw_text,\n            'processed': processed_text,\n            'validation': validation,\n            'prompt_analysis': self._analyze_prompt(prompt)\n        }\n\n    def _process_output(self, prompt: str, text: str) -> str:\n        \"\"\"Apply all text processing constraints\"\"\"\n        # Remove prompt from output\n        if text.startswith(prompt):\n            text = text[len(prompt):].strip()\n        \n        # Apply term enforcement\n        if self.required_terms:\n            text = self._enforce_terms(text)\n        \n        # Complete sentences\n        if self.complete_sentences:\n            text = self._complete_sentences(text)\n            \n        # Remove banned phrases\n        for phrase in self.banned_phrases:\n            text = text.replace(phrase, \"\")\n            \n        return text.strip()\n\n    def _enforce_terms(self, text: str) -> str:\n        \"\"\"Ensure required technical terms are present\"\"\"\n        missing = [t for t in self.required_terms \n                  if not re.search(rf'\\b{re.escape(t)}\\b', text, re.IGNORECASE)]\n        \n        if missing:\n            # Try to naturally incorporate missing terms\n            additions = []\n            for term in missing:\n                if term in self.technical_terms:\n                    addition = f\" {term} is important because {self._explain_term(term)}.\"\n                    additions.append(addition)\n            \n            text += ''.join(additions) if additions else f\"\\n\\n[Missing terms: {', '.join(missing)}]\"\n        \n        return text\n\n    def _complete_sentences(self, text: str) -> str:\n        \"\"\"Ensure output ends with complete sentence\"\"\"\n        # Find last sentence boundary\n        last_boundary = max(\n            text.rfind('.'), \n            text.rfind('!'), \n            text.rfind('?'),\n            text.rfind('\\n')\n        )\n        \n        if last_boundary > 0 and len(text) - last_boundary < 50:\n            text = text[:last_boundary+1]\n            \n        # If no proper ending, add one\n        if text and text[-1] not in {'.', '!', '?'}:\n            text += '.' if not text.endswith(',') else '..'\n            \n        return text\n\n    def _validate_response(self, prompt: str, response: str) -> Dict:\n        \"\"\"Comprehensive quality validation\"\"\"\n        # Detect topic from prompt\n        topic = next((t for t in self.technical_terms \n                     if re.search(rf'\\b{t}\\b', prompt, re.IGNORECASE)), None)\n        \n        # Check technical terms\n        missing_terms = []\n        if topic:\n            missing_terms = [t for t in self.technical_terms[topic]\n                          if not re.search(rf'\\b{re.escape(t)}\\b', response, re.IGNORECASE)]\n        \n        # Check for hallucinations\n        hallucinations = any(\n            phrase.lower() in response.lower() \n            for phrase in self.banned_phrases\n        )\n        \n        # Calculate scores\n        tech_score = 1 - (len(missing_terms) / len(self.technical_terms.get(topic, ['']))\n        clarity_score = min(1, len(response.split()) / 50)  # Normalize to 0-1\n        \n        return {\n            'technical_score': tech_score,\n            'clarity_score': clarity_score,\n            'missing_terms': missing_terms,\n            'has_hallucinations': hallucinations,\n            'is_complete': response[-1] in {'.', '!', '?'}\n        }\n\n    def _analyze_prompt(self, prompt: str) -> Dict:\n        \"\"\"Evaluate prompt quality\"\"\"\n        return {\n            'length': len(prompt.split()),\n            'has_question': '?' in prompt,\n            'technical_focus': any(\n                term in prompt.lower() \n                for term in self.technical_terms\n            ),\n            'specificity': len(set(prompt.split())) / len(prompt.split())  # Unique words ratio\n        }\n\n    def _explain_term(self, term: str) -> str:\n        \"\"\"Generate simple explanations for technical terms\"\"\"\n        explanations = {\n            'blockchain': \"it enables secure decentralized record-keeping\",\n            'private key': \"it provides secure access to cryptocurrency funds\",\n            'mining': \"it secures the network through computational work\",\n            'encryption': \"it protects data through mathematical algorithms\"\n        }\n        return explanations.get(term, f\"it's a fundamental concept in cryptocurrency\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.228067Z","iopub.status.idle":"2025-06-02T21:06:26.228487Z","shell.execute_reply.started":"2025-06-02T21:06:26.228261Z","shell.execute_reply":"2025-06-02T21:06:26.228280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enhanced Generation\n# =====================\nCRYPTO_GENERATION_CONFIG = GenerationConfig(\n    max_new_tokens=150,\n    no_repeat_ngram_size=4,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=40,\n    repetition_penalty=1.15,\n    num_beams=3,\n    early_stopping=True\n)\n\ndef generate_with_validation(model, tokenizer, prompt, max_length=200):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # First pass generation\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        generation_config=CRYPTO_GENERATION_CONFIG\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Validation checks\n    validation_passed = True\n    validation_notes = []\n    \n    # 1. Technical term check\n    last_term = model.get_last_term(prompt)\n    if last_term in model.technical_terms:\n        missing = [t for t in model.technical_terms[last_term] \n                  if t.lower() not in response.lower()]\n        if missing:\n            validation_passed = False\n            validation_notes.append(f\"Missing technical terms: {missing}\")\n    \n    # 2. Hallucination check\n    if any(phrase in response for phrase in model.banned_sequences):\n        validation_passed = False\n        validation_notes.append(\"Potential hallucination\")\n    \n    # Generate final output\n    if not validation_passed:\n        print(f\"⚠️ Validation issues: {validation_notes}\")\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            generation_config=CRYPTO_GENERATION_CONFIG,\n            bad_words_ids=[[tid] for tid in tokenizer.encode(\" \".join(model.banned_sequences), add_special_tokens=False)]\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return {\n        'response': response,\n        'validation_passed': validation_passed,\n        'validation_notes': validation_notes\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.229174Z","iopub.status.idle":"2025-06-02T21:06:26.229616Z","shell.execute_reply.started":"2025-06-02T21:06:26.229464Z","shell.execute_reply":"2025-06-02T21:06:26.229478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Configuration\n    DATASET_PATH = \"/kaggle/input/database2\"\n    OUTPUT_DIR = \"/kaggle/working/output\"\n    \n    try:\n        # 1. Data Preparation\n        print(\"\\n=== Preparing Dataset ===\")\n        def prepare_dataset():\n            try:\n                dataset = load_dataset('json', data_files=DATASET_PATH)['train']\n                print(f\"Loaded {len(dataset)} samples\")\n                \n                # Simple quality check\n                if len(dataset) < 10:\n                    raise ValueError(\"Dataset too small, using fallback\")\n                    \n                return dataset\n            except Exception as e:\n                print(f\"Error loading dataset: {e}\")\n                return Dataset.from_dict({\n                    \"text\": [\n                        \"Blockchain is a decentralized ledger technology.\",\n                        \"Cryptocurrencies use cryptographic keys for security.\",\n                        \"Proof of Work requires computational resources.\"\n                    ]\n                })\n        \n        dataset = prepare_dataset()\n        \n        # Tokenization\n        def tokenize(examples):\n            return tokenizer(\n                examples[\"text\"],\n                truncation=True,\n                max_length=128,\n                padding=\"max_length\"\n            )\n            \n        tokenized_dataset = dataset.map(tokenize, batched=True)\n        tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n        \n        # 2. Training Configuration\n        print(\"\\n=== Configuring Training ===\")\n        def configure_training(model):\n            # LoRA configuration\n            peft_config = LoraConfig(\n                r=8,\n                lora_alpha=16,\n                target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n                lora_dropout=0.05,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\"\n            )\n            \n            # Training arguments\n            training_args = TrainingArguments(\n                output_dir=OUTPUT_DIR,\n                per_device_train_batch_size=2,\n                gradient_accumulation_steps=4,\n                num_train_epochs=1,\n                learning_rate=2e-5,\n                fp16=torch.cuda.is_available(),\n                save_strategy=\"steps\",\n                save_steps=500,\n                report_to=\"none\"\n            )\n            \n            model = get_peft_model(model, peft_config)\n            model.print_trainable_parameters()\n            return model, training_args\n        \n        model, training_args = configure_training(model)\n        \n        # 3. Training Execution\n        print(\"\\n=== Starting Training ===\")\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset,\n            data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n        )\n        \n        trainer.train()\n        \n        # 4. Saving Model\n        print(\"\\n=== Saving Model ===\")\n        trainer.save_model(OUTPUT_DIR)\n        print(f\"Model saved to {OUTPUT_DIR}\")\n        \n        # 5. Testing\n        print(\"\\n=== Testing Model ===\")\n        test_prompts = [\n            \"Explain blockchain in simple terms:\",\n            \"What is the difference between hardware and software wallets?\"\n        ]\n        \n        for prompt in test_prompts:\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n            outputs = model.generate(**inputs, max_length=100)\n            print(f\"\\nPrompt: {prompt}\")\n            print(\"Response:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n            \n    except Exception as e:\n        print(f\"\\n❌ Error in training: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.230432Z","iopub.status.idle":"2025-06-02T21:06:26.230721Z","shell.execute_reply.started":"2025-06-02T21:06:26.230584Z","shell.execute_reply":"2025-06-02T21:06:26.230596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:06:26.232223Z","iopub.status.idle":"2025-06-02T21:06:26.232640Z","shell.execute_reply.started":"2025-06-02T21:06:26.232434Z","shell.execute_reply":"2025-06-02T21:06:26.232450Z"}},"outputs":[],"execution_count":null}]}