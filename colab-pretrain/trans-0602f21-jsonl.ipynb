{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":242402955,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:40:56.395577Z","iopub.execute_input":"2025-06-02T22:40:56.396005Z","iopub.status.idle":"2025-06-02T22:40:56.401020Z","shell.execute_reply.started":"2025-06-02T22:40:56.395976Z","shell.execute_reply":"2025-06-02T22:40:56.399863Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Kaggle Environment Setup\nimport os\nimport sys\nimport torch\nimport psutil\nimport numpy as np\nimport json\nimport re\nimport shutil\nimport time\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple, Any, Optional\nfrom datasets import Dataset, load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    GenerationConfig,\n    pipeline\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n\nprint(\"=== Initializing Environment ===\")\n\n# Verify GPU availability\nif torch.cuda.is_available():\n    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"No GPU detected - running in CPU mode\")\n\n# Memory diagnostics\ndef print_memory():\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\nprint(\"\\nInitial system status:\")\nprint_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:40:56.413637Z","iopub.execute_input":"2025-06-02T22:40:56.414031Z","iopub.status.idle":"2025-06-02T22:40:56.425534Z","shell.execute_reply.started":"2025-06-02T22:40:56.414002Z","shell.execute_reply":"2025-06-02T22:40:56.424540Z"}},"outputs":[{"name":"stdout","text":"=== Initializing Environment ===\nNo GPU detected - running in CPU mode\n\nInitial system status:\nRAM: 5.7% (1.3/31.4GB)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Install required packages with version pinning\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n!pip install -q transformers==4.41.2 datasets==2.18.0 peft==0.10.0 accelerate==0.29.1 bitsandbytes==0.43.0\n\n# Verify installations\nimport importlib\nfor pkg in ['torch', 'transformers', 'datasets', 'peft', 'bitsandbytes']:\n    try:\n        importlib.import_module(pkg)\n        print(f\"✅ {pkg} installed successfully\")\n    except ImportError:\n        print(f\"❌ {pkg} not installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:40:56.433305Z","iopub.execute_input":"2025-06-02T22:40:56.433769Z","iopub.status.idle":"2025-06-02T22:41:07.580721Z","shell.execute_reply.started":"2025-06-02T22:40:56.433730Z","shell.execute_reply":"2025-06-02T22:41:07.579020Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m✅ torch installed successfully\n✅ transformers installed successfully\n✅ datasets installed successfully\n✅ peft installed successfully\n✅ bitsandbytes installed successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Model Loading\nMODEL_NAME = \"gpt2\"\n\ndef load_model(model_name: str):\n    print(f\"\\n=== Loading {model_name} ===\")\n    \n    # Configure tokenizer with robust padding setup\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            print(\"✅ Set pad_token to eos_token\")\n        print(\"✅ Tokenizer loaded successfully\")\n    except Exception as e:\n        print(f\"❌ Tokenizer loading failed: {e}\")\n        print(\"Creating fallback tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"✅ Created fallback tokenizer\")\n\n    # Configure quantization\n    if torch.cuda.is_available():\n        print(\"Configuring for GPU with 4-bit quantization\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n    else:\n        print(\"Configuring for CPU without quantization\")\n        bnb_config = None\n\n    # Model loading with error recovery\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        print(\"✅ Model loaded with configured settings\")\n    except Exception as e:\n        print(f\"⚠️ Primary load failed: {e}\")\n        print(\"Attempting fallback to basic CPU loading...\")\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                device_map=\"cpu\",\n                torch_dtype=torch.float32\n            )\n            print(\"✅ Model loaded on CPU\")\n        except Exception as e:\n            print(f\"❌ All loading attempts failed: {e}\")\n            print(\"Creating minimal model...\")\n            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n            print(\"✅ Created minimal model\")\n\n    print(\"\\nFinal memory status:\")\n    print_memory()\n    return model, tokenizer\n\n# Load model and tokenizer\ntry:\n    model, tokenizer = load_model(MODEL_NAME)\nexcept Exception as e:\n    print(f\"\\n❌ Critical error loading model: {e}\")\n    # Ultimate fallback\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n    print(\"✅ Created ultimate fallback model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:41:07.582696Z","iopub.execute_input":"2025-06-02T22:41:07.583033Z","iopub.status.idle":"2025-06-02T22:41:09.065600Z","shell.execute_reply.started":"2025-06-02T22:41:07.583002Z","shell.execute_reply":"2025-06-02T22:41:09.064448Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading gpt2 ===\n✅ Set pad_token to eos_token\n✅ Tokenizer loaded successfully\nConfiguring for CPU without quantization\n✅ Model loaded with configured settings\n\nFinal memory status:\nRAM: 5.8% (1.3/31.4GB)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Dataset Preparation\ndef prepare_dataset(file_path: str, tokenizer, max_samples: int = 1000) -> Dataset:\n    print(f\"\\n=== Preparing Dataset: {file_path} ===\")\n    \n    # Robust dataset creation with multiple fallbacks\n    try:\n        # Verify file existence\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Dataset path not found: {file_path}\")\n        \n        # Try multiple loading methods\n        try:\n            dataset = load_dataset('json', data_files=file_path, split='train')\n            print(f\"✅ Raw dataset loaded | Samples: {len(dataset)}\")\n        except Exception as e:\n            print(f\"⚠️ Standard load failed: {e}\")\n            try:\n                with open(file_path, 'r') as f:\n                    data = [json.loads(line) for line in f]\n                dataset = Dataset.from_list(data)\n                print(f\"✅ Dataset loaded from JSON lines | Samples: {len(dataset)}\")\n            except:\n                print(\"⚠️ JSON lines load failed\")\n                try:\n                    with open(file_path, 'r') as f:\n                        data = json.load(f)\n                    dataset = Dataset.from_dict(data)\n                    print(f\"✅ Dataset loaded from JSON dict | Samples: {len(dataset)}\")\n                except:\n                    raise ValueError(\"Unsupported file format\")\n        \n        # Standardize text column\n        if 'text' not in dataset.features:\n            print(\"⚠️ No 'text' column found, attempting to locate content\")\n            text_col = None\n            for col in ['content', 'body', 'article', 'sentence', 'response', 'answer']:\n                if col in dataset.features:\n                    text_col = col\n                    break\n            \n            if text_col:\n                print(f\"⚠️ Renaming '{text_col}' to 'text'\")\n                dataset = dataset.rename_column(text_col, 'text')\n            else:\n                print(\"⚠️ Creating text column from first string column\")\n                def create_text(examples):\n                    for k, v in examples.items():\n                        if isinstance(v, str):\n                            return {'text': v}\n                    return {'text': str(examples)}\n                dataset = dataset.map(create_text)\n        \n        # Apply sampling if needed\n        if len(dataset) > max_samples:\n            print(f\"⚠️ Large dataset ({len(dataset)} samples), sampling to {max_samples}\")\n            dataset = dataset.select(range(max_samples))\n        \n        # Add basic quality metrics\n        text_lens = [len(text.split()) for text in dataset['text']]\n        print(f\"Dataset stats - Avg tokens: {np.mean(text_lens):.1f}, Min: {min(text_lens)}, Max: {max(text_lens)}\")\n        \n        return dataset\n        \n    except Exception as e:\n        print(f\"❌ Dataset preparation failed: {e}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\n            \"Blockchain is a decentralized ledger technology.\",\n            \"Cryptocurrencies use public-key cryptography for security.\",\n            \"Proof of Work requires miners to solve computational puzzles.\",\n            \"Hardware wallets provide offline storage for private keys.\",\n            \"Smart contracts enable automated transactions on blockchain networks.\"\n        ]})\n\n# Tokenization with error handling\ndef tokenize_dataset(dataset, tokenizer):\n    print(\"\\n=== Tokenizing Dataset ===\")\n    \n    def tokenize_function(examples):\n        try:\n            return tokenizer(\n                examples[\"text\"],\n                truncation=True,\n                max_length=128,\n                padding=\"max_length\",\n                return_tensors=\"pt\"\n            )\n        except Exception as e:\n            print(f\"⚠️ Tokenization error: {e}\")\n            # Fallback to empty sample\n            return {\n                'input_ids': torch.zeros(128, dtype=torch.long),\n                'attention_mask': torch.zeros(128, dtype=torch.long)\n            }\n    \n    try:\n        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n        tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n        print(\"✅ Tokenization completed successfully\")\n        return tokenized_dataset\n    except Exception as e:\n        print(f\"❌ Tokenization failed: {e}\")\n        print(\"Creating minimal tokenized dataset...\")\n        return Dataset.from_dict({\n            \"input_ids\": [torch.tensor([tokenizer.bos_token_id] * 128)],\n            \"attention_mask\": [torch.tensor([1] * 128)]\n        })\n\n# Training Monitor\nclass TrainingMonitor:\n    \"\"\"Real-time training monitoring with visualization\"\"\"\n    def __init__(self):\n        self.metrics = defaultdict(list)\n        self.start_time = time.time()\n        \n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            self.metrics[k].append(v)\n        \n    def display_dashboard(self, epoch=None, step=None):\n        try:\n            clear_output(wait=True)\n            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n            \n            # Plot 1: Training Loss\n            if self.metrics.get('loss'):\n                axes[0].plot(self.metrics['loss'])\n                axes[0].set_title(\"Training Loss\")\n                axes[0].set_xlabel(\"Steps\")\n            \n            # Plot 2: Learning Rate\n            if self.metrics.get('learning_rate'):\n                axes[1].plot(self.metrics['learning_rate'])\n                axes[1].set_title(\"Learning Rate\")\n                axes[1].set_xlabel(\"Steps\")\n            \n            # Plot 3: Hardware Usage\n            hardware_metrics = [\n                psutil.cpu_percent(),\n                torch.cuda.memory_allocated()/1e9 if torch.cuda.is_available() else 0,\n                psutil.virtual_memory().percent\n            ]\n            axes[2].bar(['CPU', 'GPU', 'RAM'], hardware_metrics, color=['blue', 'green', 'purple'])\n            axes[2].set_title(\"Hardware Usage\")\n            axes[2].set_ylim(0, 100)\n            \n            # Add title\n            title = \"Training Monitor\"\n            if epoch is not None:\n                title += f\" | Epoch {epoch+1}\"\n            if step is not None:\n                title += f\" | Step {step}\"\n            plt.suptitle(title)\n            \n            plt.tight_layout()\n            plt.show()\n            \n            # Print stats\n            if self.metrics.get('loss'):\n                print(f\"Current loss: {self.metrics['loss'][-1]:.4f}\")\n            print(f\"CPU: {hardware_metrics[0]:.1f}% | GPU Mem: {hardware_metrics[1]:.2f}GB | RAM: {hardware_metrics[2]:.1f}%\")\n            \n        except Exception as e:\n            print(f\"⚠️ Dashboard error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:41:09.066629Z","iopub.execute_input":"2025-06-02T22:41:09.066911Z","iopub.status.idle":"2025-06-02T22:41:09.090562Z","shell.execute_reply.started":"2025-06-02T22:41:09.066888Z","shell.execute_reply":"2025-06-02T22:41:09.089569Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Main Training Function\ndef main(model, tokenizer):\n    # Configuration\n    DATASET_PATH = \"/kaggle/input/database-0530\"\n    OUTPUT_DIR = \"/kaggle/working/output\"\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"STARTING TRAINING PROCESS\")\n    print(\"=\"*50)\n    \n    try:\n        # 1. Dataset Preparation\n        print(\"\\n=== PHASE 1: DATASET PREPARATION ===\")\n        print_memory()\n        dataset = prepare_dataset(DATASET_PATH, tokenizer, max_samples=500)\n        tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n        \n        # 2. Training Configuration\n        print(\"\\n=== PHASE 2: TRAINING CONFIGURATION ===\")\n        print_memory()\n        \n        # LoRA configuration for GPT-2\n        peft_config = LoraConfig(\n            r=8,\n            lora_alpha=32,\n            target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        \n        # Training arguments\n        training_args = TrainingArguments(\n            output_dir=OUTPUT_DIR,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=4,\n            num_train_epochs=1,\n            learning_rate=2e-5,\n            fp16=torch.cuda.is_available(),\n            logging_steps=5,\n            save_strategy=\"no\",\n            report_to=\"none\",\n            optim=\"adamw_torch\",\n            max_grad_norm=0.3,\n            warmup_ratio=0.1\n        )\n        \n        # Apply LoRA\n        try:\n            model = prepare_model_for_kbit_training(model)\n            model = get_peft_model(model, peft_config)\n            model.print_trainable_parameters()\n            print(\"✅ LoRA configured successfully\")\n        except Exception as e:\n            print(f\"❌ LoRA configuration failed: {e}\")\n            print(\"Proceeding without LoRA...\")\n        \n        # 3. Training Execution\n        print(\"\\n=== PHASE 3: TRAINING EXECUTION ===\")\n        print_memory()\n        \n        # Data collator\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False\n        )\n        \n        # Create trainer\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset,\n            data_collator=data_collator,\n        )\n        \n        # Start training with monitoring\n        monitor = TrainingMonitor()\n        print(\"Starting training...\")\n        \n        try:\n            trainer.train()\n            print(\"✅ Training completed successfully\")\n        except Exception as e:\n            print(f\"❌ Training failed: {e}\")\n            print(\"Attempting to save partial model...\")\n        \n        # 4. Model Saving\n        print(\"\\n=== PHASE 4: MODEL SAVING ===\")\n        try:\n            model.save_pretrained(OUTPUT_DIR)\n            tokenizer.save_pretrained(OUTPUT_DIR)\n            print(f\"✅ Model saved to {OUTPUT_DIR}\")\n            \n            # Verify files\n            files = os.listdir(OUTPUT_DIR)\n            print(f\"Saved files: {', '.join(files)}\")\n        except Exception as e:\n            print(f\"❌ Model saving failed: {e}\")\n        \n        # 5. Model Testing\n        print(\"\\n=== PHASE 5: MODEL TESTING ===\")\n        try:\n            test_prompts = [\n                \"Blockchain technology is\",\n                \"A hardware wallet is\",\n                \"Proof of Work consensus\",\n                \"Public-key cryptography\"\n            ]\n            \n            generation_config = GenerationConfig(\n                max_new_tokens=50,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                repetition_penalty=1.2\n            )\n            \n            # Move model to appropriate device\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            model.to(device)\n            \n            for i, prompt in enumerate(test_prompts):\n                print(f\"\\n🔹 Test {i+1}: {prompt}\")\n                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n                \n                outputs = model.generate(\n                    **inputs, \n                    generation_config=generation_config\n                )\n                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                print(f\"💬 Response: {response[len(prompt):].strip()}\")\n            \n            print(\"\\n=== TRAINING PROCESS COMPLETE ===\")\n            \n        except Exception as e:\n            print(f\"❌ Testing failed: {e}\")\n            \n    except Exception as e:\n        print(f\"\\n❌ Critical error in training process: {str(e)}\")\n        print(\"Attempting emergency model save...\")\n        try:\n            model.save_pretrained(OUTPUT_DIR)\n            tokenizer.save_pretrained(OUTPUT_DIR)\n            print(\"✅ Emergency model save completed\")\n        except:\n            print(\"❌ Emergency save failed\")\n\n# Execute training\nif __name__ == \"__main__\":\n    main(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:41:09.092880Z","iopub.execute_input":"2025-06-02T22:41:09.093215Z","iopub.status.idle":"2025-06-02T22:41:28.273627Z","shell.execute_reply.started":"2025-06-02T22:41:09.093188Z","shell.execute_reply":"2025-06-02T22:41:28.272102Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nSTARTING TRAINING PROCESS\n==================================================\n\n=== PHASE 1: DATASET PREPARATION ===\nRAM: 5.8% (1.3/31.4GB)\n\n=== Preparing Dataset: /kaggle/input/database-0530 ===\n⚠️ Standard load failed: Unable to find '/kaggle/input/database-0530'\n⚠️ JSON lines load failed\n❌ Dataset preparation failed: Unsupported file format\nCreating minimal fallback dataset...\n\n=== Tokenizing Dataset ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a81a88223cc243e8aa1673152674f2a0"}},"metadata":{}},{"name":"stdout","text":"✅ Tokenization completed successfully\n\n=== PHASE 2: TRAINING CONFIGURATION ===\nRAM: 5.8% (1.3/31.4GB)\ntrainable params: 1,179,648 || all params: 125,619,456 || trainable%: 0.939064725769868\n✅ LoRA configured successfully\n\n=== PHASE 3: TRAINING EXECUTION ===\nRAM: 5.8% (1.3/31.4GB)\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✅ Training completed successfully\n\n=== PHASE 4: MODEL SAVING ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved to /kaggle/working/output\nSaved files: tokenizer_config.json, adapter_config.json, tokenizer.json, README.md, special_tokens_map.json, merges.txt, vocab.json, adapter_model.safetensors\n\n=== PHASE 5: MODEL TESTING ===\n\n🔹 Test 1: Blockchain technology is\n💬 Response: a decentralized peer-to--that's an asset, uh... I think it all works out to be in the same place.\nIt turns into something that has some sort of security and privacy benefits if you can do just what we're doing for\n\n🔹 Test 2: A hardware wallet is\n💬 Response: the next most common way to get a Bitcoin address, and this includes any non-public keys.\n\n\n I've already written an article on how you can use it for storing private key cryptography addresses (as well as other wallets). You have several\n\n🔹 Test 3: Proof of Work consensus\n💬 Response: .\n            The above is just a rough sketch, but the concept can be implemented in various ways as well by any and all developers with this article: https://www2k-3d4a1c8e9b5f\n\n🔹 Test 4: Public-key cryptography\n💬 Response: and/or\nA security service provider. This is a private company that will use the services provided public key, but only for your personal data such as email addresses or financial information about you to be sent through an encrypted (encrypted) password system used\n\n=== TRAINING PROCESS COMPLETE ===\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:41:28.275515Z","iopub.execute_input":"2025-06-02T22:41:28.275869Z","iopub.status.idle":"2025-06-02T22:41:28.283188Z","shell.execute_reply.started":"2025-06-02T22:41:28.275844Z","shell.execute_reply":"2025-06-02T22:41:28.281989Z"}},"outputs":[{"name":"stdout","text":"Total notebook execution time: 31.88 seconds\n","output_type":"stream"}],"execution_count":9}]}