{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":242402955,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:47:47.462845Z","iopub.execute_input":"2025-06-02T21:47:47.463296Z","iopub.status.idle":"2025-06-02T21:47:47.468955Z","shell.execute_reply.started":"2025-06-02T21:47:47.463268Z","shell.execute_reply":"2025-06-02T21:47:47.467619Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Kaggle Environment Setup\nimport os\nimport sys\nimport torch\nimport psutil\n\nprint(\"=== Initializing Environment ===\")\n\n# Verify GPU availability\nif torch.cuda.is_available():\n    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"No GPU detected - running in CPU mode\")\n\n# Memory diagnostics\ndef print_memory():\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\nprint(\"\\nInitial system status:\")\nprint_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:47:47.472746Z","iopub.execute_input":"2025-06-02T21:47:47.473105Z","iopub.status.idle":"2025-06-02T21:47:47.491195Z","shell.execute_reply.started":"2025-06-02T21:47:47.473078Z","shell.execute_reply":"2025-06-02T21:47:47.489798Z"}},"outputs":[{"name":"stdout","text":"=== Initializing Environment ===\nNo GPU detected - running in CPU mode\n\nInitial system status:\nRAM: 5.2% (1.2/31.4GB)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Install required packages with version pinning\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n!pip install -q transformers==4.41.2 datasets==2.18.0 peft==0.10.0 accelerate==0.29.1 bitsandbytes==0.43.0\n\n# Verify installations\nimport importlib\nfor pkg in ['torch', 'transformers', 'datasets', 'peft', 'bitsandbytes']:\n    try:\n        importlib.import_module(pkg)\n        print(f\"✅ {pkg} installed successfully\")\n    except ImportError:\n        print(f\"❌ {pkg} not installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:47:47.492713Z","iopub.execute_input":"2025-06-02T21:47:47.493041Z","iopub.status.idle":"2025-06-02T21:47:58.906353Z","shell.execute_reply.started":"2025-06-02T21:47:47.493005Z","shell.execute_reply":"2025-06-02T21:47:58.904935Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cuda-runtime-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m✅ torch installed successfully\n✅ transformers installed successfully\n✅ datasets installed successfully\n✅ peft installed successfully\n✅ bitsandbytes installed successfully\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple, Any\nfrom datasets import Dataset, load_dataset\nimport json\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import TrainingArguments\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:47:58.907777Z","iopub.execute_input":"2025-06-02T21:47:58.908103Z","iopub.status.idle":"2025-06-02T21:47:58.915171Z","shell.execute_reply.started":"2025-06-02T21:47:58.908073Z","shell.execute_reply":"2025-06-02T21:47:58.913723Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"MODEL_NAME = \"gpt2\"  # Using GPT-2 for Kaggle compatibility\n\ndef load_model(model_name: str):\n    \"\"\"Robust model loading with fallbacks\"\"\"\n    print(f\"\\n=== Loading {model_name} ===\")\n    \n    # Configure tokenizer first\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"✅ Tokenizer loaded successfully\")\n    except Exception as e:\n        print(f\"❌ Tokenizer loading failed: {e}\")\n        raise\n\n    # Configure quantization if GPU available\n    if torch.cuda.is_available():\n        print(\"Configuring for GPU with 4-bit quantization\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n    else:\n        print(\"Configuring for CPU without quantization\")\n        bnb_config = None\n\n    # Attempt model loading with progressive fallbacks\n    try:\n        # First try with full configuration\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        print(\"✅ Model loaded with configured settings\")\n    except Exception as e:\n        print(f\"⚠️ Primary load failed: {e}\")\n        print(\"Attempting fallback to basic CPU loading...\")\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                device_map=\"cpu\",\n                torch_dtype=torch.float32\n            )\n            print(\"✅ Model loaded on CPU\")\n        except Exception as e:\n            print(f\"❌ All loading attempts failed: {e}\")\n            raise\n\n    print(\"\\nFinal memory status:\")\n    print_memory()\n    return model, tokenizer\n\n# Load model and tokenizer\ntry:\n    model, tokenizer = load_model(MODEL_NAME)\nexcept Exception as e:\n    print(f\"\\n❌ Critical error loading model: {e}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:47:58.917481Z","iopub.execute_input":"2025-06-02T21:47:58.917813Z","iopub.status.idle":"2025-06-02T21:48:00.150900Z","shell.execute_reply.started":"2025-06-02T21:47:58.917789Z","shell.execute_reply":"2025-06-02T21:48:00.149579Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading gpt2 ===\n✅ Tokenizer loaded successfully\nConfiguring for CPU without quantization\n✅ Model loaded with configured settings\n\nFinal memory status:\nRAM: 5.2% (1.2/31.4GB)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def analyze_vocab_coverage(tokenizer, domain=\"cryptocurrency\"):\n    \"\"\"Comprehensive vocabulary coverage analysis for specific domains\"\"\"\n    # Define domain-specific term lists\n    domains = {\n        \"cryptocurrency\": [\n            \"blockchain\", \"cryptocurrency\", \"decentralized\", \"ledger\", \"mining\", \n            \"wallet\", \"hardware wallet\", \"software wallet\", \"private key\", \"public key\", \n            \"address\", \"transaction\", \"consensus\", \"proof of work\", \"proof of stake\",\n            \"smart contract\", \"token\", \"exchange\", \"hash\", \"digital signature\",\n            \"crypto\", \"bitcoin\", \"ethereum\", \"altcoin\", \"defi\", \"nft\", \"dao\",\n            \"gas fee\", \"block reward\", \"halving\", \"fork\", \"node\", \"validator\",\n            \"staking\", \"liquidity pool\", \"yield farming\", \"oracle\", \"zk-snark\",\n            \"segwit\", \"taproot\", \"lightning network\", \"sidechain\", \"atomic swap\"\n        ],\n        \"finance\": [\n            \"stock\", \"bond\", \"dividend\", \"portfolio\", \"asset allocation\",\n            \"market cap\", \"liquidity\", \"volatility\", \"yield\", \"interest rate\",\n            \"inflation\", \"deflation\", \"recession\", \"bull market\", \"bear market\"\n        ],\n        \"technology\": [\n            \"algorithm\", \"encryption\", \"database\", \"cloud computing\", \"artificial intelligence\",\n            \"machine learning\", \"neural network\", \"quantum computing\", \"cybersecurity\", \"blockchain\"\n        ]\n    }\n    \n    # Select terms based on domain\n    domain_terms = domains.get(domain, domains[\"cryptocurrency\"])\n    \n    # Analyze coverage\n    covered = []\n    partially_covered = []\n    missing = []\n    \n    for term in domain_terms:\n        tokens = tokenizer.tokenize(term)\n        token_presence = [token in tokenizer.vocab for token in tokens]\n        \n        if all(token_presence):\n            covered.append(term)\n        elif any(token_presence):\n            partially_covered.append(term)\n        else:\n            missing.append(term)\n    \n    coverage_score = len(covered) / len(domain_terms)\n    partial_score = len(partially_covered) / len(domain_terms)\n    \n    return {\n        \"domain\": domain,\n        \"coverage_score\": coverage_score,\n        \"partial_coverage_score\": partial_score,\n        \"covered_terms\": covered,\n        \"partially_covered_terms\": partially_covered,\n        \"missing_terms\": missing,\n        \"vocab_size\": len(tokenizer.vocab),\n        \"domain_term_count\": len(domain_terms)\n    }\n\ndef load_and_analyze_tokenizer(model_name=\"gpt2\", domain=\"cryptocurrency\"):\n    \"\"\"Advanced tokenizer loading with domain-specific vocabulary analysis\"\"\"\n    print(f\"\\n=== Loading and Analyzing Tokenizer: {model_name} ===\")\n    print(f\"Domain: {domain.capitalize()}\")\n    \n    try:\n        # 1. Load tokenizer with error handling\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        except OSError:\n            print(f\"⚠️ Model {model_name} not found, using GPT-2 as fallback\")\n            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        \n        # 2. Configure special tokens\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token or \"<pad>\"\n        \n        if tokenizer.eos_token is None:\n            tokenizer.eos_token = \"</s>\"\n        \n        if tokenizer.bos_token is None:\n            tokenizer.bos_token = \"<s>\"\n        \n        if tokenizer.unk_token is None:\n            tokenizer.unk_token = \"<unk>\"\n        \n        print(f\"✅ Tokenizer loaded successfully | Vocab size: {len(tokenizer.vocab):,}\")\n        \n        # 3. Perform vocabulary analysis\n        vocab_report = analyze_vocab_coverage(tokenizer, domain)\n        \n        # 4. Print detailed report\n        print(f\"\\n📊 Vocabulary Coverage Report ({domain}):\")\n        print(f\"- Full coverage: {vocab_report['coverage_score']:.1%} ({len(vocab_report['covered_terms'])} terms)\")\n        print(f\"- Partial coverage: {vocab_report['partial_coverage_score']:.1%} ({len(vocab_report['partially_covered_terms'])} terms)\")\n        print(f\"- Missing coverage: {1 - vocab_report['coverage_score'] - vocab_report['partial_coverage_score']:.1%} ({len(vocab_report['missing_terms'])} terms)\")\n        \n        if vocab_report['missing_terms']:\n            print(f\"\\n⚠️ Top missing terms:\")\n            for term in vocab_report['missing_terms'][:5]:\n                tokens = tokenizer.tokenize(term)\n                missing_tokens = [t for t in tokens if t not in tokenizer.vocab]\n                print(f\"  - '{term}': Missing tokens - {', '.join(missing_tokens)}\")\n        \n        # 5. Special token verification\n        print(\"\\n🔍 Special Token Verification:\")\n        special_tokens = [\"pad_token\", \"eos_token\", \"bos_token\", \"unk_token\"]\n        for token in special_tokens:\n            value = getattr(tokenizer, token, None)\n            print(f\"- {token}: {value}\")\n\n        return tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading tokenizer: {str(e)}\")\n        print(\"Attempting minimal tokenizer creation...\")\n        from transformers import GPT2Tokenizer\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"✅ Created minimal GPT-2 tokenizer\")\n        return tokenizer\n\n# Load and analyze tokenizer with cryptocurrency domain focus\ntokenizer = load_and_analyze_tokenizer(\"gpt2\", domain=\"cryptocurrency\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:48:00.152856Z","iopub.execute_input":"2025-06-02T21:48:00.153240Z","iopub.status.idle":"2025-06-02T21:48:03.362859Z","shell.execute_reply.started":"2025-06-02T21:48:00.153212Z","shell.execute_reply":"2025-06-02T21:48:03.361509Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading and Analyzing Tokenizer: gpt2 ===\nDomain: Cryptocurrency\n✅ Tokenizer loaded successfully | Vocab size: 50,257\n\n📊 Vocabulary Coverage Report (cryptocurrency):\n- Full coverage: 100.0% (43 terms)\n- Partial coverage: 0.0% (0 terms)\n- Missing coverage: 0.0% (0 terms)\n\n🔍 Special Token Verification:\n- pad_token: <|endoftext|>\n- eos_token: <|endoftext|>\n- bos_token: <|endoftext|>\n- unk_token: <|endoftext|>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Data Preparation\n\ndef analyze_dataset(dataset, tokenizer) -> Dict:\n    \"\"\"Comprehensive dataset quality analysis with robust error handling\"\"\"\n    try:\n        # Get vocabulary\n        vocab = set(tokenizer.get_vocab().keys())\n        dataset_tokens = set()\n        text_lengths = []\n        \n        # Process each example\n        for example in dataset:\n            try:\n                # Handle different dataset formats\n                text = example['text'] if 'text' in example else str(example)\n                tokens = tokenizer.tokenize(text)\n                dataset_tokens.update(tokens)\n                text_lengths.append(len(tokens))\n            except Exception as e:\n                print(f\"⚠️ Error processing example: {e}\")\n                continue\n        \n        # Calculate vocabulary coverage\n        coverage = len(dataset_tokens & vocab) / len(vocab) if vocab else 0\n        \n        # Statistical analysis\n        length_stats = {\n            'mean': np.mean(text_lengths) if text_lengths else 0,\n            'std': np.std(text_lengths) if text_lengths else 0,\n            'min': min(text_lengths) if text_lengths else 0,\n            'max': max(text_lengths) if text_lengths else 0,\n            'percentiles': np.percentile(text_lengths, [25, 50, 75]) if text_lengths else [0, 0, 0]\n        }\n        \n        # Topic diversity analysis\n        topics = defaultdict(int)\n        for example in dataset:\n            try:\n                text = example['text'] if 'text' in example else str(example)\n                text_lower = text.lower()\n                for term in ['blockchain', 'wallet', 'mining', 'crypto', 'token', \n                             'bitcoin', 'ethereum', 'defi', 'nft', 'key']:\n                    if term in text_lower:\n                        topics[term] += 1\n            except:\n                continue\n        \n        return {\n            'vocab_coverage': round(coverage, 4),\n            'length_stats': length_stats,\n            'topic_distribution': dict(topics),\n            'total_samples': len(dataset),\n            'processed_samples': len(text_lengths)\n        }\n        \n    except Exception as e:\n        print(f\"❌ Dataset analysis failed: {e}\")\n        return {\n            'vocab_coverage': 0.0,\n            'length_stats': {},\n            'topic_distribution': {},\n            'total_samples': len(dataset),\n            'processed_samples': 0\n        }\n\ndef stratified_sample(dataset, stratify_by: str = 'label', n_samples: int = None) -> Dataset:\n    \"\"\"Stratified sampling for small datasets with fallback\"\"\"\n    try:\n        if n_samples is None:\n            n_samples = min(1000, len(dataset))\n        \n        if stratify_by not in dataset.features:\n            print(f\"⚠️ Stratification column '{stratify_by}' not found, using random sampling\")\n            return dataset.select(range(n_samples))\n        \n        from sklearn.model_selection import train_test_split\n        import pandas as pd\n        \n        df = pd.DataFrame(dataset)\n        _, sample = train_test_split(\n            df,\n            train_size=n_samples,\n            stratify=df[stratify_by],\n            random_state=42\n        )\n        return Dataset.from_pandas(sample)\n        \n    except Exception as e:\n        print(f\"❌ Stratified sampling failed: {e}\")\n        print(\"Using random sampling instead\")\n        return dataset.select(range(min(n_samples, len(dataset)))) if n_samples else dataset\n\ndef prepare_dataset(file_path: str, tokenizer, max_samples: int = 1000) -> Dataset:\n    \"\"\"Robust dataset preparation with quality checks and fallbacks\"\"\"\n    print(f\"\\n=== Preparing Dataset: {file_path} ===\")\n    \n    try:\n        # Verify file existence\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Dataset path not found: {file_path}\")\n            \n        # Load dataset\n        try:\n            dataset = load_dataset('json', data_files=file_path, split='train')\n            print(f\"✅ Raw dataset loaded | Samples: {len(dataset)}\")\n        except:\n            # Try alternative loading methods\n            try:\n                with open(file_path, 'r') as f:\n                    data = [json.loads(line) for line in f]\n                dataset = Dataset.from_list(data)\n                print(f\"✅ Dataset loaded from JSON lines | Samples: {len(dataset)}\")\n            except:\n                raise ValueError(\"Unsupported file format\")\n        \n        # Standardize text column\n        text_column = None\n        for col in ['text', 'content', 'body', 'article', 'sentence']:\n            if col in dataset.features:\n                text_column = col\n                break\n                \n        if text_column and text_column != 'text':\n            print(f\"⚠️ Renaming '{text_column}' to 'text'\")\n            dataset = dataset.rename_column(text_column, 'text')\n        elif 'text' not in dataset.features:\n            # Create text column by combining all string columns\n            print(\"⚠️ No text column found, creating from string fields\")\n            \n            def combine_columns(examples):\n                text = \"\"\n                for key, value in examples.items():\n                    if isinstance(value, str):\n                        text += value + \" \"\n                return {'text': text.strip()}\n                \n            dataset = dataset.map(combine_columns)\n        \n        # Apply sampling if needed\n        if len(dataset) > max_samples:\n            print(f\"⚠️ Large dataset ({len(dataset)} samples), sampling to {max_samples}\")\n            dataset = stratified_sample(dataset, n_samples=max_samples)\n        \n        # Quality analysis\n        quality_report = analyze_dataset(dataset, tokenizer)\n        \n        print(\"\\n📊 Dataset Quality Report:\")\n        print(f\"- Samples: {quality_report['total_samples']} (processed: {quality_report['processed_samples']})\")\n        print(f\"- Vocabulary coverage: {quality_report['vocab_coverage']:.1%}\")\n        print(f\"- Text length: avg={quality_report['length_stats']['mean']:.1f} tokens\")\n        print(f\"- Topics: {', '.join(f'{k}:{v}' for k,v in quality_report['topic_distribution'].items())[:100]}...\")\n        \n        return dataset\n        \n    except Exception as e:\n        print(f\"❌ Dataset preparation failed: {e}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\n            \"Blockchain is a decentralized ledger technology.\",\n            \"Cryptocurrencies use public-key cryptography for security.\",\n            \"Proof of Work requires miners to solve computational puzzles.\",\n            \"Hardware wallets provide offline storage for private keys.\",\n            \"Smart contracts enable automated transactions on blockchain networks.\"\n        ]})\n        \ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ndataset = prepare_dataset(\"/kaggle/input/database-0530\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:48:03.364143Z","iopub.execute_input":"2025-06-02T21:48:03.364888Z","iopub.status.idle":"2025-06-02T21:48:04.402057Z","shell.execute_reply.started":"2025-06-02T21:48:03.364841Z","shell.execute_reply":"2025-06-02T21:48:04.401001Z"}},"outputs":[{"name":"stdout","text":"\n=== Preparing Dataset: /kaggle/input/database-0530 ===\n❌ Dataset preparation failed: Unsupported file format\nCreating minimal fallback dataset...\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Training Configuration\n\ndef get_gpu_usage() -> float:\n    \"\"\"Get current GPU memory usage percentage\"\"\"\n    try:\n        if torch.cuda.is_available():\n            return torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100\n        return 0.0\n    except:\n        return 0.0\n\ndef suggest_hyperparameters(model, dataset) -> Dict[str, Any]:\n    \"\"\"Auto-suggest training parameters based on model and data\"\"\"\n    try:\n        # Calculate base parameters\n        suggested_batch_size = max(1, min(8, len(dataset) // 100))\n        params = {\n            'batch_size': suggested_batch_size,\n            'learning_rate': 2e-5,\n            'epochs': 1 if len(dataset) < 1000 else 3,\n            'grad_accum': max(1, 32 // suggested_batch_size)\n        }\n        \n        # Adjust for model size\n        try:\n            num_params = sum(p.numel() for p in model.parameters())\n            if num_params > 1e9:  # Large model\n                params['learning_rate'] = params['learning_rate'] / 2\n                params['batch_size'] = max(1, params['batch_size'] // 2)\n        except:\n            print(\"⚠️ Could not calculate model parameters, using defaults\")\n        \n        return params\n    except Exception as e:\n        print(f\"❌ Hyperparameter suggestion failed: {e}\")\n        return {\n            'batch_size': 1,\n            'learning_rate': 2e-5,\n            'epochs': 1,\n            'grad_accum': 4\n        }\n\ndef configure_training(model, dataset) -> Tuple[Any, TrainingArguments]:\n    \"\"\"Complete training configuration with LoRA and monitoring\"\"\"\n    print(\"\\n=== Configuring Training ===\")\n    \n    try:\n        # 1. Get hyperparameter suggestions\n        hyperparams = suggest_hyperparameters(model, dataset)\n        print(f\"Suggested hyperparameters: batch_size={hyperparams['batch_size']}, \"\n              f\"lr={hyperparams['learning_rate']}, epochs={hyperparams['epochs']}, \"\n              f\"grad_accum={hyperparams['grad_accum']}\")\n        \n        # 2. LoRA configuration\n        # Try to detect target modules automatically\n        target_modules = [\"q_proj\", \"v_proj\"]\n        try:\n            module_names = [name for name, _ in model.named_modules()]\n            if \"c_attn\" in module_names:  # GPT-2 style\n                target_modules = [\"c_attn\", \"c_proj\", \"mlp\"]\n            elif \"query_key_value\" in module_names:  # LLaMA style\n                target_modules = [\"query_key_value\"]\n            print(f\"Detected target modules: {target_modules}\")\n        except:\n            print(\"⚠️ Using default target modules\")\n        \n        peft_config = LoraConfig(\n            r=8,  # Reduced for Kaggle memory constraints\n            lora_alpha=16,\n            target_modules=target_modules,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        \n        # 3. Training arguments\n        training_args = TrainingArguments(\n            output_dir=\"./results\",\n            per_device_train_batch_size=hyperparams['batch_size'],\n            gradient_accumulation_steps=hyperparams['grad_accum'],\n            num_train_epochs=hyperparams['epochs'],\n            learning_rate=hyperparams['learning_rate'],\n            fp16=torch.cuda.is_available(),\n            logging_steps=10,\n            save_strategy=\"steps\",\n            save_steps=200,  # Reduced for Kaggle\n            report_to=\"none\",\n            optim=\"adamw_torch\",\n            max_grad_norm=0.3,\n            warmup_ratio=0.1\n        )\n        \n        # 4. Prepare model\n        try:\n            model = prepare_model_for_kbit_training(model)\n        except:\n            print(\"⚠️ k-bit training prep failed, proceeding without\")\n            \n        try:\n            model = get_peft_model(model, peft_config)\n            model.print_trainable_parameters()\n            print(\"✅ LoRA configured successfully\")\n        except Exception as e:\n            print(f\"❌ LoRA configuration failed: {e}\")\n            print(\"Proceeding without LoRA\")\n        \n        # 5. Create training monitor\n        monitor = TrainingMonitor()\n        \n        return model, training_args, monitor\n        \n    except Exception as e:\n        print(f\"❌ Training configuration failed: {e}\")\n        print(\"Creating minimal configuration...\")\n        \n        # Fallback configuration\n        training_args = TrainingArguments(\n            output_dir=\"./results\",\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=4,\n            num_train_epochs=1,\n            learning_rate=2e-5,\n            report_to=\"none\"\n        )\n        return model, training_args, TrainingMonitor()\n\nclass TrainingMonitor:\n    \"\"\"Real-time training monitoring with visualization\"\"\"\n    def __init__(self):\n        from collections import defaultdict\n        self.metrics = defaultdict(list)\n        self.start_time = time.time()\n        self.epoch_start = time.time()\n        \n    def update(self, **kwargs):\n        \"\"\"Update metrics with new values\"\"\"\n        for k, v in kwargs.items():\n            self.metrics[k].append(v)\n        \n    def display_dashboard(self, epoch=None, step=None):\n        \"\"\"Display training dashboard\"\"\"\n        try:\n            clear_output(wait=True)\n            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n            \n            # Plot 1: Training Loss\n            if 'loss' in self.metrics and len(self.metrics['loss']) > 0:\n                axes[0].plot(self.metrics['loss'])\n                axes[0].set_title(\"Training Loss\")\n                axes[0].set_xlabel(\"Steps\")\n                axes[0].set_ylabel(\"Loss\")\n            \n            # Plot 2: Gradient Norms\n            if 'grad_norm' in self.metrics and len(self.metrics['grad_norm']) > 0:\n                axes[1].plot(self.metrics['grad_norm'])\n                axes[1].set_title(\"Gradient Norm\")\n                axes[1].set_xlabel(\"Steps\")\n                axes[1].set_ylabel(\"Norm\")\n            \n            # Plot 3: Hardware Usage\n            hardware_metrics = [\n                psutil.cpu_percent(),\n                get_gpu_usage(),\n                psutil.virtual_memory().percent\n            ]\n            axes[2].bar(['CPU', 'GPU', 'RAM'], hardware_metrics, color=['blue', 'green', 'purple'])\n            axes[2].set_title(\"Hardware Usage (%)\")\n            axes[2].set_ylim(0, 100)\n            \n            # Add title with epoch/step info\n            title = \"Training Monitor\"\n            if epoch is not None:\n                title += f\" | Epoch {epoch+1}\"\n            if step is not None:\n                title += f\" | Step {step}\"\n            plt.suptitle(title)\n            \n            plt.tight_layout()\n            plt.show()\n            \n            # Print textual summary\n            if 'loss' in self.metrics and len(self.metrics['loss']) > 0:\n                print(f\"Current loss: {self.metrics['loss'][-1]:.4f}\")\n            print(f\"CPU: {hardware_metrics[0]:.1f}% | GPU: {hardware_metrics[1]:.1f}% | RAM: {hardware_metrics[2]:.1f}%\")\n            \n        except Exception as e:\n            print(f\"⚠️ Dashboard error: {e}\")\n        \n    def should_stop_early(self, patience=3, min_steps=10) -> bool:\n        \"\"\"Early stopping check with safeguards\"\"\"\n        try:\n            losses = self.metrics.get('loss', [])\n            if len(losses) < max(patience * 2, min_steps):\n                return False\n                \n            # Calculate moving averages\n            recent_loss = np.mean(losses[-patience:])\n            previous_loss = np.mean(losses[-patience*2:-patience])\n            \n            # Stop if loss hasn't improved\n            return recent_loss > previous_loss * 0.99  # Allow 1% tolerance\n            \n        except Exception as e:\n            print(f\"⚠️ Early stopping check failed: {e}\")\n            return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:48:04.403361Z","iopub.execute_input":"2025-06-02T21:48:04.403794Z","iopub.status.idle":"2025-06-02T21:48:04.430773Z","shell.execute_reply.started":"2025-06-02T21:48:04.403759Z","shell.execute_reply":"2025-06-02T21:48:04.429327Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Training Execution\n# =====================\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n                             'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n                             'labels': torch.stack([f['input_ids'] for f in data])}\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ndef generate_contrastive_examples(example):\n    \"\"\"Generate contrastive examples for training\"\"\"\n    # Generate negative sample by:\n    # 1. Random Q/A from different category\n    # 2. GPT-generated incorrect answer\n    # 3. Perturbed correct answer\n    return {\n        'anchor': example['answer'],\n        'positive': augment_answer(example['answer']),\n        'negative': get_negative_sample(example)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:56:54.631583Z","iopub.execute_input":"2025-06-02T21:56:54.632012Z","iopub.status.idle":"2025-06-02T21:56:54.643704Z","shell.execute_reply.started":"2025-06-02T21:56:54.631988Z","shell.execute_reply":"2025-06-02T21:56:54.642420Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Model Saving\n\nimport os\nimport shutil\nimport json\nfrom typing import Optional, Dict, Any\nfrom transformers import TrainingArguments\n\n# Model Saving\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/model-output\"\n) -> Dict[str, Any]:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both full models and adapters (LoRA) with error recovery.\n    Returns verification report.\n    \"\"\"\n    # Initialize verification report\n    verification_report = {\n        \"status\": \"started\",\n        \"output_dir\": output_dir,\n        \"saved_files\": [],\n        \"missing_files\": [],\n        \"errors\": []\n    }\n    \n    try:\n        # 1. Create output directory with cleanup if needed\n        print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n        if os.path.exists(output_dir):\n            print(\"⚠️ Output directory exists, removing old files...\")\n            shutil.rmtree(output_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # 2. Save model with multiple attempts\n        saved_model = False\n        for attempt in range(3):\n            try:\n                print(f\"💽 Saving model (attempt {attempt+1})...\")\n                model.save_pretrained(\n                    output_dir,\n                    safe_serialization=True,\n                    max_shard_size=\"200MB\"  # Shard large models\n                )\n                saved_model = True\n                break\n            except Exception as e:\n                print(f\"⚠️ Model save failed: {str(e)}\")\n                if attempt == 2:\n                    verification_report[\"errors\"].append(f\"Model save failed: {str(e)}\")\n        \n        # 3. Save tokenizer\n        try:\n            print(\"🔤 Saving tokenizer...\")\n            tokenizer.save_pretrained(output_dir)\n        except Exception as e:\n            print(f\"⚠️ Tokenizer save failed: {str(e)}\")\n            verification_report[\"errors\"].append(f\"Tokenizer save failed: {str(e)}\")\n        \n        # 4. Save training arguments if provided\n        if training_args is not None:\n            print(\"📝 Saving training arguments...\")\n            try:\n                args_path = os.path.join(output_dir, \"training_args.json\")\n                if hasattr(training_args, 'to_dict'):\n                    with open(args_path, \"w\") as f:\n                        json.dump(training_args.to_dict(), f, indent=2)\n                elif hasattr(training_args, 'to_json_string'):\n                    with open(args_path, \"w\") as f:\n                        f.write(training_args.to_json_string())\n                else:\n                    print(\"⚠️ TrainingArguments has no serialization method\")\n                    verification_report[\"errors\"].append(\"TrainingArguments has no serialization method\")\n            except Exception as e:\n                print(f\"⚠️ Failed to save training args: {str(e)}\")\n                verification_report[\"errors\"].append(f\"Training args save failed: {str(e)}\")\n        \n        # 5. Verify critical files\n        required_files = [\n            'config.json', 'pytorch_model.bin', \n            'model.safetensors', 'tokenizer.json',\n            'special_tokens_map.json', 'tokenizer_config.json'\n        ]\n        \n        # For adapter models\n        adapter_files = ['adapter_config.json', 'adapter_model.safetensors']\n        \n        print(\"\\n🔍 Verifying saved files:\")\n        for file in os.listdir(output_dir):\n            file_path = os.path.join(output_dir, file)\n            size = os.path.getsize(file_path) / 1024  # Size in KB\n            print(f\"- {file} ({size:.2f} KB)\")\n            verification_report[\"saved_files\"].append({\"name\": file, \"size_kb\": size})\n            \n            # Remove verified files from required list\n            if file in required_files:\n                required_files.remove(file)\n            if file in adapter_files:\n                adapter_files.remove(file)\n        \n        # 6. Handle missing files\n        if required_files:\n            print(f\"⚠️ Missing core files: {required_files}\")\n            verification_report[\"missing_files\"].extend(required_files)\n            \n            # Attempt recovery for adapter files\n            if adapter_files:\n                print(\"Trying to save adapter separately...\")\n                try:\n                    model.save_pretrained(\n                        output_dir,\n                        safe_serialization=True,\n                        adapter_only=True\n                    )\n                    # Re-check adapter files\n                    for file in adapter_files:\n                        if os.path.exists(os.path.join(output_dir, file)):\n                            adapter_files.remove(file)\n                    if adapter_files:\n                        print(f\"⚠️ Still missing adapter files: {adapter_files}\")\n                        verification_report[\"missing_files\"].extend(adapter_files)\n                except Exception as e:\n                    print(f\"⚠️ Adapter save failed: {str(e)}\")\n                    verification_report[\"errors\"].append(f\"Adapter save failed: {str(e)}\")\n        \n        # 7. Final verification\n        if not verification_report[\"missing_files\"] and not verification_report[\"errors\"]:\n            print(\"✅ All files saved successfully!\")\n            verification_report[\"status\"] = \"success\"\n        else:\n            print(\"⚠️ Verification completed with issues\")\n            verification_report[\"status\"] = \"partial_success\"\n        \n        return verification_report\n        \n    except Exception as e:\n        print(f\"❌ Critical error during saving: {str(e)}\")\n        verification_report[\"status\"] = \"failed\"\n        verification_report[\"errors\"].append(f\"Critical error: {str(e)}\")\n        return verification_report\n\n# Example usage:\n# report = save_model_artifacts(model, tokenizer, training_args)\n# print(\"Verification Report:\", json.dumps(report, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:56:54.646214Z","iopub.execute_input":"2025-06-02T21:56:54.646649Z","iopub.status.idle":"2025-06-02T21:56:54.675928Z","shell.execute_reply.started":"2025-06-02T21:56:54.646613Z","shell.execute_reply":"2025-06-02T21:56:54.673993Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Model Loading and Testing\n# =====================\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 250,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline - REMOVED device parameter since we're using device_map=\"auto\"\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"💬 Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T21:56:54.798030Z","iopub.execute_input":"2025-06-02T21:56:54.798489Z","iopub.status.idle":"2025-06-02T21:56:54.813419Z","shell.execute_reply.started":"2025-06-02T21:56:54.798456Z","shell.execute_reply":"2025-06-02T21:56:54.811976Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class EnhancedModelWrapper:\n    \"\"\"Advanced wrapper for constrained generation with technical enforcement\"\"\"\n    \n    def __init__(self, model, tokenizer, knowledge_base: Optional[Dict] = None):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.required_terms = []\n        self.complete_sentences = True\n        self.technical_terms = knowledge_base or {\n            'blockchain': ['decentralized', 'immutable', 'consensus', 'ledger'],\n            'wallet': ['private key', 'public key', 'address', 'security'],\n            'PoW': ['mining', 'difficulty', 'hash', 'computational'],\n            'cryptography': ['encryption', 'signature', 'asymmetric', 'algorithm'],\n            'P2P': ['network', 'nodes', 'direct', 'decentralized']\n        }\n        self.banned_phrases = [\n            \"I don't know\", \"as an AI\", \"I'm not sure\",\n            \"I can't answer\", \"my training data\"\n        ]\n\n    def set_constraints(self, \n                      required_terms: List[str] = None,\n                      complete_sentences: bool = True,\n                      technical_focus: str = None):\n        \"\"\"Configure generation constraints\"\"\"\n        self.required_terms = required_terms or []\n        self.complete_sentences = complete_sentences\n        \n        if technical_focus:\n            self.required_terms.extend(self.technical_terms.get(technical_focus, []))\n\n    def generate(self, \n                prompt: str,\n                max_length: int = 200,\n                temperature: float = 0.7,\n                **kwargs) -> Dict:\n        \"\"\"Generate response with multiple validation layers\"\"\"\n        \n        # Create generation config\n        gen_config = GenerationConfig(\n            max_length=max_length,\n            temperature=temperature,\n            do_sample=True,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            pad_token_id=self.tokenizer.eos_token_id,\n            **kwargs\n        )\n        \n        # Generate raw output\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        outputs = self.model.generate(**inputs, generation_config=gen_config)\n        raw_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Apply processing pipeline\n        processed_text = self._process_output(prompt, raw_text)\n        \n        # Validate and score\n        validation = self._validate_response(prompt, processed_text)\n        \n        return {\n            'raw': raw_text,\n            'processed': processed_text,\n            'validation': validation,\n            'prompt_analysis': self._analyze_prompt(prompt)\n        }\n\n    def _process_output(self, prompt: str, text: str) -> str:\n        \"\"\"Apply all text processing constraints\"\"\"\n        # Remove prompt from output\n        if text.startswith(prompt):\n            text = text[len(prompt):].strip()\n        \n        # Apply term enforcement\n        if self.required_terms:\n            text = self._enforce_terms(text)\n        \n        # Complete sentences\n        if self.complete_sentences:\n            text = self._complete_sentences(text)\n            \n        # Remove banned phrases\n        for phrase in self.banned_phrases:\n            text = text.replace(phrase, \"\")\n            \n        return text.strip()\n\n    def _enforce_terms(self, text: str) -> str:\n        \"\"\"Ensure required technical terms are present\"\"\"\n        missing = [t for t in self.required_terms \n                  if not re.search(rf'\\b{re.escape(t)}\\b', text, re.IGNORECASE)]\n        \n        if missing:\n            # Try to naturally incorporate missing terms\n            additions = []\n            for term in missing:\n                if term in self.technical_terms:\n                    addition = f\" {term} is important because {self._explain_term(term)}.\"\n                    additions.append(addition)\n            \n            text += ''.join(additions) if additions else f\"\\n\\n[Missing terms: {', '.join(missing)}]\"\n        \n        return text\n\n    def _complete_sentences(self, text: str) -> str:\n        \"\"\"Ensure output ends with complete sentence\"\"\"\n        # Find last sentence boundary\n        last_boundary = max(\n            text.rfind('.'), \n            text.rfind('!'), \n            text.rfind('?'),\n            text.rfind('\\n')\n        )\n        \n        if last_boundary > 0 and len(text) - last_boundary < 50:\n            text = text[:last_boundary+1]\n            \n        # If no proper ending, add one\n        if text and text[-1] not in {'.', '!', '?'}:\n            text += '.' if not text.endswith(',') else '..'\n            \n        return text\n\n    def _validate_response(self, prompt: str, response: str) -> Dict:\n        \"\"\"Comprehensive quality validation\"\"\"\n        # Detect topic from prompt\n        topic = next((t for t in self.technical_terms \n                     if re.search(rf'\\b{t}\\b', prompt, re.IGNORECASE)), None)\n        \n        # Check technical terms\n        missing_terms = []\n        if topic:\n            missing_terms = [t for t in self.technical_terms[topic]\n                          if not re.search(rf'\\b{re.escape(t)}\\b', response, re.IGNORECASE)]\n        \n        # Check for hallucinations\n        hallucinations = any(\n            phrase.lower() in response.lower() \n            for phrase in self.banned_phrases\n        )\n        \n        # Calculate scores\n        tech_score = 1 - (len(missing_terms) / len(self.technical_terms.get(topic, [''])))\n        clarity_score = min(1, len(response.split()) / 50)  # Normalize to 0-1\n        \n        return {\n            'technical_score': tech_score,\n            'clarity_score': clarity_score,\n            'missing_terms': missing_terms,\n            'has_hallucinations': hallucinations,\n            'is_complete': response[-1] in {'.', '!', '?'}\n        }\n\n    def _analyze_prompt(self, prompt: str) -> Dict:\n        \"\"\"Evaluate prompt quality\"\"\"\n        return {\n            'length': len(prompt.split()),\n            'has_question': '?' in prompt,\n            'technical_focus': any(\n                term in prompt.lower() \n                for term in self.technical_terms\n            ),\n            'specificity': len(set(prompt.split())) / len(prompt.split())  # Unique words ratio\n        }\n\n    def _explain_term(self, term: str) -> str:\n        \"\"\"Generate simple explanations for technical terms\"\"\"\n        explanations = {\n            'blockchain': \"it enables secure decentralized record-keeping\",\n            'private key': \"it provides secure access to cryptocurrency funds\",\n            'mining': \"it secures the network through computational work\",\n            'encryption': \"it protects data through mathematical algorithms\"\n        }\n        return explanations.get(term, f\"it's a fundamental concept in cryptocurrency\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:03:06.052231Z","iopub.execute_input":"2025-06-02T22:03:06.053600Z","iopub.status.idle":"2025-06-02T22:03:06.078672Z","shell.execute_reply.started":"2025-06-02T22:03:06.053561Z","shell.execute_reply":"2025-06-02T22:03:06.077451Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Enhanced Generation\n# =====================\nCRYPTO_GENERATION_CONFIG = GenerationConfig(\n    max_new_tokens=150,\n    no_repeat_ngram_size=4,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=40,\n    repetition_penalty=1.15,\n    num_beams=3,\n    early_stopping=True\n)\n\ndef generate_with_validation(model, tokenizer, prompt, max_length=200):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # First pass generation\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        generation_config=CRYPTO_GENERATION_CONFIG\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Validation checks\n    validation_passed = True\n    validation_notes = []\n    \n    # 1. Technical term check\n    last_term = model.get_last_term(prompt)\n    if last_term in model.technical_terms:\n        missing = [t for t in model.technical_terms[last_term] \n                  if t.lower() not in response.lower()]\n        if missing:\n            validation_passed = False\n            validation_notes.append(f\"Missing technical terms: {missing}\")\n    \n    # 2. Hallucination check\n    if any(phrase in response for phrase in model.banned_sequences):\n        validation_passed = False\n        validation_notes.append(\"Potential hallucination\")\n    \n    # Generate final output\n    if not validation_passed:\n        print(f\"⚠️ Validation issues: {validation_notes}\")\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            generation_config=CRYPTO_GENERATION_CONFIG,\n            bad_words_ids=[[tid] for tid in tokenizer.encode(\" \".join(model.banned_sequences), add_special_tokens=False)]\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return {\n        'response': response,\n        'validation_passed': validation_passed,\n        'validation_notes': validation_notes\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:03:06.080730Z","iopub.execute_input":"2025-06-02T22:03:06.081154Z","iopub.status.idle":"2025-06-02T22:03:06.138321Z","shell.execute_reply.started":"2025-06-02T22:03:06.081128Z","shell.execute_reply":"2025-06-02T22:03:06.136664Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_485/643574594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Enhanced Generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# =====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m CRYPTO_GENERATION_CONFIG = GenerationConfig(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'GenerationConfig' is not defined"],"ename":"NameError","evalue":"name 'GenerationConfig' is not defined","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import (\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    GenerationConfig\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import load_dataset, Dataset\nimport psutil\nimport numpy as np\n\ndef print_memory():\n    \"\"\"Print current memory usage\"\"\"\n    ram = psutil.virtual_memory()\n    gpu_mem = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n    print(f\"RAM: {ram.percent}% | GPU: {gpu_mem:.2f}GB\")\n\ndef main(model, tokenizer):\n    # Configuration\n    DATASET_PATH = \"/kaggle/input/database-0530\"\n    OUTPUT_DIR = \"/kaggle/working/output\"\n    \n    try:\n        # 1. Data Preparation with Quality Metrics\n        print(\"\\n=== Preparing Dataset ===\")\n        print_memory()\n        \n        try:\n            # Try to load dataset\n            dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n            print(f\"✅ Loaded dataset with {len(dataset)} samples\")\n            \n            # Basic quality analysis\n            text_lengths = [len(text.split()) for text in dataset['text']]\n            print(f\"Text length stats: Avg={np.mean(text_lengths):.1f}, \"\n                  f\"Min={min(text_lengths)}, Max={max(text_lengths)}\")\n            \n            # Small dataset fallback\n            if len(dataset) < 10:\n                print(\"⚠️ Small dataset detected, using fallback samples\")\n                dataset = dataset.add_item({\"text\": \"Blockchain technology enables secure decentralized transactions.\"})\n                dataset = dataset.add_item({\"text\": \"Cryptocurrency wallets store private keys for digital asset management.\"})\n                \n        except Exception as e:\n            print(f\"❌ Dataset loading failed: {e}\")\n            print(\"Creating minimal fallback dataset...\")\n            dataset = Dataset.from_dict({\n                \"text\": [\n                    \"Blockchain is a decentralized ledger technology.\",\n                    \"Cryptocurrencies use cryptographic keys for security.\",\n                    \"Proof of Work requires computational resources.\",\n                    \"Hardware wallets provide offline storage for private keys.\",\n                    \"Public-key cryptography enables secure transactions.\"\n                ]\n            })\n            print(f\"✅ Created fallback dataset with {len(dataset)} samples\")\n\n        # Tokenization with error handling\n        print(\"\\nTokenizing dataset...\")\n        try:\n            def tokenize_function(examples):\n                return tokenizer(\n                    examples[\"text\"],\n                    truncation=True,\n                    max_length=128,\n                    padding=\"max_length\",\n                    return_tensors=\"pt\"\n                )\n                \n            tokenized_dataset = dataset.map(tokenize_function, batched=True)\n            tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n            print(\"✅ Tokenization completed successfully\")\n            print_memory()\n            \n        except Exception as e:\n            print(f\"❌ Tokenization failed: {e}\")\n            print(\"Creating minimal tokenized dataset...\")\n            tokenized_dataset = Dataset.from_dict({\n                \"input_ids\": [torch.tensor([0, 1, 2, 3])],\n                \"attention_mask\": [torch.tensor([1, 1, 1, 1])]\n            })\n            tokenized_dataset.set_format(type='torch')\n\n        # 2. Training Configuration\n        print(\"\\n=== Configuring Training ===\")\n        print_memory()\n        \n        # LoRA configuration for GPT-2\n        peft_config = LoraConfig(\n            r=8,  # Reduced for Kaggle memory constraints\n            lora_alpha=16,\n            target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # GPT-2 specific modules\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        \n        # Training arguments optimized for Kaggle\n        training_args = TrainingArguments(\n            output_dir=OUTPUT_DIR,\n            per_device_train_batch_size=1,  # Reduced for memory\n            gradient_accumulation_steps=8,  # Compensate for small batch size\n            num_train_epochs=1,\n            learning_rate=2e-5,\n            fp16=torch.cuda.is_available(),\n            logging_steps=10,\n            save_strategy=\"no\",  # Disable saving to save memory\n            report_to=\"none\",\n            optim=\"adamw_torch\"\n        )\n        \n        # Apply LoRA\n        try:\n            model = prepare_model_for_kbit_training(model)\n            model = get_peft_model(model, peft_config)\n            model.print_trainable_parameters()\n            print(\"✅ LoRA configured successfully\")\n        except Exception as e:\n            print(f\"❌ LoRA configuration failed: {e}\")\n            print(\"Proceeding without LoRA...\")\n\n        # 3. Training Execution\n        print(\"\\n=== Starting Training ===\")\n        print_memory()\n        \n        try:\n            # Special data collator for causal LM\n            data_collator = DataCollatorForLanguageModeling(\n                tokenizer=tokenizer,\n                mlm=False  # Causal language modeling\n            )\n            \n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=tokenized_dataset,\n                data_collator=data_collator,\n            )\n            \n            # Start training\n            trainer.train()\n            print(\"✅ Training completed successfully\")\n            print_memory()\n            \n        except Exception as e:\n            print(f\"❌ Training failed: {e}\")\n            print(\"Skipping training, proceeding to saving...\")\n\n        # 4. Saving Model\n        print(\"\\n=== Saving Model ===\")\n        try:\n            # Create output directory\n            os.makedirs(OUTPUT_DIR, exist_ok=True)\n            \n            # Save model\n            model.save_pretrained(OUTPUT_DIR)\n            tokenizer.save_pretrained(OUTPUT_DIR)\n            print(f\"✅ Model saved to {OUTPUT_DIR}\")\n            \n            # Verify files\n            files = os.listdir(OUTPUT_DIR)\n            print(f\"Saved files: {', '.join(files)}\")\n            print_memory()\n            \n        except Exception as e:\n            print(f\"❌ Model saving failed: {e}\")\n\n        # 5. Testing\n        print(\"\\n=== Testing Model ===\")\n        try:\n            # Move model to CPU if needed\n            if torch.cuda.is_available():\n                model.to('cuda')\n            else:\n                model.to('cpu')\n                \n            test_prompts = [\n                \"Explain blockchain technology:\",\n                \"What is the difference between hardware and software wallets?\",\n                \"Describe Proof of Work:\",\n                \"How does cryptography secure transactions?\"\n            ]\n            \n            generation_config = GenerationConfig(\n                max_new_tokens=100,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                repetition_penalty=1.2\n            )\n            \n            for i, prompt in enumerate(test_prompts):\n                print(f\"\\n🔹 Test {i+1}: {prompt}\")\n                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n                \n                try:\n                    outputs = model.generate(**inputs, generation_config=generation_config)\n                    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                    print(f\"💬 Response: {response[len(prompt):].strip()}\")\n                except Exception as e:\n                    print(f\"❌ Generation failed: {e}\")\n                    print(\"Skipping this test...\")\n            \n            print(\"\\n=== Training Complete ===\")\n            \n        except Exception as e:\n            print(f\"❌ Testing failed: {e}\")\n            \n    except Exception as e:\n        print(f\"\\n❌ Critical error: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    # Load model and tokenizer first (from previous step)\n    # This would be called from your environment setup cell\n    # main(model, tokenizer)\n    print(\"Call main(model, tokenizer) after loading your model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:03:06.219875Z","iopub.execute_input":"2025-06-02T22:03:06.220229Z","iopub.status.idle":"2025-06-02T22:03:17.971680Z","shell.execute_reply.started":"2025-06-02T22:03:06.220204Z","shell.execute_reply":"2025-06-02T22:03:17.970667Z"}},"outputs":[{"name":"stderr","text":"2025-06-02 22:03:09.321610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748901789.640890     485 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748901789.741017     485 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Call main(model, tokenizer) after loading your model\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"main(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:03:17.973713Z","iopub.execute_input":"2025-06-02T22:03:17.974444Z","iopub.status.idle":"2025-06-02T22:03:42.436656Z","shell.execute_reply.started":"2025-06-02T22:03:17.974414Z","shell.execute_reply":"2025-06-02T22:03:42.435343Z"}},"outputs":[{"name":"stdout","text":"\n=== Preparing Dataset ===\nRAM: 5.9% | GPU: 0.00GB\n❌ Dataset loading failed: Unable to find '/kaggle/input/database-0530'\nCreating minimal fallback dataset...\n✅ Created fallback dataset with 5 samples\n\nTokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ebb08604ed4a86a1de915ec5e1b593"}},"metadata":{}},{"name":"stdout","text":"❌ Tokenization failed: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\nCreating minimal tokenized dataset...\n\n=== Configuring Training ===\nRAM: 5.9% | GPU: 0.00GB\ntrainable params: 1,179,648 || all params: 125,619,456 || trainable%: 0.939064725769868\n✅ LoRA configured successfully\n\n=== Starting Training ===\nRAM: 5.9% | GPU: 0.00GB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"❌ Training failed: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\nSkipping training, proceeding to saving...\n\n=== Saving Model ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved to /kaggle/working/output\nSaved files: tokenizer_config.json, adapter_config.json, tokenizer.json, README.md, special_tokens_map.json, merges.txt, vocab.json, adapter_model.safetensors\nRAM: 6.0% | GPU: 0.00GB\n\n=== Testing Model ===\n\n🔹 Test 1: Explain blockchain technology:\n💬 Response: Bitcoin, Ether and Litecoin\n\n\n (I'm using this to show the new possibilities in Ethereum. I will be writing an article here on how it should look like a cryptocurrency as well) In my opinion about why there are so many cryptocurrencies that have become more popular than any other? The problem is when you start looking for things people want or need they can just don't know what's going on with their wallet - if not something specific reasons such as currency being important because of its purpose).\n\n🔹 Test 2: What is the difference between hardware and software wallets?\n💬 Response: If you have a very good set of cards, there are two different types: an ATX-compatible version (a.k.) with just one card that has to be installed on it; or another variant called \"ATX\" which uses both versions as much more than 2GB for its core chip but not necessarily 3 GB chips! This means your computer's memory will only work if they're in some kind of C++ style like GTST/GFX+SGA+. There\n\n🔹 Test 3: Describe Proof of Work:\n💬 Response: The work is an original and well-formed, the same as or better than anything I've ever done.\n\nWhen it comes to making a perfect cupcake with your favorite toppings (like chocolate chips), this recipe was first introduced in 2012 by my husband's mother who loves her homemade ingredients! It just so happens that we made our own version at home on Christmas Eve - instead you can make one for yourself :) What more could possibly be said? If not... This post contains affiliate\n\n🔹 Test 4: How does cryptography secure transactions?\n💬 Response: The answer to this question is not that encryption can't be done. Encryption for private keys, or the security as it's called in technical terms - a form of \"decrypting\" and other cryptographic technologies used by computer systems like computers are becoming more widely available over time (e-to) than they were before cryptographically verifiable digital data was created on paper at any given purpose. In my own case I've had many occasions where one person has stolen an encrypted transaction from another\n\n=== Training Complete ===\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T22:03:42.437973Z","iopub.execute_input":"2025-06-02T22:03:42.438770Z","iopub.status.idle":"2025-06-02T22:03:42.445609Z","shell.execute_reply.started":"2025-06-02T22:03:42.438741Z","shell.execute_reply":"2025-06-02T22:03:42.444213Z"}},"outputs":[{"name":"stdout","text":"Total notebook execution time: 954.98 seconds\n","output_type":"stream"}],"execution_count":32}]}