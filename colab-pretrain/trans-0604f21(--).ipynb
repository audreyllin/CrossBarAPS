{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12062706,"sourceType":"datasetVersion","datasetId":7592589}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:26:31.307435Z","iopub.execute_input":"2025-06-04T22:26:31.307855Z","iopub.status.idle":"2025-06-04T22:26:31.318730Z","shell.execute_reply.started":"2025-06-04T22:26:31.307816Z","shell.execute_reply":"2025-06-04T22:26:31.316977Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Environment Setup - FIXED\n# =================================\nimport os\nimport sys\nimport json\nimport shutil\nimport numpy as np\nimport torch\nimport transformers\nfrom datasets import Dataset, load_dataset, DatasetDict\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"NO_TF\"] = \"1\"  # Prevent TensorFlow import issues\n\n# Install required packages\n!pip uninstall -y tensorflow  # Remove to prevent conflicts\n!pip install --upgrade pip setuptools wheel\n!pip install numpy==1.26.4 scipy==1.11.4\n!pip install torch==2.2.1+cpu torchvision==0.17.1+cpu torchaudio==2.2.1+cpu --index-url https://download.pytorch.org/whl/cpu\n!pip install transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install einops==0.7.0 tokenizers==0.19.1 sentencepiece==0.2.0\n!pip install scikit-learn==1.2.2 matplotlib==3.7.2\n!pip install langchain==0.1.16 faiss-cpu==1.7.4 tqdm==4.66.2 pandas==2.2.2\n\n# Verify installations\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:26:31.320711Z","iopub.execute_input":"2025-06-04T22:26:31.321036Z","iopub.status.idle":"2025-06-04T22:27:03.743717Z","shell.execute_reply.started":"2025-06-04T22:26:31.321011Z","shell.execute_reply":"2025-06-04T22:27:03.741799Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.11/dist-packages (1.11.4)\nLooking in indexes: https://download.pytorch.org/whl/cpu\nRequirement already satisfied: torch==2.2.1+cpu in /usr/local/lib/python3.11/dist-packages (2.2.1+cpu)\nRequirement already satisfied: torchvision==0.17.1+cpu in /usr/local/lib/python3.11/dist-packages (0.17.1+cpu)\nRequirement already satisfied: torchaudio==2.2.1+cpu in /usr/local/lib/python3.11/dist-packages (2.2.1+cpu)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (4.13.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (2023.10.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.1+cpu) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.1+cpu) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.1+cpu) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.1+cpu) (1.3.0)\nRequirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.11/dist-packages (4.41.2)\nRequirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\nRequirement already satisfied: datasets==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: accelerate==0.29.1 in /usr/local/lib/python3.11/dist-packages (0.29.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.66.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (2.2.1+cpu)\nRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (19.0.1)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.7)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.3.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.70.15)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0) (2023.10.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.20.0)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.18.0) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.4.26)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\nRequirement already satisfied: einops==0.7.0 in /usr/local/lib/python3.11/dist-packages (0.7.0)\nRequirement already satisfied: tokenizers==0.19.1 in /usr/local/lib/python3.11/dist-packages (0.19.1)\nRequirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers==0.19.1) (0.31.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2023.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.66.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (1.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.4.26)\nRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (23.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2) (1.17.0)\nRequirement already satisfied: langchain==0.1.16 in /usr/local/lib/python3.11/dist-packages (0.1.16)\nRequirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.11/dist-packages (1.7.4)\nRequirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.11/dist-packages (4.66.2)\nRequirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.0.40)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (3.11.18)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.6.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.0.38)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.1.53)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.0.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.1.147)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.11.4)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.32.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (8.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.20.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16) (3.0.0)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain==0.1.16) (23.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.16) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.16) (2.4.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.1.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.3.1)\n\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.2.1+cpu\nTransformers: 4.41.2\nCUDA available: False\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Model Loading - FIXED\n# ============================\nMODEL_NAME = \"gpt2\"\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics\"\"\"\n    import psutil\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent:.1f}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    device = \"cpu\"\n    torch_dtype = torch.float32\n    \n    try:\n        print(\"Attempting standard CPU load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=None,\n            torch_dtype=torch_dtype\n        ).to(device)\n        print(\"\\n‚úÖ Model loaded successfully on CPU!\")\n        return model\n    except Exception as e:\n        print(f\"\\n‚ùå Standard load failed: {str(e)}\")\n        raise RuntimeError(\"Unable to load model on CPU\")\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:27:03.744928Z","iopub.execute_input":"2025-06-04T22:27:03.746045Z","iopub.status.idle":"2025-06-04T22:27:04.626338Z","shell.execute_reply.started":"2025-06-04T22:27:03.746008Z","shell.execute_reply":"2025-06-04T22:27:04.624282Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 4.5% (1.0/31.4GB)\nAttempting standard CPU load...\n\n‚úÖ Model loaded successfully on CPU!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup - FIXED\n# ==============================\ndef load_tokenizer(model_name):\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:27:04.628802Z","iopub.execute_input":"2025-06-04T22:27:04.629263Z","iopub.status.idle":"2025-06-04T22:27:05.013010Z","shell.execute_reply.started":"2025-06-04T22:27:04.629231Z","shell.execute_reply":"2025-06-04T22:27:05.011534Z"}},"outputs":[{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Dataset Preparation - FIXED\n# ==================================\ndef prepare_dataset(file_path=\"/kaggle/input/database4\", max_samples=1000):\n    try:\n        # Check if path exists\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Dataset path not found: {file_path}\")\n        \n        print(f\"Loading dataset from: {file_path}\")\n        \n        # Check if it's a directory or file\n        if os.path.isdir(file_path):\n            # Load all JSON files in directory\n            json_files = [f for f in os.listdir(file_path) if f.endswith('.json')]\n            if not json_files:\n                raise ValueError(f\"No JSON files found in directory: {file_path}\")\n            \n            data_files = [os.path.join(file_path, f) for f in json_files]\n            print(f\"Found {len(json_files)} JSON files\")\n            dataset = load_dataset('json', data_files=data_files, split=f'train[:{max_samples}]')\n        else:\n            # Single file\n            dataset = load_dataset('json', data_files=file_path, split=f'train[:{max_samples}]')\n        \n        # Ensure text column exists\n        if 'text' not in dataset.column_names:\n            # Try to find a text-like column\n            text_candidates = [col for col in dataset.column_names \n                              if any(keyword in col.lower() for keyword in ['text', 'content', 'body', 'article'])]\n            \n            if text_candidates:\n                print(f\"Renaming column '{text_candidates[0]}' to 'text'\")\n                dataset = dataset.rename_column(text_candidates[0], 'text')\n            else:\n                # If no text-like column, concatenate all string columns\n                print(\"Concatenating all string columns to create 'text'\")\n                string_cols = [col for col in dataset.column_names if dataset.features[col].dtype == 'string']\n                \n                def combine_columns(examples):\n                    return {'text': ' '.join(str(examples[col]) for col in string_cols)}\n                \n                dataset = dataset.map(combine_columns, batched=True)\n        \n        print(f\"‚úÖ Loaded dataset with {len(dataset)} samples\")\n        return dataset\n    \n    except Exception as e:\n        print(f\"\\n‚ùå Dataset loading failed: {str(e)}\")\n        print(\"Creating fallback dataset...\")\n        return create_fallback_dataset()\n\ndef create_fallback_dataset():\n    \"\"\"Create cryptocurrency sample dataset\"\"\"\n    sample_texts = [\n        \"Cryptocurrency is a digital asset designed to work as a medium of exchange.\",\n        \"Blockchain technology enables secure peer-to-peer transactions.\",\n        \"Bitcoin was the first decentralized cryptocurrency created in 2009.\",\n        \"Ethereum introduced smart contracts to blockchain technology.\",\n        \"DeFi (Decentralized Finance) aims to recreate traditional financial systems without intermediaries.\",\n        \"NFTs (Non-Fungible Tokens) represent unique digital assets on the blockchain.\",\n        \"Cryptocurrency mining involves validating transactions and adding them to the blockchain.\",\n        \"Stablecoins are cryptocurrencies pegged to stable assets like the US dollar.\",\n        \"Cryptocurrency exchanges allow users to trade digital assets.\",\n        \"Wallet security is crucial for protecting cryptocurrency holdings.\"\n    ]\n    return Dataset.from_dict({\"text\": sample_texts})\n\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with error handling\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=128,  # Reduced length for efficiency\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except Exception:\n        return {\n            \"input_ids\": [[0]*128],\n            \"attention_mask\": [[1]*128],\n            \"labels\": [[0]*128]\n        }\n\nprint(\"\\n=== Starting Data Processing ===\")\ndataset = prepare_dataset(\"/kaggle/input/database4\")\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(safe_tokenize, batched=True, batch_size=4)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Split dataset\nsplit_datasets = tokenized_dataset.train_test_split(test_size=0.2)\ntokenized_dataset = DatasetDict({\n    \"train\": split_datasets[\"train\"],\n    \"test\": split_datasets[\"test\"]\n})\nprint(f\"‚úÖ Dataset split: {len(tokenized_dataset['train'])} train, {len(tokenized_dataset['test'])} test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:27:05.014481Z","iopub.execute_input":"2025-06-04T22:27:05.014812Z","iopub.status.idle":"2025-06-04T22:27:05.192248Z","shell.execute_reply.started":"2025-06-04T22:27:05.014780Z","shell.execute_reply":"2025-06-04T22:27:05.190488Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Data Processing ===\nLoading dataset from: /kaggle/input/database4\n\n‚ùå Dataset loading failed: No JSON files found in directory: /kaggle/input/database4\nCreating fallback dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76db970d346b4cf7a728d1db43f5471c"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Dataset split: 8 train, 2 test\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 5: Training Configuration - FIXED\n# =====================================\n# Enable gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# LoRA configuration\npeft_config = LoraConfig(\n    r=8,  # Reduced from 16 for CPU efficiency\n    lora_alpha=16,\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training arguments optimized for CPU\ntraining_args = TrainingArguments(\n    output_dir=f\"./{MODEL_NAME}-crypto-expert\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,  # Reduced accumulation steps\n    num_train_epochs=1,\n    learning_rate=1e-4,  # Lower learning rate for CPU\n    optim=\"adamw_torch\",\n    logging_steps=5,\n    # FIXED: Match evaluation and save strategies\n    evaluation_strategy=\"epoch\",  # Changed to match save_strategy\n    save_strategy=\"epoch\",\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    # FIXED: Use updated parameter name\n    use_cpu=True  # Instead of deprecated no_cuda=True\n)\n\n# Prepare model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:27:05.194762Z","iopub.execute_input":"2025-06-04T22:27:05.195116Z","iopub.status.idle":"2025-06-04T22:27:05.277288Z","shell.execute_reply.started":"2025-06-04T22:27:05.195082Z","shell.execute_reply":"2025-06-04T22:27:05.273559Z"}},"outputs":[{"name":"stdout","text":"trainable params: 442,368 || all params: 124,882,176 || trainable%: 0.35422829275492446\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 6: Training Execution - FIXED\n# =================================\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    model.config.use_cache = False  # Disable cache for gradient checkpointing\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"test\"],\n        data_collator=data_collator\n    )\n    \n    print(\"\\n=== Starting Training ===\")\n    trainer.train()\n    \n    # Save model\n    output_dir = training_args.output_dir\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"\\n‚úÖ Model saved to {output_dir}\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:27:05.279438Z","iopub.execute_input":"2025-06-04T22:27:05.279952Z","iopub.status.idle":"2025-06-04T22:27:18.562398Z","shell.execute_reply.started":"2025-06-04T22:27:05.279904Z","shell.execute_reply":"2025-06-04T22:27:18.560828Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Training ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:06, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>3.624588</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Model saved to ./gpt2-crypto-expert\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 7: Enhanced Model Saving with Shard Support\n# ===============================================\n\n# Add missing import\nfrom typing import Optional\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\nüíæ Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"üíΩ Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"üî§ Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"üìù Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"‚ö†Ô∏è Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n‚ö†Ô∏è Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\nüîç Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:29:15.933109Z","iopub.execute_input":"2025-06-04T22:29:15.933607Z","iopub.status.idle":"2025-06-04T22:29:15.946980Z","shell.execute_reply.started":"2025-06-04T22:29:15.933576Z","shell.execute_reply":"2025-06-04T22:29:15.945466Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 8: Robust Model Loading and Testing with PEFT support - FIXED\n# ========================================================\n# Add missing imports\nfrom peft import PeftModel\nfrom transformers import pipeline\n\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 160,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\nüîç Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\nüìÇ Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\nüîÑ Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        print(\"\\nüîÑ Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"‚ö†Ô∏è No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.float32\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    \"gpt2\",  # Load original base model\n                    torch_dtype=torch.float32\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float32\n            )\n            \n        print(\"\\nüéâ Model loaded successfully!\")\n        model.eval()  # Set to evaluation mode\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline\n        print(\"\\nüöÄ Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            device=-1  # Force CPU usage\n        )\n        \n        # Run tests\n        print(\"\\nüß™ Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\nüîπ Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"üí¨ Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Critical error loading model: {str(e)}\")\n        print(\"\\nüõ†Ô∏è Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:34:13.549555Z","iopub.execute_input":"2025-06-04T22:34:13.550243Z","iopub.status.idle":"2025-06-04T22:34:13.995675Z","shell.execute_reply.started":"2025-06-04T22:34:13.550196Z","shell.execute_reply":"2025-06-04T22:34:13.994624Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    model_path = \"/kaggle/working/gpt2-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    load_and_test_model(model_path, is_peft_model=True)\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \"What is software wallet, and what's the difference between hardware and software wallet? \",\n        \"What is PoW? \",\n        \"Explain PoW in 1 sentence. \",\n        \"Describe the key features of PoW using 3 words. \",\n        \"What is PoM? Is it something related to cryptography? \",\n        \"What is a cryptographic product? \",\n        \"What is P2P? \",\n        \"What is block chain? \",\n        \"What is public key, and what's the difference between private and public key? \"\n    ]\n    load_and_test_model(model_path, test_prompts=custom_prompts, is_peft_model=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:34:13.999050Z","iopub.execute_input":"2025-06-04T22:34:13.999788Z","iopub.status.idle":"2025-06-04T22:35:51.943357Z","shell.execute_reply.started":"2025-06-04T22:34:13.999758Z","shell.execute_reply":"2025-06-04T22:35:51.941581Z"}},"outputs":[{"name":"stdout","text":"\nüíæ Saving model artifacts to: /kaggle/working/gpt2-lora-trained\nüíΩ Saving model and adapter...\nüî§ Saving tokenizer...\nüìù Saving training arguments...\n\nüîç Verifying saved files:\n- merges.txt (445.62 KB)\n- adapter_model.safetensors (1733.91 KB)\n- README.md (4.96 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.82 KB)\n- adapter_config.json (0.62 KB)\n- vocab.json (779.45 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n\nüîç Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\nüìÇ Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.62 KB)\n- adapter_model.safetensors (1733.91 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.82 KB)\n- vocab.json (779.45 KB)\n\nüîÑ Loading tokenizer...\n\nüîÑ Loading model...\nFound adapter files: ['adapter_model.safetensors', 'adapter_config.json']\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\nüéâ Model loaded successfully!\n\nüöÄ Creating text generation pipeline...\n\nüß™ Running generation tests...\n\nüîπ Test 1: What is hardware wallet?? \nüí¨ Response: What is hardware wallet?? ???\n\n\nI do not have a physical machine but I can read the instructions on my PC, and it works fine. It also comes with USB 2.0 support which allows me to use any mobile device (with an internet connection) without having your phone in play! However, if you're using Android phones or tablets that don't include SD cards... then please take note of this FAQ!! My only other question: How will i get Bitcoin? You must add all required permissions for both bitcoin address(s), transaction amount/time block header etc.. The answer may be different depending upon what kind crypto currency would work best as opposed just accepting digital payments from BTC addresses so longas they are valid; e-mailing them via their verified email account has been\n\nüîπ Test 2: What is Proof of Work (PoW)?? \nüí¨ Response: What is Proof of Work (PoW)?? ¬†\"If I have a proof of work, then the person who has it can claim that he/she made up their mind about how to make money. If there's no evidence against them for making such claims...\"\nSo what does PoS do? Does anyone actually write code in Java or Python? Or even C++! The reason why these are hard coded languages like Javascript and PHP. But where did they come from?! Well you need good documentation on your website if this isn't something people want! So here we go :- )\n\nüîπ Test 3: What is cryptography?? \nüí¨ Response: What is cryptography?? ¬†How do I encrypt my email?\nA little background on crypto. Cryptography refers to the concept that a cryptographic algorithm uses random numbers or pseudorandom number generators (SNGs). They are often used by researchers and security experts alike for storing encrypted information such as passwords, user credentials etc.. The encryption algorithms use special bits of data from many different sources: computers with memory cards; mobile phones containing smart phone cameras ; large quantities in storage devices like laptops, tablets & printers. A good example would be an app called Telegram which allows you access your personal messages online via SMS/email when there's no sign-in required - just click \"send\". This gives users more control over their private communications without having to trust third parties who will keep track if they get\n\nüîπ Test 4: What is Peer-to-Peer (P2P)?? \nüí¨ Response: What is Peer-to-Peer (P2P)?? ¬†How do you get your data? How are they stored and used in the cloud, which services can I use to access it? Where should my information be kept for a period of time before being sent back with new requests or if there's nothing we could find that would give me an answer. What about bulk eCommerce service providers like Amazon Web Services who offer direct mail from users on their web browsers so customers don't have any need go into those servers themselves as much when using P2Ps where no user has ever been asked what kind email address was given them by this company??? There really isn¬¥t anything wrong here! This issue just seems obvious now because these companies know how important p2p communication technology truly\n\nüîπ Test 5: What is block chain?? \nüí¨ Response: What is block chain?? ¬†That's how it works. It requires a lot of time, and can be expensive to implement (not that I'd recommend doing so).\nWhy do you want to use the Block Chain? Why are there any disadvantages when using this in production? Let me know what your thoughts on whether or not we should consider implementing these changes into our application if possible. In fact, let us see some examples for those who would like more detail: 1) A Simple User-Agent Example 2), How We Can Improve Security 3): An API Client 4), What Kinds Of Stuff You Should Include 5), And More! 6) If There Is Any Use For This Feature 7/4 - 9\n\nüîπ Test 6: What is private key?? \nüí¨ Response: What is private key?? ????\nWell it depends on your type of device and how you use the internet. It's possible that this may be an issue with a computer or mobile phone, but I can't see any reason to assume otherwise - there are some features in Chrome for which users don' need these permissions (iPad has such permission lists). There have been many forums where people describe issues related directly unto their own devices as well. My guess would probably be if someone was using Windows XP running Opera 7/8 instead! If not then what?\n\nüîç Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\nüìÇ Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.62 KB)\n- adapter_model.safetensors (1733.91 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.82 KB)\n- vocab.json (779.45 KB)\n\nüîÑ Loading tokenizer...\n\nüîÑ Loading model...\nFound adapter files: ['adapter_model.safetensors', 'adapter_config.json']\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\nüéâ Model loaded successfully!\n\nüöÄ Creating text generation pipeline...\n\nüß™ Running generation tests...\n\nüîπ Test 1: What is software wallet, and what's the difference between hardware and software wallet? \nüí¨ Response: What is software wallet, and what's the difference between hardware and software wallet? ¬†It has two parts: a physical card that can store data on it (such as your phone number) or an online account where you may send money to other people. There are no separate accounts in Bitcoin wallets for any of these purposes; they just hold transactions sent from one computer into another at different points over time using addresses found within each transaction record so there isn't much need when trying out new things like bitcoin-like apps which allow users access multiple computers simultaneously while storing all their information locally across devices such Asynchronous Payment System.\nIn this article we'll look briefly how most developers use Linux Wallet, including its builtin support! We will also be covering some basic concepts about operating systems used by cryptocurrency\n\nüîπ Test 2: What is PoW? \nüí¨ Response: What is PoW? ¬†It's a program that allows users to run programs on the Internet through an online browser, rather than by downloading files or using FTP. The idea behind it is simple: instead of having every user download one file at once (and then start over), each time they click \"Done\" as if this were just another web page (\"done\"), there will be only five downloads per day and no additional information for those who choose not-to-downloads links back into their computers after clicking some more buttons in order get started with them again!\nSo how does PWN work?! ¬† Here are six tips from my own experience running OpenVPN...\n\nüîπ Test 3: Explain PoW in 1 sentence. \nüí¨ Response: Explain PoW in 1 sentence. ¬†Do not use this word \"do not know\", as the definition of a polynomial is much more ambiguous than that, but it's possible to find out what you're doing by looking at your code and observing how many lines are taken up during each line (the output will be an array).\nHere I am using some basic math for making sure my variable names aren't confusingly similar: $variableName = 'foo' ; // Assume variables with no underscore join(' foo', function () { return true; }); } )\n\nüîπ Test 4: Describe the key features of PoW using 3 words. \nüí¨ Response: Describe the key features of PoW using 3 words. ¬†Here is a short summary:\nThe following information about \"Po-Seed\" was provided by Greg Nye (https://githubusercontent, github!com/gmaxwell) in his recent post on Bitcoin Core and Cryptocurrency Mining at http:/ /bitcointalkfoundation/, where he wrote that it's not just some generic word like SHA1 or BIP148 which are used for cryptographic hashes; there have been other uses as well including pseudorandom numbers such to create consensus blocks with random data but they're all very different from this one [ https. ]\n\n\n\nüîπ Test 5: What is PoM? Is it something related to cryptography? \nüí¨ Response: What is PoM? Is it something related to cryptography? ¬†I think that the answer depends on what you mean by \"crypto\". I'm not sure if there are any cryptographic terms in this post, but please feel free to add your own.\nWhy crypto-based software development isn't as fun or secure (or at least more reliable)? ¬† We use cryptosystems for many purposes and they all have their advantages: we can write smart contracts with no risk of being compromised; our clients don' t care about how hard a user tries, nor do developers want to rely solely upon code from third parties who might be able modify things like security/performance data storage. On top though, these kinds ciphers make them even less useful because most people will never know exactly\n\nüîπ Test 6: What is a cryptographic product? \nüí¨ Response: What is a cryptographic product? ¬†This term has been coined in the field of cryptography for many years, but it seems to be more popular now than ever.\nA good example would come from an article about Cryptography and Information Security by Alex Eberhard: In this post I discuss how data encryption works; as well its use with software such Asymmetric Random Access Memory (ASRAM). The paper describes ASR's implementation on top-of AEMM which will allow you access to your machine without having TOO much trouble or expense if needed - because all that needs are two pieces! Here we have another case where one piece can store information when both parts share memory space using different algorithms instead...\n\nüîπ Test 7: What is P2P? \nüí¨ Response: What is P2P? ¬†It's a form of social networking, where people share their information and ideas about different things. It allows users to chat in real-time with each other while sharing knowledge on topics such as race relations or health care issues (see below).\nHow does it work? Well, the first thing you need are some basic internet connections: your phone number, email address and password. Now that we know what kind there really was at our house last week, let us talk more... What do I get when connecting online over pinterest / Reddit/ etc.? One quick note - these sites allow anyone who wants access via ebay accounts from anywhere worldwide for free use without any fees! For example if someone has an account using Amazon Prime they can connect directly into this\n\nüîπ Test 8: What is block chain? \nüí¨ Response: What is block chain? ¬†It's a mechanism that allows you to link two different blocks of information into one. In other words, the most important thing in any transaction will be its inputs and outputs; for example, if I buy an expensive car on eBay or Amazon then my money goes directly onto their servers rather than being transferred over via Bitcoin (which would also allow me not only to sell your house but purchase goods from them).\nIn addition this means there are many ways where someone can spend bitcoins without having had access at all: they could send funds through Bitcointalk, go out with some friends online which has already been used by several hundred people who have bought bitcoin as collateral ‚Äì simply because it gives more anonymity among users while allowing everyone else within those communities much greater control about\n\nüîπ Test 9: What is public key, and what's the difference between private and public key? \nüí¨ Response: What is public key, and what's the difference between private and public key? ¬†\"You can't tell me which one to use. I don' know why you're using it.\"\nThere are two types of keys: Public keys (or PKCS#) that have been used in a database for more than 3 years; or Private keys. The first type has become popular with users who want access only when they need certain functions such as \"send messages\". As an example, if someone needs your email address but doesn''t like sending him letters from his home office... he could always call my telephone number through this form-only password system called OpenSSH - no further questions asked! And then there are those where people would just send mail without knowing anything about their own personal\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T22:35:51.945889Z","iopub.execute_input":"2025-06-04T22:35:51.946434Z","iopub.status.idle":"2025-06-04T22:35:51.953247Z","shell.execute_reply.started":"2025-06-04T22:35:51.946358Z","shell.execute_reply":"2025-06-04T22:35:51.952211Z"}},"outputs":[{"name":"stdout","text":"Total notebook execution time: 560.63 seconds\n","output_type":"stream"}],"execution_count":14}]}
