{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom typing import List, Dict, Optional\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T22:33:43.745796Z","iopub.execute_input":"2025-05-19T22:33:43.746155Z","iopub.status.idle":"2025-05-19T22:33:43.753486Z","shell.execute_reply.started":"2025-05-19T22:33:43.746122Z","shell.execute_reply":"2025-05-19T22:33:43.751432Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Configuration\nclass Config:\n    DB_PATH = \"qna_database.db\"\n    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n    CHUNK_SIZE = 500  # For batch processing\n    VECTOR_DIM = 384  # Dimension for all-MiniLM-L6-v2 embeddings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T22:33:43.754779Z","iopub.execute_input":"2025-05-19T22:33:43.755195Z","iopub.status.idle":"2025-05-19T22:33:43.780408Z","shell.execute_reply.started":"2025-05-19T22:33:43.755155Z","shell.execute_reply":"2025-05-19T22:33:43.779325Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Database Manager\nclass QnADatabase:\n    def __init__(self, db_path: str = Config.DB_PATH):\n        \"\"\"Initialize with optimized SQLite settings\"\"\"\n        self.db_path = db_path\n        self.conn = None\n        self.embedding_model = None\n        self._initialize_db()\n        \n    def _initialize_db(self):\n        \"\"\"Create database with optimized schema\"\"\"\n        self.conn = sqlite3.connect(self.db_path, timeout=30)\n        self.conn.execute(\"PRAGMA journal_mode = WAL\")\n        self.conn.execute(\"PRAGMA synchronous = NORMAL\")\n        self.conn.execute(\"PRAGMA cache_size = -100000\")  # 100MB cache\n        \n        # Main Q&A table\n        self.conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS qna_pairs (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            question TEXT NOT NULL,\n            answer TEXT NOT NULL,\n            category TEXT,\n            word_count INTEGER,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            last_accessed TIMESTAMP,\n            usage_count INTEGER DEFAULT 0\n        )\"\"\")\n        \n        # Vector embeddings table\n        self.conn.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS qna_embeddings (\n            qna_id INTEGER PRIMARY KEY,\n            question_vector BLOB,\n            answer_vector BLOB,\n            FOREIGN KEY (qna_id) REFERENCES qna_pairs(id)\n        )\"\"\")\n        \n        # Create indexes\n        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_category ON qna_pairs(category)\")\n        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_word_count ON qna_pairs(word_count)\")\n        \n        # Full-text search\n        self.conn.execute(\"\"\"\n        CREATE VIRTUAL TABLE IF NOT EXISTS qna_search \n        USING fts5(question, answer, tokenize='porter unicode61')\n        \"\"\")\n\n    def _get_embedding_model(self):\n        \"\"\"Lazy load embedding model\"\"\"\n        if self.embedding_model is None:\n            self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)\n        return self.embedding_model\n\n    def _text_to_vector(self, text: str) -> bytes:\n        \"\"\"Convert text to compressed vector\"\"\"\n        model = self._get_embedding_model()\n        vector = model.encode(text)\n        return vector.tobytes()\n\n    def _vector_to_array(self, blob: bytes) -> np.ndarray:\n        \"\"\"Convert blob back to numpy array\"\"\"\n        return np.frombuffer(blob, dtype=np.float32)\n\n    def batch_insert(self, qna_list: List[Dict]):\n        \"\"\"Optimized bulk insert with duplicate prevention\"\"\"\n        if not qna_list:\n            return\n            \n        with self.conn:\n            cursor = self.conn.cursor()\n            \n            # Check for existing questions to prevent duplicates\n            existing_questions = set()\n            cursor.execute(\"SELECT question FROM qna_pairs\")\n            for row in cursor.fetchall():\n                existing_questions.add(row[0].strip().lower())\n            \n            # Filter out duplicates\n            unique_qna = []\n            for qna in qna_list:\n                norm_question = qna['question'].strip().lower()\n                if norm_question not in existing_questions:\n                    unique_qna.append(qna)\n                    existing_questions.add(norm_question)\n            \n            if not unique_qna:\n                print(\"No new Q&A pairs to insert\")\n                return\n            \n            # Insert only unique Q&A pairs\n            cursor.executemany(\"\"\"\n            INSERT OR IGNORE INTO qna_pairs (question, answer, category, word_count)\n            VALUES (?, ?, ?, ?)\n            \"\"\", [(q['question'], q['answer'], q.get('category'), \n                  len(q['answer'].split())) for q in unique_qna])\n\n    def semantic_search(self, query: str, top_k: int = 5, threshold: float = 0.7):\n        \"\"\"Updated semantic search with unique results\"\"\"\n        cursor = self.conn.cursor()\n        \n        # Get query embedding\n        query_vec = self._text_to_vector(query)\n        query_arr = self._vector_to_array(query_vec)\n        \n        # Find similar questions with grouping\n        cursor.execute(\"\"\"\n        SELECT qna_id, question_vector \n        FROM qna_embeddings\n        GROUP BY qna_id  -- Ensure unique entries\n        \"\"\")\n        \n        results = []\n        seen_questions = set()\n        \n        for qna_id, q_vec_blob in cursor.fetchall():\n            q_vec = self._vector_to_array(q_vec_blob)\n            similarity = cosine_similarity([query_arr], [q_vec])[0][0]\n            if similarity >= threshold:\n                # Get the actual question text\n                cursor.execute(\"SELECT question, answer FROM qna_pairs WHERE id = ?\", (qna_id,))\n                question, answer = cursor.fetchone()\n                \n                # Deduplicate by question text\n                norm_question = question.strip().lower()\n                if norm_question not in seen_questions:\n                    results.append((question, answer, similarity))\n                    seen_questions.add(norm_question)\n\n    # Sort by similarity and get top unique results\n        results.sort(key=lambda x: x[2], reverse=True)\n        return [(q, a) for q, a, _ in results[:top_k]]\n\n    def keyword_search(self, query: str, limit: int = 5):\n        \"\"\"Traditional keyword search\"\"\"\n        cursor = self.conn.cursor()\n        \n        # Try FTS first\n        cursor.execute(\"\"\"\n        SELECT question, answer \n        FROM qna_search \n        WHERE qna_search MATCH ?\n        ORDER BY rank\n        LIMIT ?\n        \"\"\", (f'\"{query}\"', limit))\n        \n        results = cursor.fetchall()\n        \n        # Fallback to LIKE if no results\n        if not results:\n            cursor.execute(\"\"\"\n            SELECT question, answer \n            FROM qna_pairs \n            WHERE question LIKE ? OR answer LIKE ?\n            LIMIT ?\n            \"\"\", (f'%{query}%', f'%{query}%', limit))\n            results = cursor.fetchall()\n            \n        return results\n\n    def get_all_data(self, limit: Optional[int] = None):\n        \"\"\"Export all data with optional limit\"\"\"\n        query = \"SELECT * FROM qna_pairs\"\n        if limit:\n            query += f\" LIMIT {limit}\"\n        return pd.read_sql(query, self.conn)\n\n    def optimize(self):\n        \"\"\"Database maintenance\"\"\"\n        print(\"Optimizing database...\")\n        self.conn.execute(\"VACUUM\")\n        self.conn.execute(\"ANALYZE\")\n        self.conn.execute(\"PRAGMA optimize\")\n\n    def close(self):\n        \"\"\"Clean up resources\"\"\"\n        if self.conn:\n            self.conn.close()\n        if self.embedding_model:\n            del self.embedding_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T22:33:43.781754Z","iopub.execute_input":"2025-05-19T22:33:43.782121Z","iopub.status.idle":"2025-05-19T22:33:43.805551Z","shell.execute_reply.started":"2025-05-19T22:33:43.782082Z","shell.execute_reply":"2025-05-19T22:33:43.804335Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Data Processing Utilities\nclass DataProcessor:\n    @staticmethod\n    def parse_text_file(file_path: str, question_prefix: str = \"Q:\", answer_prefix: str = \"A:\"):\n        \"\"\"Parse Q&A from text files with paragraph answers\"\"\"\n        qna_pairs = []\n        current_q = None\n        current_a = []\n        \n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith(question_prefix):\n                    if current_q is not None:\n                        qna_pairs.append({\n                            \"question\": current_q,\n                            \"answer\": \"\\n\".join(current_a).strip()\n                        })\n                    current_q = line[len(question_prefix):].strip()\n                    current_a = []\n                elif line.startswith(answer_prefix):\n                    current_a.append(line[len(answer_prefix):].strip())\n                elif current_a and line:\n                    current_a.append(line)\n            \n            # Add the last pair\n            if current_q is not None:\n                qna_pairs.append({\n                    \"question\": current_q,\n                    \"answer\": \"\\n\".join(current_a).strip()\n                })\n                \n        return qna_pairs\n\n    @staticmethod\n    def chunk_list(lst, chunk_size):\n        \"\"\"Yield successive chunk_size chunks from lst\"\"\"\n        for i in range(0, len(lst), chunk_size):\n            yield lst[i:i + chunk_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T22:33:43.806610Z","iopub.execute_input":"2025-05-19T22:33:43.806942Z","iopub.status.idle":"2025-05-19T22:33:43.835704Z","shell.execute_reply.started":"2025-05-19T22:33:43.806917Z","shell.execute_reply":"2025-05-19T22:33:43.834628Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def main():\n    # Initialize database with cleanup\n    if os.path.exists(Config.DB_PATH):\n        os.remove(Config.DB_PATH)\n    db = QnADatabase()\n    \n    # Sample data for demonstration\n    sample_data = [\n        {\n            \"question\": \"What is P2P? \",\n            \"answer\": \"A Peer-to-Peer (P2P) payment system, seamlessly integrated with blockchain technology, a decentralized application (DApp), and MetaMask wallet, orchestrates a streamlined and secure process for transparent transactions among users.\"\n        },\n        {\n            \"question\": \"Compare and Contrast Private and Public Key: \",\n            \"answer\": \"The private key allows you to have access to your funds through the crypto wallet. it is used to send Bitcoin and must be protected and secured. As for the public key, it is used to receive Bitcoin and can be published anywhere safely.\"\n        }\n    ]\n    \n    # Auto-categorize questions\n    def detect_category(question: str) -> str:\n        question_lower = question.lower()\n        if 'p2p' in question_lower or 'peer-to-peer' in question_lower:\n            return \"networking\"\n        elif 'private key' in question_lower or 'public key' in question_lower:\n            return \"security\"\n        elif 'blockchain' in question_lower:\n            return \"fundamentals\"\n        elif 'proof of work' in question_lower or 'pow' in question_lower:\n            return \"consensus\"\n        elif 'wallet' in question_lower:\n            return \"wallets\"\n        elif 'smart contract' in question_lower:\n            return \"development\"\n        else:\n            return \"general\"\n    \n    # Add categories to sample data\n    for item in sample_data:\n        item[\"category\"] = detect_category(item[\"question\"])\n    \n    # Process and insert data\n    print(\"Inserting sample data...\")\n    db.batch_insert(sample_data)\n    \n    # For large files\n    try:\n        file_path = \"/kaggle/input/db-19-txt\"\n        if os.path.exists(file_path):\n            print(\"Processing large file...\")\n            qna_pairs = DataProcessor.parse_text_file(file_path)\n            \n            # Auto-categorize parsed questions\n            for item in qna_pairs:\n                item[\"category\"] = detect_category(item[\"question\"])\n            \n            print(f\"Processing {len(qna_pairs)} Q&A pairs...\")\n            for chunk in DataProcessor.chunk_list(qna_pairs, Config.CHUNK_SIZE):\n                db.batch_insert(chunk)\n    except Exception as e:\n        print(f\"Error processing large file: {e}\")\n    \n    # Example searches\n    print(\"\\nSemantic search results for 'P2P':\")\n    print(db.semantic_search(\"P2P\"))\n    \n    print(\"\\nKeyword search results for 'private key':\")\n    print(db.keyword_search(\"private key\"))\n    \n    # Export data with categories\n    df = db.get_all_data(limit=10)\n    print(\"\\nSample data from database:\")\n    print(df[['question', 'category']].head())\n    \n    # Show category distribution\n    print(\"\\nCategory distribution:\")\n    print(df['category'].value_counts())\n    \n    # Maintenance\n    db.optimize()\n    db.close()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T22:33:43.836607Z","iopub.execute_input":"2025-05-19T22:33:43.836997Z","iopub.status.idle":"2025-05-19T22:33:45.197739Z","shell.execute_reply.started":"2025-05-19T22:33:43.836933Z","shell.execute_reply":"2025-05-19T22:33:45.196626Z"}},"outputs":[{"name":"stdout","text":"Inserting sample data...\n\nSemantic search results for 'P2P':\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f484d09ba0844c31b226be9060ec789f"}},"metadata":{}},{"name":"stdout","text":"[]\n\nKeyword search results for 'private key':\n[('Compare and Contrast Private and Public Key: ', 'The private key allows you to have access to your funds through the crypto wallet. it is used to send Bitcoin and must be protected and secured. As for the public key, it is used to receive Bitcoin and can be published anywhere safely.')]\n\nSample data from database:\n                                        question    category\n0                                  What is P2P?   networking\n1  Compare and Contrast Private and Public Key:     security\n\nCategory distribution:\ncategory\nnetworking    1\nsecurity      1\nName: count, dtype: int64\nOptimizing database...\n","output_type":"stream"}],"execution_count":25}]}