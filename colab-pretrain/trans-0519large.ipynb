{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom typing import List, Dict, Optional, Tuple, Union\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport string\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:55:38.122686Z","iopub.execute_input":"2025-05-20T16:55:38.123009Z","iopub.status.idle":"2025-05-20T16:55:38.128817Z","shell.execute_reply.started":"2025-05-20T16:55:38.122977Z","shell.execute_reply":"2025-05-20T16:55:38.127787Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Configuration\nclass Config:\n    DB_PATH = \"qna_database.db\"\n    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n    CHUNK_SIZE = 500  # For batch processing\n    VECTOR_DIM = 384  # Dimension for all-MiniLM-L6-v2 embeddings\n    MIN_WORD_LENGTH = 3  # Minimum word length for keyword processing\n    STOPWORDS = {\n        'the', 'and', 'of', 'in', 'to', 'a', 'is', 'for', 'on', 'that', 'it', 'with', 'as', 'be', 'by', 'this', 'are', 'at'\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:55:38.130211Z","iopub.execute_input":"2025-05-20T16:55:38.130618Z","iopub.status.idle":"2025-05-20T16:55:38.148949Z","shell.execute_reply.started":"2025-05-20T16:55:38.130593Z","shell.execute_reply":"2025-05-20T16:55:38.147918Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import sqlite3\nimport re\nfrom typing import List, Dict, Tuple, Optional\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass QnADatabase:\n    def __init__(self, db_path: str = Config.DB_PATH):\n        \"\"\"Initialize with optimized SQLite settings\"\"\"\n\n        self.synonym_map = {\n            'wallet': 'wallet hot cold storage address',\n            'miner': 'mining validator node blockchain',\n            'p2p': 'peer-to-peer decentralized distributed',\n            'cold wallet': 'offline storage hardware wallet',\n            'hot wallet': 'online wallet software wallet',\n            'blockchain': 'distributed ledger',\n            'smart contract': 'dapp decentralized application'\n        }\n        \n        self.db_path = db_path\n        self.conn = None\n        self.embedding_model = None\n        self._initialize_db()\n        \n    def _initialize_db(self):\n        \"\"\"Create database with optimized schema\"\"\"\n        self.conn = sqlite3.connect(self.db_path, timeout=30)\n        self.conn.execute(\"PRAGMA journal_mode = WAL\")\n        self.conn.execute(\"PRAGMA synchronous = NORMAL\")\n        self.conn.execute(\"PRAGMA cache_size = -100000\")  # 100MB cache\n        \n        # Main Q&A table with additional metadata fields\n        self.conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS qna_pairs (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            question TEXT NOT NULL,\n            answer TEXT NOT NULL,\n            category TEXT,\n            word_count INTEGER,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            last_accessed TIMESTAMP,\n            usage_count INTEGER DEFAULT 0,\n            keywords TEXT,\n            normalized_question TEXT\n        )\"\"\")\n        \n        # Vector embeddings table\n        self.conn.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS qna_embeddings (\n            qna_id INTEGER PRIMARY KEY,\n            question_vector BLOB,\n            answer_vector BLOB,\n            FOREIGN KEY (qna_id) REFERENCES qna_pairs(id)\n        )\"\"\")\n        \n        # Create indexes\n        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_category ON qna_pairs(category)\")\n        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_word_count ON qna_pairs(word_count)\")\n        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_normalized_question ON qna_pairs(normalized_question)\")\n        \n        # Full-text search with additional configuration\n        self.conn.execute(\"\"\"\n        CREATE VIRTUAL TABLE IF NOT EXISTS qna_search \n        USING fts5(question, answer, keywords, tokenize='porter unicode61')\n        \"\"\")\n\n    def _get_embedding_model(self):\n        \"\"\"Get better embedding model\"\"\"\n        if self.embedding_model is None:\n            # Consider using a larger model for better accuracy\n            self.embedding_model = SentenceTransformer('all-mpnet-base-v2')  # Better than default\n        return self.embedding_model\n\n    def _text_to_vector(self, text: str) -> bytes:\n        \"\"\"Convert text to compressed vector\"\"\"\n        model = self._get_embedding_model()\n        vector = model.encode(text)\n        return vector.tobytes()\n\n    def _vector_to_array(self, blob: bytes) -> np.ndarray:\n        \"\"\"Convert blob back to numpy array\"\"\"\n        return np.frombuffer(blob, dtype=np.float32)\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normalize text for exact matching\"\"\"\n        text = text.lower().strip()\n        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n        return text\n    \n    def _extract_keywords(self, text: str) -> str:\n        \"\"\"Enhanced keyword extraction with technical term handling\"\"\"\n        # Preserve important punctuation and technical terms\n        text = re.sub(r\"[^\\w\\s\\-./]\", '', text.lower())\n        \n        # Extract special crypto terms (like P2P, PoW, etc.)\n        special_terms = re.findall(r'\\b([A-Za-z0-9]{2,}\\-[A-Za-z0-9]{2,}|[A-Z]{2,})\\b', text)\n        \n        # Tokenize and filter\n        words = []\n        for word in text.split():\n            # Split hyphenated terms if they're not special terms\n            if '-' in word and word not in special_terms:\n                words.extend(word.split('-'))\n            else:\n                words.append(word)\n        \n        # Filter and count\n        words = [word for word in words \n                 if len(word) >= Config.MIN_WORD_LENGTH \n                 and word not in Config.STOPWORDS]\n        \n        # Add special terms back\n        words.extend(special_terms)\n        \n        # Count and get most important\n        word_counts = Counter(words)\n        return ' '.join([word for word, count in word_counts.most_common(15)])  # Increased to 15 keywords\n    \n    def batch_insert(self, qna_list: List[Dict]):\n        \"\"\"Optimized bulk insert with embeddings and keyword processing\"\"\"\n        if not qna_list:\n            return\n            \n        with self.conn:\n            cursor = self.conn.cursor()\n            \n            # Check for existing questions to prevent duplicates using normalized version\n            existing_questions = set()\n            cursor.execute(\"SELECT normalized_question FROM qna_pairs\")\n            for row in cursor.fetchall():\n                existing_questions.add(row[0])\n            \n            # Process and filter out duplicates\n            processed_qna = []\n            for qna in qna_list:\n                norm_question = self._normalize_text(qna['question'])\n                if norm_question not in existing_questions:\n                    # Extract keywords from both question and answer\n                    question_keywords = self._extract_keywords(qna['question'])\n                    answer_keywords = self._extract_keywords(qna['answer'])\n                    combined_keywords = f\"{question_keywords} {answer_keywords}\"\n                    \n                    processed_qna.append({\n                        'question': qna['question'],\n                        'answer': qna['answer'],\n                        'category': qna.get('category'),\n                        'word_count': len(qna['answer'].split()),\n                        'keywords': combined_keywords,\n                        'normalized_question': norm_question\n                    })\n                    existing_questions.add(norm_question)\n            \n            if not processed_qna:\n                print(\"No new Q&A pairs to insert\")\n                return\n            \n            # Insert only unique Q&A pairs with additional metadata\n            cursor.executemany(\"\"\"\n            INSERT INTO qna_pairs (question, answer, category, word_count, keywords, normalized_question)\n            VALUES (?, ?, ?, ?, ?, ?)\n            \"\"\", [(q['question'], q['answer'], q['category'], q['word_count'], \n                  q['keywords'], q['normalized_question']) for q in processed_qna])\n            \n            # Get inserted IDs\n            cursor.execute(\"SELECT last_insert_rowid() - ? + 1, last_insert_rowid()\", (len(processed_qna),))\n            first_id, last_id = cursor.fetchone()\n            \n            # Generate and store embeddings\n            for i in tqdm(range(len(processed_qna)), desc=\"Generating embeddings\"):\n                qna = processed_qna[i]\n                q_vector = self._text_to_vector(qna['question'])\n                a_vector = self._text_to_vector(qna['answer'])\n                cursor.execute(\"\"\"\n                INSERT INTO qna_embeddings (qna_id, question_vector, answer_vector)\n                VALUES (?, ?, ?)\n                \"\"\", (first_id + i, q_vector, a_vector))\n            \n            # Update full-text search index with keywords\n            cursor.executemany(\"\"\"\n            INSERT INTO qna_search (question, answer, keywords)\n            VALUES (?, ?, ?)\n            \"\"\", [(q['question'], q['answer'], q['keywords']) for q in processed_qna])\n    \n    def semantic_search(self, query: str, top_k: int = 5, threshold: float = 0.15) -> List[Tuple[str, str]]:\n        \"\"\"Original version that just returns questions and answers\"\"\"\n        results = self._semantic_search_with_scores(query, top_k, threshold)\n        return [(q, a) for q, a, _, _ in results]  # Strip scores and IDs\n    \n    def _semantic_search_with_scores(self, query: str, top_k: int, threshold: float):\n        \"\"\"Internal method that returns full results\"\"\"\n        # Expand query with synonyms\n        expanded_query = self._expand_query(query)\n        query_vec = self._text_to_vector(expanded_query)\n        query_arr = self._vector_to_array(query_vec)\n        \n        # Get all stored embeddings with their full context\n        cursor = self.conn.execute(\"\"\"\n        SELECT qna_pairs.id, qna_pairs.question, qna_pairs.answer, \n               qna_embeddings.question_vector, qna_embeddings.answer_vector,\n               qna_pairs.keywords, qna_pairs.category\n        FROM qna_pairs\n        JOIN qna_embeddings ON qna_pairs.id = qna_embeddings.qna_id\n        \"\"\")\n        \n        results = []\n        for qid, question, answer, q_vec_blob, a_vec_blob, keywords, category in cursor.fetchall():\n            q_vec = self._vector_to_array(q_vec_blob)\n            a_vec = self._vector_to_array(a_vec_blob)\n            \n            # Calculate similarities with expanded terms\n            q_sim = cosine_similarity([query_arr], [q_vec])[0][0]\n            a_sim = cosine_similarity([query_arr], [a_vec])[0][0]\n            \n            # Contextual scoring\n            context_score = 0\n            if keywords:\n                kw_vec = self._text_to_vector(keywords)\n                kw_sim = cosine_similarity([query_arr], [self._vector_to_array(kw_vec)])[0][0]\n                context_score += 0.2 * kw_sim\n            \n            # Category boosting if available\n            category_boost = 0\n            if category and any(cat_term in query.lower() for cat_term in category.lower().split()):\n                category_boost = 0.1\n            \n            # Combined score\n            similarity = (\n                0.4 * q_sim + \n                0.3 * a_sim + \n                0.2 * context_score + \n                0.1 * category_boost\n            )\n            \n            if similarity >= threshold:\n                results.append((question, answer, similarity, qid))\n        \n        # Sort and return with scores\n        return sorted(results, key=lambda x: x[2], reverse=True)[:top_k]\n\n    def retrieve_contexts(self, query: str, top_k: int = 3) -> List[Dict]:\n        \"\"\"RAG-optimized context retrieval\"\"\"\n        results = self.semantic_search(query, top_k=top_k*2)  # Get extra for filtering\n        \n        # Process results for RAG\n        contexts = []\n        seen_ids = set()\n        \n        for question, answer, score, qid in results:\n            if qid not in seen_ids:\n                contexts.append({\n                    'question': question,\n                    'answer': answer,\n                    'score': score,\n                    'keywords': self._extract_keywords(f\"{question} {answer}\"),\n                    'combined_text': f\"Question: {question}\\nAnswer: {answer}\"\n                })\n                seen_ids.add(qid)\n                if len(contexts) >= top_k:\n                    break\n        \n        return contexts\n    \n    def keyword_search(self, query: str, limit: int = 5) -> List[Tuple[str, str]]:\n        \"\"\"Precision keyword search that matches terms and evaluates context\"\"\"\n        cursor = self.conn.cursor()\n        \n        # Normalize and extract meaningful terms\n        query = self._normalize_text(query)\n        query_terms = [term for term in query.split() \n                      if len(term) >= Config.MIN_WORD_LENGTH \n                      and term not in Config.STOPWORDS]\n        \n        if not query_terms:\n            return []\n        \n        # Get all potential matches from the database\n        all_qna = []\n        cursor.execute(\"\"\"\n        SELECT question, answer, keywords \n        FROM qna_pairs\n        \"\"\")\n        all_qna = cursor.fetchall()\n        \n        # Score each potential match\n        scored_results = []\n        for question, answer, keywords in all_qna:\n            # Combine question, answer and keywords for matching\n            content = f\"{question} {answer} {keywords}\".lower()\n            norm_question = self._normalize_text(question)\n            \n            # Exact match scoring\n            exact_matches = sum(\n                1 for term in query_terms \n                if f\" {term} \" in f\" {content} \"\n            )\n            \n            # Partial match scoring (substring matches)\n            partial_matches = sum(\n                1 for term in query_terms \n                if term in content\n            )\n            \n            # Positional scoring - terms in question get higher weight\n            question_terms = sum(\n                2 for term in query_terms  # Higher weight for question matches\n                if term in norm_question\n            )\n            \n            # Combine scores with weighting\n            score = (\n                (exact_matches * 3) +  # Strong weight for exact matches\n                (partial_matches * 1) +  # Lower weight for partial matches\n                question_terms  # Additional weight for question matches\n            )\n            \n            if score > 0:\n                scored_results.append((question, answer, score))\n        \n        # Sort by score and remove duplicates\n        unique_results = {}\n        for question, answer, score in sorted(scored_results, key=lambda x: x[2], reverse=True):\n            norm_q = self._normalize_text(question)\n            if norm_q not in unique_results or score > unique_results[norm_q][2]:\n                unique_results[norm_q] = (question, answer, score)\n        \n        # Return top results\n        sorted_results = sorted(unique_results.values(), key=lambda x: x[2], reverse=True)\n        return [(q, a) for q, a, _ in sorted_results[:limit]]\n\n    def hybrid_search(self, query: str, top_k: int = 5) -> List[Tuple[str, str]]:\n        # First try exact matches\n        norm_query = self._normalize_text(query)\n        cursor = self.conn.cursor()\n        cursor.execute(\"\"\"\n            SELECT question, answer FROM qna_pairs \n            WHERE normalized_question LIKE ? \n            LIMIT ?\n        \"\"\", (f\"%{norm_query}%\", top_k))\n        exact_matches = cursor.fetchall()\n        \n        if exact_matches:\n            return exact_matches[:top_k]\n        \n        # Fall back to combined approach\n        return super().hybrid_search(query, top_k)\n    \n    def _expand_query(self, query: str) -> str:\n        \"\"\"More sophisticated query expansion with synonyms\"\"\"\n        expanded_terms = []\n        for term in query.lower().split():\n            if term in self.synonym_map:\n                expanded_terms.append(self.synonym_map[term])\n            else:\n                expanded_terms.append(term)\n        \n        # Add reverse synonyms (if value contains term, add key)\n        for key, values in self.synonym_map.items():\n            if any(term in values for term in query.lower().split()):\n                expanded_terms.append(key)\n        \n        return ' '.join(expanded_terms)\n\n    def get_all_data(self, limit: Optional[int] = None) -> pd.DataFrame:\n        \"\"\"Export all data with optional limit\"\"\"\n        query = \"SELECT * FROM qna_pairs\"\n        if limit:\n            query += f\" LIMIT {limit}\"\n        return pd.read_sql(query, self.conn)\n\n    def optimize(self):\n        \"\"\"Database maintenance\"\"\"\n        print(\"Optimizing database...\")\n        self.conn.execute(\"VACUUM\")\n        self.conn.execute(\"ANALYZE\")\n        self.conn.execute(\"PRAGMA optimize\")\n\n    def close(self):\n        \"\"\"Clean up resources\"\"\"\n        if self.conn:\n            self.conn.close()\n        if self.embedding_model:\n            del self.embedding_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:55:38.265385Z","iopub.execute_input":"2025-05-20T16:55:38.265725Z","iopub.status.idle":"2025-05-20T16:55:38.305049Z","shell.execute_reply.started":"2025-05-20T16:55:38.265703Z","shell.execute_reply":"2025-05-20T16:55:38.304117Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Data Processing Utilities\nclass DataProcessor:\n    @staticmethod\n    def parse_text_file(file_path: str, question_prefix: str = \"Q:\", answer_prefix: str = \"A:\") -> List[Dict]:\n        \"\"\"Improved text file parser with better error handling\"\"\"\n        qna_pairs = []\n        current_q = None\n        current_a = []\n        \n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:  # Skip empty lines\n                        continue\n                        \n                    if line.startswith(question_prefix):\n                        if current_q is not None:\n                            qna_pairs.append({\n                                \"question\": current_q,\n                                \"answer\": \"\\n\".join(current_a).strip()\n                            })\n                        current_q = line[len(question_prefix):].strip()\n                        current_a = []\n                    elif line.startswith(answer_prefix):\n                        current_a.append(line[len(answer_prefix):].strip())\n                    elif current_a:  # Only add to answer if we're in an answer block\n                        current_a.append(line)\n                \n                # Add the last pair if it exists\n                if current_q is not None:\n                    qna_pairs.append({\n                        \"question\": current_q,\n                        \"answer\": \"\\n\".join(current_a).strip()\n                    })\n        except Exception as e:\n            print(f\"Error processing file {file_path}: {str(e)}\")\n            return []\n            \n        return qna_pairs\n\n    @staticmethod\n    def chunk_list(lst: List, chunk_size: int):\n        \"\"\"Yield successive chunk_size chunks from lst\"\"\"\n        for i in range(0, len(lst), chunk_size):\n            yield lst[i:i + chunk_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:55:38.306480Z","iopub.execute_input":"2025-05-20T16:55:38.306811Z","iopub.status.idle":"2025-05-20T16:55:38.329228Z","shell.execute_reply.started":"2025-05-20T16:55:38.306781Z","shell.execute_reply":"2025-05-20T16:55:38.328068Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def main():\n    # Initialize database with cleanup\n    if os.path.exists(Config.DB_PATH):\n        os.remove(Config.DB_PATH)\n    db = QnADatabase()\n    \n    # Sample data for demonstration\n    sample_data = [\n        {\n            \"question\": \"What is P2P? \",\n            \"answer\": \"A Peer-to-Peer (P2P) payment system, seamlessly integrated with blockchain technology, a decentralized application (DApp), and MetaMask wallet, orchestrates a streamlined and secure process for transparent transactions among users.\"\n        },\n        {\n            \"question\": \"Compare and Contrast Private and Public Key: \",\n            \"answer\": \"The private key allows you to have access to your funds through the crypto wallet. it is used to send Bitcoin and must be protected and secured. As for the public key, it is used to receive Bitcoin and can be published anywhere safely.\"\n        }\n    ]\n    \n    # Auto-categorize questions\n    def detect_category(question: str) -> str:\n        question_lower = question.lower()\n        if 'p2p' in question_lower or 'peer-to-peer' in question_lower:\n            return \"networking\"\n        elif 'private key' in question_lower or 'public key' in question_lower:\n            return \"security\"\n        elif 'blockchain' in question_lower:\n            return \"fundamentals\"\n        elif 'proof of work' in question_lower or 'pow' in question_lower:\n            return \"consensus\"\n        elif 'wallet' in question_lower:\n            return \"wallets\"\n        elif 'smart contract' in question_lower:\n            return \"development\"\n        else:\n            return \"general\"\n    \n    # Add categories to sample data\n    for item in sample_data:\n        item[\"category\"] = detect_category(item[\"question\"])\n    \n    # Process and insert data\n    print(\"Inserting sample data...\")\n    db.batch_insert(sample_data)\n    \n    # For large files\n    try:\n        file_path = \"/kaggle/input/db-19-txt\"\n        if os.path.exists(file_path):\n            print(\"Processing large file...\")\n            qna_pairs = DataProcessor.parse_text_file(file_path)\n            \n            # Auto-categorize parsed questions\n            for item in qna_pairs:\n                item[\"category\"] = detect_category(item[\"question\"])\n            \n            print(f\"Processing {len(qna_pairs)} Q&A pairs...\")\n            for chunk in DataProcessor.chunk_list(qna_pairs, Config.CHUNK_SIZE):\n                db.batch_insert(chunk)\n    except Exception as e:\n        print(f\"Error processing large file: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n    \n    # Test with new questions not in the sample data\n    test_questions = [\n        \"What are the advantages of P2P networks?\",\n        \"Explain the difference between hot and cold wallets\",\n        \"What is the role of miners in blockchain?\"\n    ]\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Testing with new questions not in sample data\")\n    print(\"=\"*50)\n    \n    for question in test_questions:\n        print(f\"\\nQuestion: '{question}'\")\n        \n        # Semantic search\n        print(\"\\nSemantic search results:\")\n        semantic_results = db._semantic_search_with_scores(question, top_k=5, threshold=0.15)  # Now using the method that returns scores\n        if semantic_results:\n            for i, (q, a, score, _) in enumerate(semantic_results, 1):\n                print(f\"{i}. Question: {q}\")\n                print(f\"   Answer: {a}\")\n                print(f\"   Score: {score:.3f}\")  # Optional: show the similarity score\n                print(f\"   {'-'*50}\")\n        else:\n            print(\"No semantic matches found\")\n        \n        # Keyword search\n        print(\"\\nKeyword search results:\")\n        keyword_results = db.keyword_search(question)\n        if keyword_results:\n            for i, (q, a) in enumerate(keyword_results, 1):\n                print(f\"{i}. Question: {q}\")\n                print(f\"   Answer: {a}\")\n                print(f\"   {'-'*50}\")\n        else:\n            print(\"No keyword matches found\")\n    \n    # Export data with categories\n    df = db.get_all_data(limit=10)\n    print(\"\\nSample data from database:\")\n    print(df[['question', 'category']].head())\n    \n    # Show category distribution\n    print(\"\\nCategory distribution:\")\n    print(df['category'].value_counts())\n    \n    # Maintenance\n    db.optimize()\n    db.close()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:10:32.700537Z","iopub.execute_input":"2025-05-20T17:10:32.700934Z","iopub.status.idle":"2025-05-20T17:10:36.493369Z","shell.execute_reply.started":"2025-05-20T17:10:32.700908Z","shell.execute_reply":"2025-05-20T17:10:36.492469Z"}},"outputs":[{"name":"stdout","text":"Inserting sample data...\n","output_type":"stream"},{"name":"stderr","text":"Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70bdde0a18884565a57479a2df5beeb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eafd978e471446bc859f1bd54183f619"}},"metadata":{}},{"name":"stderr","text":"Generating embeddings:  50%|█████     | 1/2 [00:02<00:02,  2.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a265312439b4762b8d9d55c5cc73046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"183789d054dd4aebbc837b05435c25fd"}},"metadata":{}},{"name":"stderr","text":"Generating embeddings: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTesting with new questions not in sample data\n==================================================\n\nQuestion: 'What are the advantages of P2P networks?'\n\nSemantic search results:\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7d84637d8724cf4bee8febe07c4c4d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9ac125c3740468682f8db5584f5807f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dee1f2ae84a4eb39ca5264cbd13bc7e"}},"metadata":{}},{"name":"stdout","text":"1. Question: What is P2P? \n   Answer: A Peer-to-Peer (P2P) payment system, seamlessly integrated with blockchain technology, a decentralized application (DApp), and MetaMask wallet, orchestrates a streamlined and secure process for transparent transactions among users.\n   Score: 0.366\n   --------------------------------------------------\n2. Question: Compare and Contrast Private and Public Key: \n   Answer: The private key allows you to have access to your funds through the crypto wallet. it is used to send Bitcoin and must be protected and secured. As for the public key, it is used to receive Bitcoin and can be published anywhere safely.\n   Score: 0.176\n   --------------------------------------------------\n\nKeyword search results:\n1. Question: What is P2P? \n   Answer: A Peer-to-Peer (P2P) payment system, seamlessly integrated with blockchain technology, a decentralized application (DApp), and MetaMask wallet, orchestrates a streamlined and secure process for transparent transactions among users.\n   --------------------------------------------------\n\nQuestion: 'Explain the difference between hot and cold wallets'\n\nSemantic search results:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee222b9b55648cba5c03db2c9304aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9ef9233b1349c1902e885d593e3b79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c117c16abef497bb2e862bf56fcd294"}},"metadata":{}},{"name":"stdout","text":"1. Question: Compare and Contrast Private and Public Key: \n   Answer: The private key allows you to have access to your funds through the crypto wallet. it is used to send Bitcoin and must be protected and secured. As for the public key, it is used to receive Bitcoin and can be published anywhere safely.\n   Score: 0.222\n   --------------------------------------------------\n2. Question: What is P2P? \n   Answer: A Peer-to-Peer (P2P) payment system, seamlessly integrated with blockchain technology, a decentralized application (DApp), and MetaMask wallet, orchestrates a streamlined and secure process for transparent transactions among users.\n   Score: 0.153\n   --------------------------------------------------\n\nKeyword search results:\nNo keyword matches found\n\nQuestion: 'What is the role of miners in blockchain?'\n\nSemantic search results:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac523f021be4e28be117a589c1e9971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db0453640d9c4ada887fa7499cf81f64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67dcd0dc4ad2470e8fd71bb2a27606ac"}},"metadata":{}},{"name":"stdout","text":"1. Question: What is P2P? \n   Answer: A Peer-to-Peer (P2P) payment system, seamlessly integrated with blockchain technology, a decentralized application (DApp), and MetaMask wallet, orchestrates a streamlined and secure process for transparent transactions among users.\n   Score: 0.309\n   --------------------------------------------------\n\nKeyword search results:\n1. Question: What is P2P? \n   Answer: A Peer-to-Peer (P2P) payment system, seamlessly integrated with blockchain technology, a decentralized application (DApp), and MetaMask wallet, orchestrates a streamlined and secure process for transparent transactions among users.\n   --------------------------------------------------\n\nSample data from database:\n                                        question    category\n0                                  What is P2P?   networking\n1  Compare and Contrast Private and Public Key:     security\n\nCategory distribution:\ncategory\nnetworking    1\nsecurity      1\nName: count, dtype: int64\nOptimizing database...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"db = QnADatabase()\ncontexts = db.retrieve_contexts(\"What's the difference between hot and cold storage?\")\nfor ctx in contexts:\n    print(f\"Score: {ctx['score']:.3f}\")\n    print(f\"Keywords: {ctx['keywords']}\")\n    print(ctx['combined_text'])\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:10:36.494812Z","iopub.execute_input":"2025-05-20T17:10:36.495161Z","iopub.status.idle":"2025-05-20T17:10:40.591493Z","shell.execute_reply.started":"2025-05-20T17:10:36.495132Z","shell.execute_reply":"2025-05-20T17:10:40.590198Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2998f78843274b608f0325bc94483c8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c999d198e189443bb685a5e3652aff36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c288d42aeaa04053954a7f110046d72d"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3572046571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQnADatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What's the difference between hot and cold storage?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Score: {ctx['score']:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Keywords: {ctx['keywords']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1299081517.py\u001b[0m in \u001b[0;36mretrieve_contexts\u001b[0;34m(self, query, top_k)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mseen_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mqid\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 contexts.append({\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"],"ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 2)","output_type":"error"}],"execution_count":14}]}