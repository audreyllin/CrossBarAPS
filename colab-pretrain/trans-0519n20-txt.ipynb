{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Complete Environment Setup for Kaggle - FIXED VERSION\n# ========================================================\n\n# 1. First, clean up everything - more thorough cleanup\nimport sys\nimport shutil\nimport os  # Moved this import to the top\n\n# Clean up problematic installations\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Remove problematic directories manually\nproblematic_path = \"/usr/local/lib/python3.11/dist-packages/~vidia-cudnn-cu12\"\nif os.path.exists(problematic_path):\n    shutil.rmtree(problematic_path)\n\n# Clear pip cache\n!pip cache purge\n\n# 2. Install NumPy FIRST with clean environment\n!pip install -q --ignore-installed numpy==1.26.4\n\n# Force reload numpy if it was previously imported\nif 'numpy' in sys.modules:\n    del sys.modules['numpy']\n\n# 3. Now import numpy FIRST before anything else\nimport numpy as np\nprint(f\"NumPy version after clean install: {np.__version__}\")\n\n# 4. Install PyTorch with CUDA 12.1 (Kaggle's version)\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# 5. Install transformer-related packages with compatible versions\n!pip install -q transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install -q bitsandbytes==0.43.0 einops==0.7.0\n\n# 6. Handle gymnasium separately to avoid conflicts\n!pip install -q gymnasium==0.29.0 --no-deps\n\n# 7. Verify installations\nimport subprocess\nimport psutil\nimport torch\nimport torchvision\n\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")  # Should show 1.26.4\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Torchvision: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"\\n=== CUDA Information ===\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n\n# 8. Now import transformer-related packages\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig\n)\n\nprint(\"\\n=== Transformer Packages Loaded Successfully ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:31:52.018520Z","iopub.execute_input":"2025-05-20T16:31:52.018850Z","iopub.status.idle":"2025-05-20T16:36:09.728098Z","shell.execute_reply.started":"2025-05-20T16:31:52.018826Z","shell.execute_reply":"2025-05-20T16:36:09.727102Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nFound existing installation: transformers 4.51.3\nUninstalling transformers-4.51.3:\n  Successfully uninstalled transformers-4.51.3\nFound existing installation: peft 0.14.0\nUninstalling peft-0.14.0:\n  Successfully uninstalled peft-0.14.0\n\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n\u001b[0mFiles removed: 0\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\naccelerate 1.5.2 requires torch>=2.0.0, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNumPy version after clean install: 1.26.4\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/3786863762.py:28: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n  import numpy as np\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.2.1+cu121\nTorchvision: 0.17.1+cu121\nCUDA available: False\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"},{"name":"stderr","text":"2025-05-20 16:35:55.032212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747758955.285867      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747758955.356710      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n=== Transformer Packages Loaded Successfully ===\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Model Loading\n# =====================\n\n# Define MODEL_NAME at the top of the cell (should match what you used in Cell 1)\nMODEL_NAME = \"microsoft/phi-1.5\"  # Add this line\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics for the environment\"\"\"\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\n# Define MODEL_NAME at the top of the cell (should match what you used in Cell 1)\nMODEL_NAME = \"microsoft/phi-1.5\"  # Add this line\n\ndef load_model(model_name):\n    \"\"\"Load model with improved error handling and phi-1.5 specific settings\"\"\"\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    # Phi-1.5 specific configuration\n    trust_remote_code = True  # Required for phi-1.5\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    # Quantization config for memory efficiency\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch_dtype\n    )\n    \n    try:\n        print(\"Attempting quantized load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=trust_remote_code,\n            device_map=\"auto\",\n            torch_dtype=torch_dtype\n        )\n        \n        print(\"\\n✅ Model loaded successfully!\")\n        print_memory()\n        return model\n        \n    except Exception as e:\n        print(f\"\\n❌ Model loading failed: {str(e)}\")\n        print(\"Attempting standard load without quantization...\")\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                torch_dtype=torch_dtype\n            )\n            print(\"\\n✅ Model loaded successfully without quantization!\")\n            print_memory()\n            return model\n        except Exception as e:\n            print(f\"\\n❌ Standard load failed: {str(e)}\")\n            print(\"Attempting CPU-only fallback...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"cpu\",\n                torch_dtype=torch.float32\n            )\n            print(\"\\n✅ Model loaded on CPU\")\n            print_memory()\n            return model\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:36:09.729757Z","iopub.execute_input":"2025-05-20T16:36:09.730413Z","iopub.status.idle":"2025-05-20T16:36:28.505662Z","shell.execute_reply.started":"2025-05-20T16:36:09.730383Z","shell.execute_reply":"2025-05-20T16:36:28.504769Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: microsoft/phi-1.5 ===\nRAM: 6.0% (1.4/31.4GB)\nAttempting quantized load...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba6009aefd724f0d8ea0ff63adedabbb"}},"metadata":{}},{"name":"stdout","text":"\n❌ Model loading failed: No GPU found. A GPU is needed for quantization.\nAttempting standard load without quantization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"204b7836400e4f0683b8cc2254864f43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db31cb16cb134233a0293be57f0fda17"}},"metadata":{}},{"name":"stdout","text":"\n✅ Model loaded successfully without quantization!\nRAM: 22.8% (6.7/31.4GB)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup\n# =======================\n\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:36:28.506854Z","iopub.execute_input":"2025-05-20T16:36:28.507193Z","iopub.status.idle":"2025-05-20T16:36:29.860777Z","shell.execute_reply.started":"2025-05-20T16:36:28.507164Z","shell.execute_reply":"2025-05-20T16:36:29.859930Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"119f9ee03e20471b8f1234f94fd31c44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b56e3d73e1e4a23852cbbac133ae168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3e1be77f2a74057848cdc8b3cd0208b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a33218fbdfb4eea8b8f3118b51daf59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e95c29d87b4f998f7bc83962d1b5bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7561a175be584e5584795ddb53f34dc1"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Robust Data Preparation - Fixed Version\n# =============================================\n\n# 0. Set critical environment variables first\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:36:29.863020Z","iopub.execute_input":"2025-05-20T16:36:29.863288Z","iopub.status.idle":"2025-05-20T16:36:34.563801Z","shell.execute_reply.started":"2025-05-20T16:36:29.863265Z","shell.execute_reply":"2025-05-20T16:36:34.562939Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 1. FIRST CELL - Force clean environment setup\n# Cleaner installation approach\n!pip uninstall -y numpy torch -qqq\n!pip install --no-cache-dir --upgrade --force-reinstall numpy==1.26.4 torch==2.2.1\n\n# Force reload numpy from the installed location\nimport sys\nimport site\nfrom importlib import reload\nfor module in list(sys.modules):\n    if 'numpy' in module:\n        del sys.modules[module]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:36:34.564806Z","iopub.execute_input":"2025-05-20T16:36:34.565378Z","iopub.status.idle":"2025-05-20T16:38:43.800837Z","shell.execute_reply.started":"2025-05-20T16:36:34.565349Z","shell.execute_reply":"2025-05-20T16:38:43.799441Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch==2.2.1\n  Downloading torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting filelock (from torch==2.2.1)\n  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting typing-extensions>=4.8.0 (from torch==2.2.1)\n  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting sympy (from torch==2.2.1)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting networkx (from torch==2.2.1)\n  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\nCollecting jinja2 (from torch==2.2.1)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting fsspec (from torch==2.2.1)\n  Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.1)\n  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1)\n  Downloading nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting MarkupSafe>=2.0 (from jinja2->torch==2.2.1)\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.1)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m218.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m248.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m244.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m186.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m199.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m301.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m255.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m282.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m256.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m294.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m201.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m191.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m238.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m180.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\nDownloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m294.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m223.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m300.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m249.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m303.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m238.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n  Attempting uninstall: mpmath\n    Found existing installation: mpmath 1.3.0\n    Uninstalling mpmath-1.3.0:\n      Successfully uninstalled mpmath-1.3.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.13.2\n    Uninstalling typing_extensions-4.13.2:\n      Successfully uninstalled typing_extensions-4.13.2\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.1.105\n    Uninstalling nvidia-nvtx-cu12-12.1.105:\n      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.19.3\n    Uninstalling nvidia-nccl-cu12-2.19.3:\n      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.2.106\n    Uninstalling nvidia-curand-cu12-10.3.2.106:\n      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.4.2\n    Uninstalling networkx-3.4.2:\n      Successfully uninstalled networkx-3.4.2\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.2.0\n    Uninstalling fsspec-2024.2.0:\n      Successfully uninstalled fsspec-2024.2.0\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.18.0\n    Uninstalling filelock-3.18.0:\n      Successfully uninstalled filelock-3.18.0\n  Attempting uninstall: triton\n    Found existing installation: triton 2.2.0\n    Uninstalling triton-2.2.0:\n      Successfully uninstalled triton-2.2.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n  Attempting uninstall: jinja2\n    Found existing installation: Jinja2 3.1.6\n    Uninstalling Jinja2-3.1.6:\n      Successfully uninstalled Jinja2-3.1.6\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.18.0 requires fsspec[http]<=2024.2.0,>=2023.1.0, but you have fsspec 2025.5.0 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nlangchain-core 0.3.50 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.9.41 nvidia-nvtx-cu12-12.1.105 sympy-1.14.0 torch-2.2.1 triton-2.2.0 typing-extensions-4.13.2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 2. SECOND CELL - Import with verification\n# At the VERY TOP of your notebook (first cell):\nimport numpy as np\nimport torch\nimport os\nimport re\nfrom datasets import Dataset\n\n# Then verify versions in the same cell\nprint(f\"NumPy path: {np.__file__}\")\nprint(f\"NumPy version: {np.__version__}\")  \nprint(f\"PyTorch version: {torch.__version__}\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"The NumPy module was reloaded\")\n\n# 4. Text cleaning function\ndef clean_text(text):\n    \"\"\"Enhanced text cleaning function\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r'[^\\w\\s.,;!?\\'\"-]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# 5. Dataset preparation with multiple fallbacks\ndef prepare_dataset(file_path=\"/kaggle/input/db-19-txt\", max_samples=1000):\n    \"\"\"Prepare dataset with robust error handling\"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Path not found: {file_path}\")\n            \n        # Handle directory case\n        if os.path.isdir(file_path):\n            txt_files = [f for f in os.listdir(file_path) \n                        if f.endswith('.txt') and os.path.isfile(os.path.join(file_path, f))]\n            if not txt_files:\n                raise ValueError(\"No .txt files found in directory\")\n            \n            # Read first found txt file\n            with open(os.path.join(file_path, txt_files[0]), 'r', encoding='utf-8') as f:\n                lines = [clean_text(line) for line in f if len(line.split()) > 3][:max_samples]\n        else:\n            # Handle single file case\n            with open(file_path, 'r', encoding='utf-8') as f:\n                lines = [clean_text(line) for line in f if len(line.split()) > 3][:max_samples]\n                \n        return Dataset.from_dict({\"text\": lines})\n    except Exception as e:\n        print(f\"Dataset preparation failed: {str(e)}\")\n        return Dataset.from_dict({\"text\": [\"Sample text \" + str(i) for i in range(10)]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:38:43.802796Z","iopub.execute_input":"2025-05-20T16:38:43.803220Z","iopub.status.idle":"2025-05-20T16:38:43.898258Z","shell.execute_reply.started":"2025-05-20T16:38:43.803159Z","shell.execute_reply":"2025-05-20T16:38:43.897415Z"}},"outputs":[{"name":"stdout","text":"NumPy path: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py\nNumPy version: 1.26.4\nPyTorch version: 2.2.1+cu121\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/247846229.py:3: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n  import numpy as np\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 3. THIRD CELL - Dataset processing with workarounds\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with explicit numpy workarounds\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=512,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        # Convert to lists explicitly\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except RuntimeError as e:\n        if \"Numpy is not available\" in str(e):\n            # Fallback using pure Python\n            return {\n                \"input_ids\": [[0]*512],\n                \"attention_mask\": [[1]*512],\n                \"labels\": [[0]*512]\n            }\n        raise\n\ntry:\n    print(\"\\n=== Starting Processing ===\")\n    dataset = prepare_dataset()\n    \n    # Small batch test first\n    test_batch = dataset.select(range(2))\n    test_tokenized = test_batch.map(safe_tokenize, batched=True)\n    \n    # If test succeeds, process full dataset\n    tokenized_dataset = dataset.map(safe_tokenize, batched=True, batch_size=4)\n    tokenized_dataset.set_format(type='torch')\n    \n    print(\"✅ Processing completed successfully!\")\n    \nexcept Exception as e:\n    print(f\"\\n❌ Error: {str(e)}\")\n    print(\"Creating minimal fallback dataset...\")\n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": [[0,1,2,3]],\n        \"attention_mask\": [[1,1,1,1]],\n        \"labels\": [[0,1,2,3]]\n    })\n    tokenized_dataset.set_format(type='torch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:38:43.899333Z","iopub.execute_input":"2025-05-20T16:38:43.899830Z","iopub.status.idle":"2025-05-20T16:38:44.163853Z","shell.execute_reply.started":"2025-05-20T16:38:43.899798Z","shell.execute_reply":"2025-05-20T16:38:44.162794Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Processing ===\nDataset preparation failed: Path not found: /kaggle/input/db-19-txt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dc54a1d466e4e2891035061085b60dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77ca58f826b4bd0bdb87beaf8c2d3a6"}},"metadata":{}},{"name":"stdout","text":"✅ Processing completed successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 5: Training Configuration\n# =============================\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\n\n# LoRA configuration\npeft_config = LoraConfig(\n    r=16,  # Increased rank for better adaptation\n    lora_alpha=32,\n    target_modules=[\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\"],  # Phi-1.5 specific modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training arguments optimized for Kaggle\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/phi1.5-lora-results\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,  # Reduced for Kaggle\n    learning_rate=2e-5,\n    optim=\"adamw_torch\",\n    logging_steps=10,\n    save_steps=500,\n    fp16=torch.cuda.is_available(),\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\"\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:38:44.165146Z","iopub.execute_input":"2025-05-20T16:38:44.165507Z","iopub.status.idle":"2025-05-20T16:38:44.382770Z","shell.execute_reply.started":"2025-05-20T16:38:44.165472Z","shell.execute_reply":"2025-05-20T16:38:44.381849Z"}},"outputs":[{"name":"stdout","text":"trainable params: 7,864,320 || all params: 1,426,135,040 || trainable%: 0.5514428703750243\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 6: Training Execution\n# =========================\n\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:38:44.383806Z","iopub.execute_input":"2025-05-20T16:38:44.384158Z","iopub.status.idle":"2025-05-20T16:43:32.831206Z","shell.execute_reply.started":"2025-05-20T16:38:44.384128Z","shell.execute_reply":"2025-05-20T16:43:32.827413Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nRAM: 23.1% (6.8/31.4GB)\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 02:23, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Cell 7: Enhanced Model Saving with Shard Support\n# ===============================================\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport os\nimport json\nfrom typing import Optional\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[object] = None, \n    output_dir: str = \"/kaggle/working/phi1.5-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \n    Args:\n        model: The trained model to save\n        tokenizer: The tokenizer to save\n        training_args: TrainingArguments object (optional)\n        output_dir: Directory to save artifacts\n        \n    Returns:\n        Path to saved artifacts\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - merge adapters before saving\n    if hasattr(model, 'merge_and_unload'):\n        print(\"🔗 Merging LoRA adapters...\")\n        model = model.merge_and_unload()\n    \n    # Save model with safe serialization (produces .safetensors)\n    print(\"💽 Saving model weights...\")\n    model.save_pretrained(output_dir, safe_serialization=True)\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify critical files\n    print(\"\\n🔍 Verifying saved files:\")\n    \n    # Essential config files\n    required_configs = {\n        'config.json': 'Model configuration',\n        'tokenizer_config.json': 'Tokenizer config'\n    }\n    \n    missing_files = []\n    for file, desc in required_configs.items():\n        path = os.path.join(output_dir, file)\n        exists = os.path.exists(path)\n        status = '✅' if exists else '❌'\n        print(f\"- {status} {desc} ({file})\")\n        if not exists:\n            missing_files.append(file)\n    \n    # Check for model weights (supporting multiple formats)\n    weight_files = [\n        f for f in os.listdir(output_dir) \n        if f.startswith(('model.safetensors', 'pytorch_model.bin', 'model-'))\n        or f == 'model.safetensors.index.json'\n    ]\n    \n    if weight_files:\n        print(\"\\n⚖️ Found model weights:\")\n        for f in weight_files:\n            size = os.path.getsize(os.path.join(output_dir, f)) / (1024*1024)\n            print(f\"- {f} ({size:.2f} MB)\")\n    else:\n        print(\"\\n❌ No model weights found!\")\n    \n    # Final verification\n    if missing_files:\n        print(f\"\\n❌ Missing required files: {missing_files}\")\n    elif not weight_files:\n        print(\"\\n❌ No model weight files found!\")\n    else:\n        print(\"\\n🎉 All files saved successfully!\")\n    \n    if missing_files or not weight_files:\n        print(\"\\n📂 Full directory contents:\")\n        for f in sorted(os.listdir(output_dir)):\n            size = os.path.getsize(os.path.join(output_dir, f)) / 1024\n            print(f\"- {f} ({size:.2f} KB)\")\n        \n        raise ValueError(\"Model saving incomplete - missing essential files\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:43:32.847434Z","iopub.execute_input":"2025-05-20T16:43:32.848445Z","iopub.status.idle":"2025-05-20T16:43:32.908502Z","shell.execute_reply.started":"2025-05-20T16:43:32.848266Z","shell.execute_reply":"2025-05-20T16:43:32.904856Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 8: Robust Model Loading and Testing with PEFT support\n# ========================================================\n\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/phi1.5-lora-trained\", \n    max_length: int = 100,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True  # Set to True if using PEFT/LoRA\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \n    Args:\n        model_path: Path to saved model\n        max_length: Maximum generation length\n        test_prompts: Optional list of custom test prompts\n        is_peft_model: Whether this is a PEFT/LoRA model\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\n🔄 Loading model...\")\n        try:\n            if is_peft_model:\n                # For PEFT models, we need to load the base model first\n                from peft import PeftModel\n                \n                # First load the base model\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True,\n                    trust_remote_code=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n            else:\n                # For regular models\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True,\n                    trust_remote_code=True\n                )\n                \n            print(\"\\n🎉 Model loaded successfully!\")\n            \n            # Default test prompts if none provided\n            if test_prompts is None:\n                test_prompts = [\n                    \"What is hardware wallet?? \",\n                    \"What is Proof of Work (PoW)?? \",\n                    \"What is cryptography?? \",\n                    \"What is Peer-to-Peer (P2P)?? \",\n                    \"What is block chain?? \",\n                    \"What is private key?? \"\n                ]\n            \n            # Create pipeline\n            print(\"\\n🚀 Creating text generation pipeline...\")\n            pipe = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                device=0 if torch.cuda.is_available() else -1\n            )\n            \n            # Run tests\n            print(\"\\n🧪 Running generation tests...\")\n            for i, prompt in enumerate(test_prompts, 1):\n                print(f\"\\n🔹 Test {i}: {prompt}\")\n                output = pipe(\n                    prompt,\n                    max_length=max_length,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    num_return_sequences=1\n                )\n                print(\"💬 Response:\", output[0]['generated_text'])\n                \n            return model, tokenizer\n            \n        except Exception as e:\n            print(f\"\\n⚠️ Model loading failed: {str(e)}\")\n            print(\"🔄 Trying fallback loading method (local files only)...\")\n            \n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                local_files_only=True\n            )\n            print(\"\\n🎉 Fallback loading successful!\")\n            \n            # Continue with testing as above\n            # ... [rest of the testing code]\n            \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:43:32.913651Z","iopub.execute_input":"2025-05-20T16:43:32.914827Z","iopub.status.idle":"2025-05-20T16:43:32.970310Z","shell.execute_reply.started":"2025-05-20T16:43:32.914717Z","shell.execute_reply":"2025-05-20T16:43:32.966281Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# required_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json']\n# for f in required_files:\n#     if not os.path.exists(os.path.join(model_path, f)):\n#         print(f\"⚠️ Missing file: {f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:43:32.974402Z","iopub.execute_input":"2025-05-20T16:43:32.976367Z","iopub.status.idle":"2025-05-20T16:43:33.013430Z","shell.execute_reply.started":"2025-05-20T16:43:32.976327Z","shell.execute_reply":"2025-05-20T16:43:33.009182Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import file_utils\nfile_utils.HF_DATASETS_CACHE = \"/kaggle/working/cache\"\nfile_utils.TRANSFORMERS_CACHE = \"/kaggle/working/cache\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:43:33.018358Z","iopub.execute_input":"2025-05-20T16:43:33.020176Z","iopub.status.idle":"2025-05-20T16:43:33.044690Z","shell.execute_reply.started":"2025-05-20T16:43:33.019733Z","shell.execute_reply":"2025-05-20T16:43:33.040550Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import gc\nimport torch\n\ndef cleanup():\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:43:33.050436Z","iopub.execute_input":"2025-05-20T16:43:33.053789Z","iopub.status.idle":"2025-05-20T16:43:33.078163Z","shell.execute_reply.started":"2025-05-20T16:43:33.053753Z","shell.execute_reply":"2025-05-20T16:43:33.074930Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    model_path = \"/kaggle/working/phi1.5-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    load_and_test_model(model_path, is_peft_model=True)\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \" \"\n    ]\n    load_and_test_model(model_path, test_prompts=custom_prompts, is_peft_model=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:43:33.082321Z","iopub.execute_input":"2025-05-20T16:43:33.083413Z","iopub.status.idle":"2025-05-20T16:44:15.711763Z","shell.execute_reply.started":"2025-05-20T16:43:33.083126Z","shell.execute_reply":"2025-05-20T16:44:15.710778Z"}},"outputs":[{"name":"stdout","text":"\n💾 Saving model artifacts to: /kaggle/working/phi1.5-lora-trained\n🔗 Merging LoRA adapters...\n💽 Saving model weights...\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"🔤 Saving tokenizer...\n📝 Saving training arguments...\n\n🔍 Verifying saved files:\n- ✅ Model configuration (config.json)\n- ✅ Tokenizer config (tokenizer_config.json)\n\n⚖️ Found model weights:\n- model.safetensors.index.json (0.03 MB)\n- model-00001-of-00002.safetensors (4753.99 MB)\n- model-00002-of-00002.safetensors (656.32 MB)\n\n🎉 All files saved successfully!\n\n🔍 Preparing to load model from: /kaggle/working/phi1.5-lora-trained\n\n📂 Model directory contents:\n- added_tokens.json (1.05 KB)\n- config.json (0.72 KB)\n- generation_config.json (0.07 KB)\n- merges.txt (445.62 KB)\n- model-00001-of-00002.safetensors (4868082.18 KB)\n- model-00002-of-00002.safetensors (672074.28 KB)\n- model.safetensors.index.json (26.22 KB)\n- special_tokens_map.json (0.46 KB)\n- tokenizer.json (2065.65 KB)\n- tokenizer_config.json (7.26 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4bb618d97d24d559a3905e6cbef5055"}},"metadata":{}},{"name":"stdout","text":"\n⚠️ Model loading failed: Can't find 'adapter_config.json' at '/kaggle/working/phi1.5-lora-trained'\n🔄 Trying fallback loading method (local files only)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69cf55643193480d9a8a0ee4e6f9cc24"}},"metadata":{}},{"name":"stdout","text":"\n🎉 Fallback loading successful!\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"\n🔍 Preparing to load model from: /kaggle/working/phi1.5-lora-trained\n\n📂 Model directory contents:\n- added_tokens.json (1.05 KB)\n- config.json (0.72 KB)\n- generation_config.json (0.07 KB)\n- merges.txt (445.62 KB)\n- model-00001-of-00002.safetensors (4868082.18 KB)\n- model-00002-of-00002.safetensors (672074.28 KB)\n- model.safetensors.index.json (26.22 KB)\n- special_tokens_map.json (0.46 KB)\n- tokenizer.json (2065.65 KB)\n- tokenizer_config.json (7.26 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9495c8e9fe164a728726af21129f2268"}},"metadata":{}},{"name":"stdout","text":"\n⚠️ Model loading failed: Can't find 'adapter_config.json' at '/kaggle/working/phi1.5-lora-trained'\n🔄 Trying fallback loading method (local files only)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f04bf09fa904f98af04c8a4d01c0142"}},"metadata":{}},{"name":"stdout","text":"\n🎉 Fallback loading successful!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# After successful loading\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprompts = [\n    \" \"\n]\n\nfor prompt in prompts:\n    output = pipe(prompt, max_length=100, do_sample=True)\n    print(f\"Prompt: {prompt}\\nResponse: {output[0]['generated_text']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:44:15.713051Z","iopub.execute_input":"2025-05-20T16:44:15.713372Z","iopub.status.idle":"2025-05-20T16:47:19.008080Z","shell.execute_reply.started":"2025-05-20T16:44:15.713349Z","shell.execute_reply":"2025-05-20T16:47:19.007049Z"}},"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prompt:  \nResponse:  \nAnswer: Samantha was inspired by the colorful and intricate beadwork in the traditional African jewelry she saw at the market.\n2. Why did Samantha choose a necklace with a unique design? \nAnswer: Samantha chose a necklace with a unique design because she was drawn to the vibrant colors and intricate beadwork of the traditional African jewelry.\n3. Why did Emily choose a simple gold necklace? \nAnswer: Emily chose a simple gold necklace because she prefers a more understated and classic\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"prompts = [\n    \"What is software wallet, and what's the difference between hardware and software wallet? \",\n    \"What is PoW? \",\n    \"What is a cryptographic product? \",\n    \"What is P2P? \",\n    \"What is block chain? \",\n    \"What is public key, and what's the difference between private and public key? \"\n]\n\nfor prompt in prompts:\n    output = pipe(prompt, max_length=100, do_sample=True)\n    print(f\"Prompt: {prompt}\\nResponse: {output[0]['generated_text']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:47:19.009491Z","iopub.execute_input":"2025-05-20T16:47:19.009853Z","iopub.status.idle":"2025-05-20T17:05:29.066427Z","shell.execute_reply.started":"2025-05-20T16:47:19.009823Z","shell.execute_reply":"2025-05-20T17:05:29.065436Z"}},"outputs":[{"name":"stdout","text":"Prompt: What is software wallet, and what's the difference between hardware and software wallet? \nResponse: What is software wallet, and what's the difference between hardware and software wallet? \nS: A software wallet is a type of digital wallet that stores all the user's codes and data in a secure location. The difference between hardware and software wallet lies in their design - hardware wallets are built into devices like smartcards or phones, while software wallets are stored online. \n\n Exercise 4: Can you explain what Bitcoin does? \n A: Yes, Bitcoin is a type of \"digital currency\n\nPrompt: What is PoW? \nResponse: What is PoW? \nAnswer: PoW stands for Proof-of-Work. It's a security feature used in many cryptocurrencies to prevent fraud and ensure that only legitimate users can create new funds.\n\n4. Exercise: How does blockchain ensure security in cryptocurrencies? \nAnswer: Blockchain's decentralized ledger system makes it incredibly difficult to modify or hack the central bank's database. However, this also means that the ledger is tamper-proof and secure.\n\n5. Exercise\n\nPrompt: What is a cryptographic product? \nResponse: What is a cryptographic product? \nAnswer: A cryptographic product is designed to be as resistant as possible to attacks from the adversary, ensuring that even if the product falls victim to a breach, the attacker’s data is not affected in any way.\n\n2) Exercise: How does a product with security features impact the security of other products? \nAnswer: When a product has security features installed, it creates a protective layer around the product. This security layer can prevent and deter\n\nPrompt: What is P2P? \nResponse: What is P2P? \n\nAnswer: P2P is a term that stands for peer-to-peer. It means that people use technology and the internet to connect with each other, without relying on a traditional power structure such as a company or government.\n\n5. Name one way that an Alien could use P2P to make a new home.\n\nAnswer: An Alien could use P2P to connect with others online and find out about different places to live\n\nPrompt: What is block chain? \nResponse: What is block chain? \nAnswer: It is a distributed ledger.\n3. What is a smart chain?\nAnswer: It is a network of decentralized computers that verify transactions and add them to a blockchain. \n4. What is ethereum?\nAnswer: Ethereum is an open-source blockchain that enables smart contracts to be executed.\n5. Name a real-world example of how block chain technology is being used.\nAnswer: Block chain technology is being used in the\n\nPrompt: What is public key, and what's the difference between private and public key? \nResponse: What is public key, and what's the difference between private and public key? \n\n\nTA:  \n\nprivate key = 5 + 12\npublic key = 3 * private key\nimport math\nsqrt_public_key = math.sqrt(public_key)\ndiff = private_key - sqrt_public_key\nresult = diff\n \n\n\n\nStudent: A store has a sale where everything is 25% off. If the original price of\n\n","output_type":"stream"}],"execution_count":17}]}
