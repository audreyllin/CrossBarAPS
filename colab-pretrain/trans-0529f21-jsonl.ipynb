{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12000428,"sourceType":"datasetVersion","datasetId":7548900},{"sourceId":12000455,"sourceType":"datasetVersion","datasetId":7548921},{"sourceId":12009799,"sourceType":"datasetVersion","datasetId":7555612}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:46:02.816099Z","iopub.execute_input":"2025-05-30T17:46:02.816439Z","iopub.status.idle":"2025-05-30T17:46:02.827365Z","shell.execute_reply.started":"2025-05-30T17:46:02.816406Z","shell.execute_reply":"2025-05-30T17:46:02.826271Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Complete Environment Setup\n# ========================================================\n\n# 1. First, clean up everything - more thorough cleanup\nimport sys\nimport shutil\nimport os\nimport json\n\n# Clean up problematic installations\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Remove problematic directories manually\nproblematic_path = \"/usr/local/lib/python3.11/dist-packages/~vidia-cudnn-cu12\"\nif os.path.exists(problematic_path):\n    shutil.rmtree(problematic_path)\n\n# Clear pip cache\n!pip cache purge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:46:02.829323Z","iopub.execute_input":"2025-05-30T17:46:02.829793Z","iopub.status.idle":"2025-05-30T17:46:36.886699Z","shell.execute_reply.started":"2025-05-30T17:46:02.829751Z","shell.execute_reply":"2025-05-30T17:46:36.885220Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nFound existing installation: transformers 4.51.3\nUninstalling transformers-4.51.3:\n  Successfully uninstalled transformers-4.51.3\nFound existing installation: peft 0.14.0\nUninstalling peft-0.14.0:\n  Successfully uninstalled peft-0.14.0\n\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n\u001b[0mFiles removed: 0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 2. Install NumPy FIRST with clean environment\n!pip install -q --ignore-installed numpy==1.26.4\n\n# Force reload numpy if it was previously imported\nif 'numpy' in sys.modules:\n    del sys.modules['numpy']\n\n# 3. Now import numpy FIRST before anything else\nimport numpy as np\nprint(f\"NumPy version after clean install: {np.__version__}\")\n\n# 4. Install PyTorch with CUDA 12.1 (Kaggle's version)\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# 5. Install transformer-related packages with compatible versions\n!pip install -q transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install -q bitsandbytes==0.43.0 einops==0.7.0\n\n# 6. Handle gymnasium separately to avoid conflicts\n!pip install -q gymnasium==0.29.0 --no-deps\n\n# 7. Verify installations\nimport subprocess\nimport psutil\nimport torch\nimport torchvision\n\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")  # Should show 1.26.4\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Torchvision: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"\\n=== CUDA Information ===\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n\n# 8. Now import transformer-related packages\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    pipeline\n)\nfrom typing import Optional\nprint(\"\\n=== Transformer Packages Loaded Successfully ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:46:36.888191Z","iopub.execute_input":"2025-05-30T17:46:36.888624Z","iopub.status.idle":"2025-05-30T17:50:51.987778Z","shell.execute_reply.started":"2025-05-30T17:46:36.888580Z","shell.execute_reply":"2025-05-30T17:50:51.986602Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\naccelerate 1.5.2 requires torch>=2.0.0, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNumPy version after clean install: 1.26.4\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_64/2961444526.py:9: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n  import numpy as np\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m904.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m84.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.2.1+cu121\nTorchvision: 0.17.1+cu121\nCUDA available: False\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"},{"name":"stderr","text":"2025-05-30 17:50:35.176934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748627435.447284      64 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748627435.523450      64 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n=== Transformer Packages Loaded Successfully ===\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 2: Model Loading\n# =====================\n\n# Define MODEL_NAME at the top of the cell (should match what is used in Cell 1)\nMODEL_NAME = \"gpt2\"  # Change to \"meta-llama/Llama-2-7b-chat-hf\" for Llama\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics for the environment\"\"\"\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    \"\"\"Load model with improved error handling and phi-1.5 specific settings\"\"\"\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    # Phi-1.5 specific configuration\n    trust_remote_code = True  # Required for phi-1.5\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    # Quantization config for memory efficiency\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch_dtype\n    )\n    \n    try:\n        print(\"Attempting quantized load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=trust_remote_code,\n            device_map=\"auto\",\n            torch_dtype=torch_dtype\n        )\n        \n        print(\"\\n✅ Model loaded successfully!\")\n        print_memory()\n        return model\n        \n    except Exception as e:\n        print(f\"\\n❌ Model loading failed: {str(e)}\")\n        print(\"Attempting standard load without quantization...\")\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                torch_dtype=torch_dtype\n            )\n            print(\"\\n✅ Model loaded successfully without quantization!\")\n            print_memory()\n            return model\n        except Exception as e:\n            print(f\"\\n❌ Standard load failed: {str(e)}\")\n            print(\"Attempting CPU-only fallback...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"cpu\",\n                torch_dtype=torch.float32\n            )\n            print(\"\\n✅ Model loaded on CPU\")\n            print_memory()\n            return model\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:50:51.989354Z","iopub.execute_input":"2025-05-30T17:50:51.990358Z","iopub.status.idle":"2025-05-30T17:50:55.912734Z","shell.execute_reply.started":"2025-05-30T17:50:51.990314Z","shell.execute_reply":"2025-05-30T17:50:55.911752Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 5.8% (1.4/31.4GB)\nAttempting quantized load...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"326fe6f4ecc245aa87e078800a59a1e5"}},"metadata":{}},{"name":"stdout","text":"\n❌ Model loading failed: No GPU found. A GPU is needed for quantization.\nAttempting standard load without quantization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffcfc816d9d4bbd8baec0798007af7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d07b355f6b75479e9acafe98481f08ed"}},"metadata":{}},{"name":"stdout","text":"\n✅ Model loaded successfully without quantization!\nRAM: 7.5% (1.9/31.4GB)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup\n# =======================\n\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:50:55.915166Z","iopub.execute_input":"2025-05-30T17:50:55.915465Z","iopub.status.idle":"2025-05-30T17:50:57.767729Z","shell.execute_reply.started":"2025-05-30T17:50:55.915444Z","shell.execute_reply":"2025-05-30T17:50:57.766596Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa744b78324c4b42b8d8afec6bfb3217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd21ce767857417b937f37633a91f1a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0dc2e4de2745d1b093ec53ed079c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194e2d999fd648d5aeada9cb564db8bc"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 4: Robust Data Preparation\n# =============================================\n\n# 0. Set critical environment variables first\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n# 1. Dataset preparation with multiple fallbacks\ndef prepare_dataset(file_path=\"/kaggle/input/database-0520\", max_samples=1000):\n    \"\"\"Prepare dataset with robust error handling\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"❌ Path not found: {file_path}\")\n            \n        # Load JSONL file directly using datasets library\n        dataset = load_dataset('json', data_files=file_path, split='train[:{}]'.format(max_samples))\n        \n        # If the dataset has multiple columns, we just want the text\n        if 'text' not in dataset.features:\n            # Try to find the first text-like column\n            text_columns = [col for col in dataset.features if any(t in col.lower() for t in ['text', 'content', 'body'])]\n            if text_columns:\n                dataset = dataset.rename_column(text_columns[0], 'text')\n            else:\n                # If no text column found, combine all string columns\n                string_columns = [col for col in dataset.features if dataset.features[col].dtype == 'string']\n                if string_columns:\n                    def combine_columns(examples):\n                        return {'text': ' '.join(str(examples[col]) for col in string_columns)}\n                    dataset = dataset.map(combine_columns)\n                else:\n                    raise ValueError(\"No text columns found in dataset\")\n        \n        print(f\"✅ Loaded dataset with {len(dataset)} samples\")\n        return dataset\n        \n    except Exception as e:\n        print(f\"\\n❌ Dataset preparation failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\"Sample text \" + str(i) for i in range(10)]})\n\n# 2. Tokenization function\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with explicit numpy workarounds\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=90,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        # Convert to lists explicitly\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except RuntimeError as e:\n        if \"Numpy is not available\" in str(e):\n            # Fallback using pure Python\n            return {\n                \"input_ids\": [[0]*512],\n                \"attention_mask\": [[1]*512],\n                \"labels\": [[0]*512]\n            }\n        raise\n\ntry:\n    print(\"\\n=== Starting Processing ===\")\n    dataset = prepare_dataset()\n    \n    # Small batch test first\n    test_batch = dataset.select(range(2))\n    test_tokenized = test_batch.map(safe_tokenize, batched=True)\n    \n    # If test succeeds, process full dataset\n    tokenized_dataset = dataset.map(safe_tokenize, batched=True, batch_size=4)\n    tokenized_dataset.set_format(type='torch')\n    \n    print(\"✅ Processing completed successfully!\")\n    \nexcept Exception as e:\n    print(f\"\\n❌ Error: {str(e)}\")\n    print(\"Creating minimal fallback dataset...\")\n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": [[0,1,2,3]],\n        \"attention_mask\": [[1,1,1,1]],\n        \"labels\": [[0,1,2,3]]\n    })\n    tokenized_dataset.set_format(type='torch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:50:57.768876Z","iopub.execute_input":"2025-05-30T17:50:57.769201Z","iopub.status.idle":"2025-05-30T17:50:58.287183Z","shell.execute_reply.started":"2025-05-30T17:50:57.769179Z","shell.execute_reply":"2025-05-30T17:50:58.286084Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Processing ===\n\n❌ Dataset preparation failed: Unable to find '/kaggle/input/database-0520'\nCreating minimal fallback dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89e7c3f7da354fae90755f8f16b21fab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026d1291f75640e2a03a9a7085b2d699"}},"metadata":{}},{"name":"stdout","text":"✅ Processing completed successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 5: Training Configuration\n# =============================\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\n\n# LoRA configuration\nfrom peft import LoraConfig\n\npeft_config = LoraConfig(\n    r=16,  \n    lora_alpha=32,\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # GPT-2 compatible modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    fan_in_fan_out=True\n)\n\n# Training arguments optimized for Kaggle\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/phi1.5-lora-results\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,  # Reduced for Kaggle\n    learning_rate=2e-5,\n    optim=\"adamw_torch\",\n    logging_steps=10,\n    save_steps=500,\n    fp16=torch.cuda.is_available(),\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\"\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:50:58.288454Z","iopub.execute_input":"2025-05-30T17:50:58.289006Z","iopub.status.idle":"2025-05-30T17:50:58.413519Z","shell.execute_reply.started":"2025-05-30T17:50:58.288973Z","shell.execute_reply":"2025-05-30T17:50:58.412453Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 6: Training Execution\n# =========================\n\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:50:58.414683Z","iopub.execute_input":"2025-05-30T17:50:58.415068Z","iopub.status.idle":"2025-05-30T17:51:06.664463Z","shell.execute_reply.started":"2025-05-30T17:50:58.415036Z","shell.execute_reply":"2025-05-30T17:51:06.662904Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nRAM: 7.5% (1.9/31.4GB)\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:03, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 7: Enhanced Model Saving with Shard Support\n# ===============================================\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"💽 Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n⚠️ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:51:06.666398Z","iopub.execute_input":"2025-05-30T17:51:06.666772Z","iopub.status.idle":"2025-05-30T17:51:06.677815Z","shell.execute_reply.started":"2025-05-30T17:51:06.666749Z","shell.execute_reply":"2025-05-30T17:51:06.676806Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 8: Robust Model Loading and Testing with PEFT support\n# ========================================================\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 250,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline - REMOVED device parameter since we're using device_map=\"auto\"\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"💬 Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:51:06.678759Z","iopub.execute_input":"2025-05-30T17:51:06.679045Z","iopub.status.idle":"2025-05-30T17:51:06.701615Z","shell.execute_reply.started":"2025-05-30T17:51:06.679023Z","shell.execute_reply":"2025-05-30T17:51:06.700229Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    model_path = \"/kaggle/working/gpt2-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    load_and_test_model(model_path, is_peft_model=True)\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \"What is software wallet, and what's the difference between hardware and software wallet? \",\n        \"What is PoW? \",\n        \"Explain PoW in 1 sentence. \",\n        \"Describe the key features of PoW using 3 words. \",\n        \"What is PoM? Is it something related to cryptography? \",\n        \"What is a cryptographic product? \",\n        \"What is P2P? \",\n        \"What is block chain? \",\n        \"What is public key, and what's the difference between private and public key? \"\n    ]\n    load_and_test_model(model_path, test_prompts=custom_prompts, is_peft_model=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:51:06.702935Z","iopub.execute_input":"2025-05-30T17:51:06.703319Z","iopub.status.idle":"2025-05-30T17:53:24.636261Z","shell.execute_reply.started":"2025-05-30T17:51:06.703289Z","shell.execute_reply":"2025-05-30T17:53:24.635119Z"}},"outputs":[{"name":"stdout","text":"\n💾 Saving model artifacts to: /kaggle/working/gpt2-lora-trained\n💽 Saving model and adapter...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"🔤 Saving tokenizer...\n📝 Saving training arguments...\n\n🔍 Verifying saved files:\n- training_args.json (3.86 KB)\n- adapter_model.safetensors (9227.88 KB)\n- tokenizer_config.json (0.49 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- adapter_config.json (0.66 KB)\n- vocab.json (779.45 KB)\n- README.md (4.96 KB)\n- tokenizer.json (2058.55 KB)\n\n🔍 Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\n📂 Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.66 KB)\n- adapter_model.safetensors (9227.88 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\nFound adapter files: ['adapter_model.safetensors', 'adapter_config.json']\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\n🎉 Model loaded successfully!\n\n🚀 Creating text generation pipeline...\n\n🧪 Running generation tests...\n\n🔹 Test 1: What is hardware wallet?? \n💬 Response: What is hardware wallet??  It's a simple, fast and secure way to store your money in an easy-touse digital currency that you can use for transactions anywhere on the internet. It doesn't have any central bank controls or restrictions like Bitcoin does (like it did with Visa). The only difference between this technology and traditional paper wallets are their security features:\nThe main advantage of using cash over electronic currencies such as dollars isn´t just convenience but also because there aren` t many other ways around them - some even require having physical keys too! There will always be someone waiting at home who may need access codes from within Bitcoins which could easily change hands if needed; all these things should give users confidence when dealing directly into online banking networks without breaking banks' trust system!! This makes buying bitcoins easier than ever before!!! And here I want one more example why we would not consider doing anything against crypto... In order get started looking beyond our current \"legal\" requirements while supporting cryptocurrencies including fiat ones.. We believe blockchain has proven its value by being so much safer & simpler comparedTo existing forms OF financial intermediation. As well, once created through peer review processes / auditing systems where participants agree upon rules based on public consensus then they\n\n🔹 Test 2: What is Proof of Work (PoW)?? \n💬 Response: What is Proof of Work (PoW)??  \"Proofs are tools that can be used to verify a claim or prove something about the statement. They give you access, and some information on what proof it was made for.\" - David Hume\nIn my opinion this concept comes from an older text called \"The Law,\" which states:\nIt seems pretty clear why we cannot say whether our knowledge must have been derived in any way by means other than through its being forged; if so then there would not seem much difference between us believing such things as were given out without their evidence but merely because they could only come into existence after certain conditions had gone awry. But since I am inclined towards supposing either no one has ever done anything like 'proof' with regard thereto, let me add here another fact ; once upon a time when all proofs became known beyond doubt — even though every man knew them themselves! Nowadays almost everyone knows these matters at hand : especially those who know how many persons possess copies! This makes possible very important discoveries concerning people's understanding [i]t rather more powerful ones!\" – William Lane Craig\n\n🔹 Test 3: What is cryptography?? \n💬 Response: What is cryptography??  I don't know why it's such a big deal to have something so secret, but I think that most of the people who use Cryptography are probably unaware what they're doing. In fact: as long ago in 2011 when we were talking about \"the next generation\", everyone thought cryptographers would be able just by changing their minds and not being caught up with all this new technology (and yes there will always still been some debate on whether or NOT crypto should even exist at present). It wasn´t until 2013 where many well-known researchers became aware of how much information security had changed from traditional methods for securing keys between users/consumers; nowadays more than ever you need an outside authority like someone else using your software - especially if one knows exactly which key(s) has got access...\nAs mentioned before these guys really understand encryption better then anyone can do anything other wise because no matter any kind words used here either manor he understood clearly enough everything needed fixing etc.. He was also very good after writing his book. One thing i did notice though regarding him :\n\n🔹 Test 4: What is Peer-to-Peer (P2P)?? \n💬 Response: What is Peer-to-Peer (P2P)??  I don't know, but it's still pretty cool.\nThis one was so fun to get my hands on that I made a game for myself called 'The Drowning Pool'. You can download the demo here. The idea behind this project and how we are going about getting things up into production at these times were: 1.) We would need an internet connection; 2). A decent amount of time needed in order not be overwhelmed by what you're looking through - as longas everyone knows exactly where they go before doing anything else :) 3), All our tools should work together when all goes well enough 4,) Some new equipment needs will probably come along later! That being said, if any kind person or entity wants to help us out with their own PUBG projects please let them KNOW!! Just send me some pictures/videos via etsy...and do your best ;)\n\n🔹 Test 5: What is block chain?? \n💬 Response: What is block chain??  Why do you call it a \"block?\" Why does the name of any particular place on this planet refer to itself as such? Do we have no idea how many people are involved with what has been called Bitcoin and why not everyone just calls themselves Satoshi Nakamoto? Is there really only one person in our community who makes decisions for us, or am I missing something here that should be taken seriously by all those around me - someone like myself.\nBlockchain isn't about making money! Blockchains change everything! Blockchain technology enables users to make more than they ever could before: from savings accounts (coins) through smart contracts at central banks, directly into payments networks where anyone can transact freely without fear of being blocked out ; literally every single day since 2010. It also brings freedom over borders : if citizens choose their own private space insteadof having government control its activities; otherwise governments will find ways elsewhere via laws so simple-minded individuals might feel free to use them whereverthey want but nobody else would get shut down because others were trying too hard ;) And now... What's next??? How long until blocksize decreases again?? Who knows???? We're still building new systems & processes.... But these changes need time..\n\n🔹 Test 6: What is private key?? \n💬 Response: What is private key??  The public keys are generated and used to generate the code that you want from your website. This makes it a bit harder for third party developers who have no idea what they're doing, but will use this knowledge as an excuse or justification when creating their own web pages without knowing how much data can be stored in them (i know I'm talking about people here).\nA little background on our site: http://www2k4n5bj7v8e3f-6tXzVw1QrMgCqJmEZRxB9lW/   We created some really cool HTML files with JavaScript so we could write better javascripts using JSML. In order not only do these things work well across browsers, there's also quite literally nothing like having any kind of API built right into those very powerful tools!\n\n🔍 Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\n📂 Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.66 KB)\n- adapter_model.safetensors (9227.88 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\nFound adapter files: ['adapter_model.safetensors', 'adapter_config.json']\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\n🎉 Model loaded successfully!\n\n🚀 Creating text generation pipeline...\n\n🧪 Running generation tests...\n\n🔹 Test 1: What is software wallet, and what's the difference between hardware and software wallet? \n💬 Response: What is software wallet, and what's the difference between hardware and software wallet?  First off we have to ask if this coin has a blockchain. Bitcoin uses an unconfirmed hash of 1 BTC for transactions which are not yet confirmed by any other source but from one individual that did something different with their coins so it makes sense they would use some kind block chain instead as opposed only using two or three random blocks in order get confirmation on them all once you know how many bits there were at each point... I'm sure most people will say \"why do these things exist?\", while others just feel like why does bitcoin work when its decentralized (or even non-deregulated) system can be used solely based upon mining power rather than having every transaction being recorded directly into your computer/tablet(s)? Well my answer here isn't really good enough because trust must always make up half of everything since no matter who runs our systems then those trusting parties should never take over ownership! We don' t want money changing hands around us either - otherwise anyone could create someone else making bitcoins now regardless whether he owns anything real value such currency nor still run his own company.... As far back time maybe 16 years ago before cryptocurrency was invented perhaps 14\n\n🔹 Test 2: What is PoW? \n💬 Response: What is PoW?  PoL has been around for a long time and it's the first project I've written in which we have actually done something.   We had this idea back when that was all there were to do with writing on software, but now everything else comes from other disciplines (and sometimes even some of our own).    It started as an experiment after my daughter came into school; she thought about what would happen if people who are using Python could run their code through another language instead... But no one really understood why they didn't want anything like Go or C++ - just because you can use different languages isn´t enough! And so here goes:\nThe main problem faced by those doing development at home doesn`T mean much more than programming directly via Javascript/Python / JavaScriptScripts. The core point still stands though : If your game engine supports scripting then maybe someday someone will write nice stuff too :) In fact many developers spend most weekends trying out various kinds Of Software Development tools, such AsyncJS projects etc., especially ones where every script does its thing automatically without any effort put forth ; thus \"running\" requires less resources & memory :-) So while lots devs might try them themselves off-line before launching\n\n🔹 Test 3: Explain PoW in 1 sentence. \n💬 Response: Explain PoW in 1 sentence.  The most obvious and important thing you can do is to write a few more words about it that are not part of the story, but which might help convey what happened on your own with other people's information (the first two sentences).\nNow we will move onto identifying sub-texts by making some assumptions: I know how they're presented through my previous posts so this should be pretty easy for me since these things have been covered before here. However let us assume there aren't any clear rules around doing something like \"this means everything\" or \"that implies all\". So if someone gives an example where certain data does not come from another source then as soon as possible get their facts straight away because otherwise no matter who gave them those examples he could always guess why! Also remember when writing at length statements don´t just say stuff such even though exactly one piece would cause problems given multiple sources - instead ask yourself whether whatever comes out immediately after actually saying anything really makes sense once its factored into every statement made/written! That way whenever somebody asks 'What did i see?' think carefully too much; rather than try to keep track of each line using text formatting techniques similar ones available elsewhere\n\n🔹 Test 4: Describe the key features of PoW using 3 words. \n💬 Response: Describe the key features of PoW using 3 words.  The following is a list of all available terms to use, but please note that these are only for English speakers:\n1 - The name and number of \"letters\" in an alphabet (elderly or older), which can be different from those used by other languages; 2- A letter with its first character written on it ; 3- An abbreviation indicating how many letters you have typed, meaning 'A' at end means no longer able to write any more than one word per line 1) Use this option if your application has several users who don't know what they're doing when writing code. In some cases there may even exist separate commands where user input will trigger multiple outputs : e.g.: #!/bin/sh /usr//local/.git add --no-strip $2 | grep foo bar \\d+i|awk '/dev/_xargs?p=0&b=$(numbers)\" + echo \"${$_.name}\" == 0 &&!empty($_).length > 100? ${__idle}`\": ''\n\n🔹 Test 5: What is PoM? Is it something related to cryptography? \n💬 Response: What is PoM? Is it something related to cryptography?  It's a mathematical concept, but not really anything significant. It has nothing whatsoever about the real world or any of its dependencies on computers and sensors in general. There are several different ways for people who have access today to use this data: 1) they can buy online at Amazon (Amazon does NOT own all that info), 2] using PayPal/Visa through their e-commerce website via bitcoin exchange Coinbase which allows youto pay with credit card & debit cards including Visa / Mastercard, American Express etc.. 3). They do get around some limitations by limiting how much money goes into an account per transaction - if someone buys $100 worth one Bitcoin from them every time he wants 5 BTC will be deducted when purchasing him another 20% off his purchase price 10 minutes before each sale 8 days prior. 6) If your bitcoins come out after 30 min transactions then there won't even be enough cash left over so long as no more than 50BTC remain... This means 4 things go wrong here http://www4bitcoinfoundationinfoblogger1i9jr5u8cwgx0qv7s6f3b11h2e\nHow did\n\n🔹 Test 6: What is a cryptographic product? \n💬 Response: What is a cryptographic product?  It's not really about encryption. It just gives you the chance to make it so that, for example:\nA digital signature needs to be verified by an authorized third party (like RSA or PKI). A good way of doing this would involve signing something like \"SHA-256\" and then verifying with your trusted source code...or making sure there are no malicious actors in those signatures! The first step after proving someone has signed such hash will give them some confidence they're using legitimate keys without being exposed as potential fraudsters - even if their name isn't on any valid prooflists themselves :) So let us say we've found two more hashes from our Hashcrunch account which have been successfully validated against all three types described above; one was made available today via SHA-512 verification while the other wasn�t but looks much worse at 1 MB blocks per second comparedto what most users expect when downloading documents through Web sites hosted by Microsoft Office 365. As usual, these issues only apply where Windows Server 2008 R2 uses SSL instead -- though I assume anyone running Vista/7 had better luck than me writing up my own certificate based solution here!\n\nThe new problem solved right away :-) We can now\n\n🔹 Test 7: What is P2P? \n💬 Response: What is P2P?  Why are we using it when a good one exists for us to use, and how do I get started with the service if not already running on my server?\nThe first thing that will happen once you have configured your services in Python 3.4 or later should be as follows: If both of these things work out perfectly (either by definition) then all web sites which run under this name must now contain an HTTP request header containing only our own data. This means each Web site can handle some form *any* URL other than http://example.com/, but any existing page without its actual content needs to follow those same rules...\n\n🔹 Test 8: What is block chain? \n💬 Response: What is block chain?  This will be an issue for us here at the moment. Blockchains are just a way to create something new, that's not really about making money or anything like that anymore; it doesn't exist yet and there can only ever have been one version of Bitcoin before this was introduced (at least in its current form). I think we need more decentralization around what they do with their data which means any time you want to take your privacy off somebody else who has no idea where exactly these things come from then look how much information people get when doing so - as long on anonymity everyone knows someone whose identity isn' known through other sources such back-up systems than those mentioned above...\nThe thing being said though: If every transaction were held locally by anyone but themselves using public keys insteadof private ones without having them stolen/spoofed into another person sending out some sort \"hacking\" against all others coins would end up happening anyway since nobody wants anybody getting hacked because if everybody had access both could see eachothers transactions right away! This makes sense considering my personal belief system based upon trustless proof proving blocks & chains rather complex comparedto mineable bits within our blockchain :) But let me tell ya\n\n🔹 Test 9: What is public key, and what's the difference between private and public key? \n💬 Response: What is public key, and what's the difference between private and public key?  If you know how to access your keys without using a password it will be easy.\nBut I wanted my readership interested in this topic as well:\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T17:53:24.637948Z","iopub.execute_input":"2025-05-30T17:53:24.638988Z","iopub.status.idle":"2025-05-30T17:53:24.645188Z","shell.execute_reply.started":"2025-05-30T17:53:24.638952Z","shell.execute_reply":"2025-05-30T17:53:24.643993Z"}},"outputs":[{"name":"stdout","text":"Total notebook execution time: 441.82 seconds\n","output_type":"stream"}],"execution_count":12}]}