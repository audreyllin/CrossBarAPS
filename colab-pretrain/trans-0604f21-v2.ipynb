{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12062706,"sourceType":"datasetVersion","datasetId":7592589}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Environment Setup\n# =================================\nimport os\nimport sys\nimport json\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport transformers\nfrom datasets import Dataset, load_dataset, DatasetDict, concatenate_datasets\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    pipeline\n)\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"NO_TF\"] = \"1\"  # Prevent TensorFlow import issues\n\n# Verify installations\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:28:14.870711Z","iopub.execute_input":"2025-06-05T04:28:14.870989Z","iopub.status.idle":"2025-06-05T04:28:49.838920Z","shell.execute_reply.started":"2025-06-05T04:28:14.870965Z","shell.execute_reply":"2025-06-05T04:28:49.838192Z"}},"outputs":[{"name":"stderr","text":"2025-06-05 04:28:33.396239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749097713.646506      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749097713.722304      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.6.0+cu124\nTransformers: 4.51.3\nCUDA available: False\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Model Loading\n# ============================\nMODEL_NAME = \"gpt2\"\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics\"\"\"\n    import psutil\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent:.1f}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    device = \"cpu\"\n    torch_dtype = torch.float32\n    \n    try:\n        print(\"Attempting standard CPU load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=None,\n            torch_dtype=torch_dtype\n        ).to(device)\n        print(\"\\n✅ Model loaded successfully on CPU!\")\n        return model\n    except Exception as e:\n        print(f\"\\n❌ Standard load failed: {str(e)}\")\n        raise RuntimeError(\"Unable to load model on CPU\")\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:28:49.840203Z","iopub.execute_input":"2025-06-05T04:28:49.840776Z","iopub.status.idle":"2025-06-05T04:28:52.987173Z","shell.execute_reply.started":"2025-06-05T04:28:49.840755Z","shell.execute_reply":"2025-06-05T04:28:52.986057Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 6.0% (1.4/31.4GB)\nAttempting standard CPU load...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d72b06afad694c72a55fec5c22af221f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d54214f6cadc4724beeaedab1a354364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2533d189345435d84aa5b54ae6fa87d"}},"metadata":{}},{"name":"stdout","text":"\n✅ Model loaded successfully on CPU!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup\n# ==============================\ndef load_tokenizer(model_name):\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:28:52.988085Z","iopub.execute_input":"2025-06-05T04:28:52.988318Z","iopub.status.idle":"2025-06-05T04:28:54.038519Z","shell.execute_reply.started":"2025-06-05T04:28:52.988293Z","shell.execute_reply":"2025-06-05T04:28:54.037741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc052b3f3afd4f8181f8c9b2599a726f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f11c3e78d22432ebaf566089ebe1a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b53e8edd3fed432ab1220bf260f3eb14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e09172fc43b4e43aeedf0da874e3c65"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Dataset Preparation\n# ==================================\ndef extract_qna_from_notebook(notebook_path):\n    \"\"\"Extract Q&A pairs from Jupyter notebook cells\"\"\"\n    try:\n        with open(notebook_path, 'r', encoding='utf-8') as f:\n            notebook = json.load(f)\n        \n        qna_pairs = []\n        qna_patterns = [\n            r\"(?:^|\\n)(Q|Question)[:：]?\\s*(.+?)\\s*(A|Answer)[:：]?\\s*(.+?)(?=\\n\\s*(?:Q|Question|$))\",\n            r\"##\\s*(.+?)\\s*\\n([\\s\\S]+?)\\n(?:##|\\Z)\",\n            r\"\\\"\\\"\\\"\\s*Q:(.+?)\\nA:(.+?)\\\"\\\"\\\"\",\n            r\"<qa>\\n<q>(.+?)</q>\\n<a>(.+?)</a>\\n</qa>\"\n        ]\n        \n        for cell in notebook['cells']:\n            if cell['cell_type'] == 'markdown':\n                text = ''.join(cell['source'])\n                \n                for pattern in qna_patterns:\n                    matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n                    if matches:\n                        for match in matches:\n                            if len(match) == 4:\n                                question = match[1].strip()\n                                answer = match[3].strip()\n                            elif len(match) == 2:\n                                question = match[0].strip()\n                                answer = match[1].strip()\n                            else:\n                                continue\n                                \n                            if question and answer:\n                                qna_pairs.append(f\"Q: {question}\\nA: {answer}\")\n        \n        return qna_pairs\n    \n    except Exception as e:\n        print(f\"Error processing notebook: {str(e)}\")\n        return []\n\ndef create_fallback_dataset():\n    \"\"\"Create fallback dataset using reliable public datasets\"\"\"\n    crypto_qa = [\n        \"Q: What is Bitcoin? A: Bitcoin is a decentralized digital currency.\",\n        \"Q: What is Ethereum? A: Ethereum is a blockchain platform with smart contract functionality.\",\n        \"Q: What is a blockchain? A: A distributed ledger technology recording transactions across networks.\",\n        \"Q: What is a crypto wallet? A: Software/hardware storing private keys to manage cryptocurrencies.\",\n        \"Q: What is mining in cryptocurrency? A: The process of validating transactions and creating new coins.\",\n        \"Q: What is DeFi? A: Decentralized finance using blockchain without traditional intermediaries.\",\n        \"Q: What is an NFT? A: Non-fungible token representing unique digital ownership.\",\n        \"Q: What is a smart contract? A: Self-executing contracts with terms directly written into code.\",\n        \"Q: What is proof-of-stake? A: Consensus mechanism where validators stake crypto to secure network.\",\n        \"Q: What is a DAO? A: Decentralized Autonomous Organization governed by smart contracts.\"\n    ]\n    return Dataset.from_dict({\"text\": crypto_qa})\n\ndef prepare_dataset(file_path=\"/kaggle/input/database4\", max_samples=1000):\n    try:\n        print(f\"\\n🔍 Searching for dataset at: {file_path}\")\n        data_path = Path(file_path)\n        \n        if not data_path.exists():\n            raise FileNotFoundError(f\"Dataset path not found: {data_path}\")\n        \n        # 1. Check for specific notebook file\n        specific_notebook = data_path / \"database-0604.ipynb\"\n        if specific_notebook.exists():\n            print(f\"✅ Found specific notebook file: {specific_notebook}\")\n            qna_pairs = extract_qna_from_notebook(specific_notebook)\n            \n            if qna_pairs:\n                print(f\"Extracted {len(qna_pairs)} Q&A pairs from notebook\")\n                return Dataset.from_dict({\"text\": qna_pairs[:max_samples]})\n            else:\n                print(\"No Q&A pairs found in notebook, checking other sources\")\n        \n        # 2. Look for other notebook files\n        notebook_files = list(data_path.rglob('*.ipynb'))\n        if notebook_files:\n            print(f\"Found {len(notebook_files)} notebook files\")\n            for notebook_path in notebook_files:\n                if notebook_path == specific_notebook:\n                    continue\n                print(f\"Processing notebook: {notebook_path.name}\")\n                qna_pairs = extract_qna_from_notebook(notebook_path)\n                if qna_pairs:\n                    print(f\"Extracted {len(qna_pairs)} Q&A pairs from {notebook_path.name}\")\n                    return Dataset.from_dict({\"text\": qna_pairs[:max_samples]})\n        \n        # 3. Try loading Q&A pairs from JSON\n        qna_files = list(data_path.rglob(\"*qna*.json\")) + list(data_path.rglob(\"*qa*.json\"))\n        if qna_files:\n            print(f\"Found {len(qna_files)} Q&A files\")\n            dataset = load_dataset('json', data_files=[str(f) for f in qna_files], \n                                  split=f'train[:{max_samples}]')\n            return dataset\n        \n        # 4. Fallback to raw text extraction\n        text_files = list(data_path.rglob('*.txt')) + list(data_path.rglob('*.md'))\n        if text_files:\n            print(f\"Found {len(text_files)} text files\")\n            texts = []\n            for file in text_files[:10]:\n                try:\n                    with open(file, 'r', encoding='utf-8') as f:\n                        texts.append(f.read())\n                except Exception as e:\n                    print(f\"Skipped file {file.name}: {str(e)}\")\n            return Dataset.from_dict({\"text\": texts[:max_samples]})\n        \n        raise ValueError(\"No usable data files found\")\n    \n    except Exception as e:\n        print(f\"\\n❌ Dataset loading failed: {str(e)}\")\n        print(\"Creating fallback dataset...\")\n        return create_fallback_dataset()\n\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with error handling\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=256,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except Exception as e:\n        print(f\"Tokenization error: {str(e)}\")\n        return {\n            \"input_ids\": [[0]*256],\n            \"attention_mask\": [[1]*256],\n            \"labels\": [[0]*256]\n        }\n\n# Process dataset\nprint(\"\\n=== Starting Data Processing ===\")\ndataset = prepare_dataset(\"/kaggle/input/database4\")\n\n# Show sample data\nprint(\"\\nSample data:\")\nfor i in range(min(3, len(dataset))):\n    print(f\"\\nSample {i+1}:\")\n    print(dataset[i]['text'][:200] + \"...\")\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(\n    safe_tokenize, \n    batched=True, \n    batch_size=4\n)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Split dataset\nsplit_datasets = tokenized_dataset.train_test_split(test_size=0.2)\ntokenized_dataset = DatasetDict({\n    \"train\": split_datasets[\"train\"],\n    \"test\": split_datasets[\"test\"]\n})\nprint(f\"✅ Dataset split: {len(tokenized_dataset['train'])} train, {len(tokenized_dataset['test'])} test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:28:54.039506Z","iopub.execute_input":"2025-06-05T04:28:54.039801Z","iopub.status.idle":"2025-06-05T04:28:54.184843Z","shell.execute_reply.started":"2025-06-05T04:28:54.039776Z","shell.execute_reply":"2025-06-05T04:28:54.184103Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Data Processing ===\n\n🔍 Searching for dataset at: /kaggle/input/database4\n✅ Found specific notebook file: /kaggle/input/database4/database-0604.ipynb\nNo Q&A pairs found in notebook, checking other sources\nFound 1 notebook files\n\n❌ Dataset loading failed: No usable data files found\nCreating fallback dataset...\n\nSample data:\n\nSample 1:\nQ: What is Bitcoin? A: Bitcoin is a decentralized digital currency....\n\nSample 2:\nQ: What is Ethereum? A: Ethereum is a blockchain platform with smart contract functionality....\n\nSample 3:\nQ: What is a blockchain? A: A distributed ledger technology recording transactions across networks....\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6092b3b3bb804ae19e885287c4f6588d"}},"metadata":{}},{"name":"stdout","text":"✅ Dataset split: 8 train, 2 test\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 5: Training Configuration - FIXED\n# =====================================\nmodel.gradient_checkpointing_enable()\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\ntraining_args = TrainingArguments(\n    output_dir=f\"./{MODEL_NAME}-crypto-expert\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    num_train_epochs=3,\n    learning_rate=2e-5,\n    optim=\"adamw_torch\",\n    logging_steps=10,\n    eval_strategy=\"epoch\",  # Changed from evaluation_strategy to eval_strategy\n    save_strategy=\"epoch\",\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    use_cpu=True\n)\n\n# Prepare model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:30:37.987998Z","iopub.execute_input":"2025-06-05T04:30:37.988273Z","iopub.status.idle":"2025-06-05T04:30:38.043766Z","shell.execute_reply.started":"2025-06-05T04:30:37.988254Z","shell.execute_reply":"2025-06-05T04:30:38.042611Z"}},"outputs":[{"name":"stdout","text":"trainable params: 884,736 || all params: 125,324,544 || trainable%: 0.7060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 6: Training Execution\n# =================================\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"test\"],\n        data_collator=data_collator\n    )\n    \n    print(\"\\n=== Starting Training ===\")\n    trainer.train()\n    \n    # Save model\n    output_dir = training_args.output_dir\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"\\n✅ Model saved to {output_dir}\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:30:38.045130Z","iopub.execute_input":"2025-06-05T04:30:38.045482Z","iopub.status.idle":"2025-06-05T04:31:29.088115Z","shell.execute_reply.started":"2025-06-05T04:30:38.045451Z","shell.execute_reply":"2025-06-05T04:31:29.087418Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Starting Training ===\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>3.724031</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>3.720621</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>3.718847</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n✅ Model saved to ./gpt2-crypto-expert\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 7: Model Testing\n# =================================\ndef test_model(model, tokenizer, test_prompts=None):\n    \"\"\"Test the trained model\"\"\"\n    if test_prompts is None:\n        test_prompts = [\n            \"What is a hardware wallet?\",\n            \"Explain Proof of Work in blockchain\",\n            \"How does cryptocurrency mining work?\",\n            \"What are the benefits of decentralized finance?\",\n            \"Describe how smart contracts work\"\n        ]\n    \n    pipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        device=-1  # Force CPU\n    )\n    \n    print(\"\\n=== Model Test Results ===\")\n    for prompt in test_prompts:\n        print(f\"\\nPrompt: {prompt}\")\n        output = pipe(\n            prompt,\n            max_length=150,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            num_return_sequences=1\n        )\n        print(\"Response:\", output[0]['generated_text'])\n\ntest_model(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:31:29.089349Z","iopub.execute_input":"2025-06-05T04:31:29.089686Z","iopub.status.idle":"2025-06-05T04:31:57.566753Z","shell.execute_reply.started":"2025-06-05T04:31:29.089665Z","shell.execute_reply":"2025-06-05T04:31:57.565967Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\nThe model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Model Test Results ===\n\nPrompt: What is a hardware wallet?\nResponse: What is a hardware wallet?\n\nAn electronic wallet is an electronic device that can be used to create a digital wallet. It is used by the user to store, exchange and transfer funds between computers.\n\nThe user must not use the electronic wallet for any reason.\n\nHow can I use the wallet for my personal use?\n\nThe online wallet is a digital wallet that is a digital currency. It is used to create a digital wallet.\n\nWhat is a digital wallet?\n\nThe digital wallet is an electronic device that allows the user to store and exchange bitcoins. The digital wallet is used by the user to store, exchange and transfer bitcoins.\n\nHow do I use it for business?\n\nThe online wallet\n\nPrompt: Explain Proof of Work in blockchain\nResponse: Explain Proof of Work in blockchain technology\n\nAt the time, this was a relatively new concept and a very new and unique technology, but with the emergence of the Ethereum blockchain, it became a common sight in the crypto world. As it turned out, this technology is not only a new concept, but also a very different and important one.\n\nOne of the biggest problems with blockchain technology is that it is not easy to track the exact steps taken by the participants in the project. This means that any single person could have done a lot of different things in the past. A very large number of people had their work in the blockchain but only a small number of people had their work in the Ethereum blockchain. The result of this is that\n\nPrompt: How does cryptocurrency mining work?\nResponse: How does cryptocurrency mining work?\n\nThe main reason behind cryptocurrency mining is to mine coins for a profit. The mining process is very simple. You need to use a mining software to mine the coins. You also need to pay the miner a fee (usually 10-15 cents per coin) to mine the coins.\n\nHow do I find out more about cryptocurrencies?\n\nYou can check out our website at http://www.cryptonews.com.\n\nWhat's the difference between Bitcoin and Litecoin?\n\nLitecoin is a cryptocurrency that uses the blockchain to store money. It uses a public ledger to keep track of all transactions.\n\nLitecoin is a cryptocurrency that uses a blockchain to store money.\n\nPrompt: What are the benefits of decentralized finance?\nResponse: What are the benefits of decentralized finance?\n\nA lot. You can create a platform for people to participate in a decentralized economy. It can make an economic sense to buy and sell goods and services, and you can create a system that allows people to control what's happening around them. This is where we're at right now.\n\nA lot of people think that decentralized finance is the next big thing. The future of finance is going to be decentralized. It's going to be a lot more decentralized. I think it's going to be a lot more decentralized. It's going to be a lot more decentralized.\n\nWe're going to see how much the next 10 years of finance will allow us to do. We're going to\n\nPrompt: Describe how smart contracts work\nResponse: Describe how smart contracts work in detail in our book.\n\nYou will need an HTML5 capable browser to see this content. Play Replay with sound Play with\n\nsound 00:00 00:00\n\nThis is a very long paper that is based on a collection of papers from the ACM Transactions on Financial Markets, which were published in May 2016. It was published as part of the ACM Transactions on Financial Markets conference in San Francisco, CA.\n\nA paper titled \"Smart Contract Implementation in a Non-Recursive Market\" was released in February 2016.\n\nThe paper provides a basic framework for implementing smart contracts, such as contracts that deal with a single value, and that allow for the execution of multiple contracts at\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 7: Enhanced Model Saving with Shard Support\n# ===============================================\n\n# Add missing import\nfrom typing import Optional\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"💽 Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n⚠️ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:31:57.568466Z","iopub.execute_input":"2025-06-05T04:31:57.568777Z","iopub.status.idle":"2025-06-05T04:31:57.577994Z","shell.execute_reply.started":"2025-06-05T04:31:57.568749Z","shell.execute_reply":"2025-06-05T04:31:57.577239Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 8: Robust Model Loading and Testing with PEFT support - COMPLETE\n# ========================================================\nfrom typing import Optional\nfrom peft import PeftModel\nfrom transformers import pipeline\n\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 160,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    - Uses the same base model we trained on (GPT-2)\n    - Handles both PEFT and regular model loading\n    - Includes detailed error reporting\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.float32\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first - using the same base model we trained on (GPT-2)\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    MODEL_NAME,  # Using our original MODEL_NAME (gpt2)\n                    torch_dtype=torch.float32\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float32\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        model.eval()  # Set to evaluation mode\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?\",\n                \"What is Proof of Work (PoW)?\",\n                \"What is cryptography?\",\n                \"What is Peer-to-Peer (P2P)?\",\n                \"What is block chain?\",\n                \"What is private key?\"\n            ]\n        \n        # Create pipeline\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            device=-1,  # Force CPU usage\n            pad_token_id=tokenizer.eos_token_id  # Ensure proper padding\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        results = []\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            try:\n                output = pipe(\n                    prompt,\n                    max_length=max_length,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    num_return_sequences=1,\n                    repetition_penalty=1.2\n                )\n                response = output[0]['generated_text']\n                print(\"💬 Response:\", response)\n                results.append({\"prompt\": prompt, \"response\": response})\n            except Exception as e:\n                print(f\"⚠️ Error generating response: {str(e)}\")\n                results.append({\"prompt\": prompt, \"error\": str(e)})\n        \n        return model, tokenizer, results\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # Save model and adapter\n    print(\"💽 Saving model and adapter...\")\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify saved files\n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:37:00.100937Z","iopub.execute_input":"2025-06-05T04:37:00.101245Z","iopub.status.idle":"2025-06-05T04:37:00.119281Z","shell.execute_reply.started":"2025-06-05T04:37:00.101223Z","shell.execute_reply":"2025-06-05T04:37:00.118223Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    model_path = \"/kaggle/working/gpt2-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    print(\"\\n=== Testing with default prompts ===\")\n    model, tokenizer, default_results = load_and_test_model(\n        model_path, \n        is_peft_model=True\n    )\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \"What is software wallet, and what's the difference between hardware and software wallet?\",\n        \"What is PoW?\",\n        \"Explain PoW in 1 sentence.\",\n        \"Describe the key features of PoW using 3 words.\",\n        \"What is PoM? Is it something related to cryptography?\",\n        \"What is a cryptographic product?\",\n        \"What is P2P?\",\n        \"What is block chain?\",\n        \"What is public key, and what's the difference between private and public key?\"\n    ]\n    \n    print(\"\\n=== Testing with custom prompts ===\")\n    model, tokenizer, custom_results = load_and_test_model(\n        model_path, \n        test_prompts=custom_prompts, \n        is_peft_model=True\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T04:37:00.120726Z","iopub.execute_input":"2025-06-05T04:37:00.121330Z","iopub.status.idle":"2025-06-05T04:38:23.115664Z","shell.execute_reply.started":"2025-06-05T04:37:00.121304Z","shell.execute_reply":"2025-06-05T04:38:23.114794Z"}},"outputs":[{"name":"stdout","text":"\n💾 Saving model artifacts to: /kaggle/working/gpt2-lora-trained\n💽 Saving model and adapter...\n🔤 Saving tokenizer...\n📝 Saving training arguments...\n\n🔍 Verifying saved files:\n- tokenizer.json (3474.39 KB)\n- adapter_config.json (0.69 KB)\n- merges.txt (445.62 KB)\n- README.md (4.96 KB)\n- vocab.json (779.45 KB)\n- special_tokens_map.json (0.13 KB)\n- training_args.json (3.95 KB)\n- adapter_model.safetensors (3461.98 KB)\n- tokenizer_config.json (0.52 KB)\n\n=== Testing with default prompts ===\n\n🔍 Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\n📂 Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.69 KB)\n- adapter_model.safetensors (3461.98 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (3474.39 KB)\n- tokenizer_config.json (0.52 KB)\n- training_args.json (3.95 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\nFound adapter files: ['adapter_config.json', 'adapter_model.safetensors']\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\n🎉 Model loaded successfully!\n\n🚀 Creating text generation pipeline...\n\n🧪 Running generation tests...\n\n🔹 Test 1: What is hardware wallet?\n💬 Response: What is hardware wallet?\nThe software to store your money in a digital currency called Bitcoin. The technology, which was developed by Chinese-based developer Guo Huijun and his team at the University of Zurich (UZ), has been used as an alternative payment system for many years but its popularity continues growing rapidly because it enables users to pay with their smartphone or tablet without ever having even had access into what's known as \"virtual wallets\". It can be downloaded from any major online retailer such Google Play Store - not including eBay where there are no physical stores selling bitcoins yet -- although they have also received support through Bitpay: https://bitcointalk.org/index...\n\n🔹 Test 2: What is Proof of Work (PoW)?\n💬 Response: What is Proof of Work (PoW)?\nThe PoS model uses a number of algorithms to determine if the output has proof. The algorithm used in this paper, called \"Proof of Stake\", was created by Thomas Wiens and Daniel Juhlmann at Harvard University using data from three different sources: peer-reviewed journals such as IEEE Transactions on Computer Networks; industry research publications including Journal Abstracts for Scientific Research International's Institute for Information Security Technology Standards Committee member journal Nature Communications Protocol Working Group guidelines that describe how each protocol works under its own rules or standards—including those based upon existing protocols published before 2003.[11] In addition,[12][13]. Although it may be possible with an older version [14], there are still many problems associated with relying solely onto old versions because these\n\n🔹 Test 3: What is cryptography?\n💬 Response: What is cryptography?\nThere are several different cryptographic techniques and protocols that we use to encrypt data. The most common one being AES, or Encrypt-Time (ETC). It's basically the same as RSA but with a key length of 2 bits instead in addition to its elliptic curve algorithm which means it doesn't take up more than 3 seconds for an attacker to download your computer from Google Drive if they're using SSL/TLS servers like Dropbox. In this case however you can make sure there aren´t any issues because once again all encryption keys will be encrypted before anyone else gets them! This technique has been used by some companies such Asymmetric Key Exchange Systems Incorporated Ltd., eCryptoSecurity LLC, NIST Research Labs Corporation www_cryptocurrency A number have\n\n🔹 Test 4: What is Peer-to-Peer (P2P)?\n💬 Response: What is Peer-to-Peer (P2P)?\nA P1P network consists of a peer and an observer that can act as the \"proxy\" between these two nodes. The proxy has to perform many operations on each other, such processes are called peers or proxies in this context. A good example would be how you could get your own website from someone else without having any money for hosting it: http://www3p6hjrzkqf4g/peerage/. You use public keys by sending them over encrypted channels so they cannot steal our data! However if we connect directly with those same people using their private key then what's going through all my files? Why not just send us some random email which will show up at every step along the way\n\n🔹 Test 5: What is block chain?\n💬 Response: What is block chain?\nThe basic idea behind this protocol has been to allow you to store all transactions in one place on your blockchain, and make it easy for anyone who wants them. It's just not a good solution if the network doesn't support many of these features at once - as long that means there are no more transaction outputs being sent from nodes where they don (or shouldn) be allowed. You could also create an extra layer between blocks which would have different benefits over other protocols like Bitcoin or Ethereum; each node will provide its own version of their respective consensus algorithm but we're still going with those two things because even though some people may prefer chains designed around \"one-way\" connections I think most miners already do both! So what does BlockChain really offer? The current proposal\n\n🔹 Test 6: What is private key?\n💬 Response: What is private key?\nThe Private Key of a Public Certificate. The public keys that are created by this method must be issued to an authorized user as follows: the person issuing it, or at least one trusted party who can verify their identity using your credentials (e-mail addresses) and other sensitive information from within its own system (\"private\"). In most cases all users will need to provide these certificates for any use with whom they have communicated securely through email before you'll get them back in our service! All certificate providers should check out how many people sign up on each server which means if there's only 1 non-signed signature we may not take care what happens when no more signatures come into play since some servers do generate multiple signed ones every time; once again just make sure those numbers\n\n=== Testing with custom prompts ===\n\n🔍 Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\n📂 Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.69 KB)\n- adapter_model.safetensors (3461.98 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (3474.39 KB)\n- tokenizer_config.json (0.52 KB)\n- training_args.json (3.95 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\nFound adapter files: ['adapter_config.json', 'adapter_model.safetensors']\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\n🎉 Model loaded successfully!\n\n🚀 Creating text generation pipeline...\n\n🧪 Running generation tests...\n\n🔹 Test 1: What is software wallet, and what's the difference between hardware and software wallet?\n💬 Response: What is software wallet, and what's the difference between hardware and software wallet?\nIt depends. Hardware wallets are more secure because they allow you to store your money in a safe place without having it be stored on an external hard drive or USB stick (or even outside of jail). Software-based systems that use \"smart\" technology such as Bluetooth make their transactions safer by making sure no data from any device ever gets lost while using them. That means anyone can easily change funds at will — especially if someone else has access for security reasons! You'll find out which methods work best when comparing different versions now with our guides here. It also allows us better control over how much I need my Bitcoins so we're able both keep track of all transaction fees across multiple devices rather than just one per user\n\n🔹 Test 2: What is PoW?\n💬 Response: What is PoW?\nIn this post, I'll share some of the concepts behind it. In my previous posts on WIP and ZFS (and other tools), we covered how to use NTFS filesystems with a new tool called OpenSSH which can be used as an example for creating SSH tunnels from your local network using PTP or similar techniques like ssh2create_ssh. The following sections will show you what these methods are all about:\n\n A quick note before reading any further! This article assumes that everyone has read our blog over at Bitbucket, so don't worry if they aren't there in order not only because their website isn´d great but also due both technical knowledge being available through GitHub's open source community-driven project repository - https\n\n🔹 Test 3: Explain PoW in 1 sentence.\n💬 Response: Explain PoW in 1 sentence.\nLifetime Warranty of Lifetime Value If you own a home or business, your lifetime warranty is very good if it lasts more than 5 years (as long as the product meets all other conditions). The life expectancy for any given item can vary from 3-10 years depending on how well that part performs and its quality/availability; however, after 7+ months this would be considered to have expired at some point because most products are not manufactured until 2045. As such, we recommend purchasing an original copy by clicking here: http://www1mw3r9sxkfq4d2pvz8tjbh0n6y7e/. It will cover almost ALL parts covered under our warranties except those required due\n\n🔹 Test 4: Describe the key features of PoW using 3 words.\n💬 Response: Describe the key features of PoW using 3 words.\nThe following is an example that shows how to create a new user with this command:\n\n- --password /path/to/user_key -t '{\"username\":\"your password\",\"expirationDate\":\"2016011221\"}' // This will take care of all your passwords, but you can also include them in some other order (see below) add mynewuid=<email address> {myname = \"My Name\";...} } Now we have our username and email addresses on file account for now so let's get started! We'll use SSH as first method after adding it from root. The most important thing here would be if there are any problems or errors while doing anything else go ahead :) And\n\n🔹 Test 5: What is PoM? Is it something related to cryptography?\n💬 Response: What is PoM? Is it something related to cryptography? Do you think that's what the project aims to achieve with its cryptographic tools, and are there any questions we should ask about this particular issue as well?\nTrevor: I don't know how much information on Cryptowire actually exists. There certainly isn't anything like a public forum for people who have access or want help building their own cryptography solutions (as opposed ToS). It might be possible but probably not in practice so far due solely towards having enough interest from those willing pay some money if they can find an audience of interested parties looking into using your codebase instead than just getting paid by someone else trying out existing crypto-currencies without giving up control over them yet again... The only thing really relevant here\n\n🔹 Test 6: What is a cryptographic product?\n💬 Response: What is a cryptographic product?\nThe first part of the question asks: what's your security model. In general, there are three main categories that define an encryption standard for encrypting data and why it matters to you at all;\n\n \"Key-value pairs\" (KMS) or cryptographically signed key/key pair cryptography principles based on digital signatures used by software in order to validate transactions with each other as well as secure messaging protocols like Tor which protect against fraud attacks because they rely on keys being securely stored between them while not requiring users to enter any passwords into their browser so long before receiving payments via Bitcoin). They also include crypto tools such Asymmetric Encryption Key Exchange Protocols – one thing this specification does offer however most people will probably never even bother trying once again using these\n\n🔹 Test 7: What is P2P?\n💬 Response: What is P2P?\nI don't know how to explain the word \"pipeline\". I'm not a huge fan of it. It's an umbrella term that refers specifically, as you might imagine by its very name and in this case for your convenience: traffic management services (TMS). These are all different things - they're both designed with various purposes but one has their own specific goals so there will be many differences between them which may or perhaps cannot help anyone understand each other well enough though when considering what these two concepts actually do together we can see some interesting points about 'transport' being used here on my blog post below...\n\n So let me start off out without any further ado! Let us begin our journey into pwnage; first up was trying make\n\n🔹 Test 8: What is block chain?\n💬 Response: What is block chain?\nThe term \"blockchain\" refers to a set of cryptographic protocols or mechanisms that can be used by any computer system. For example, it may include some software systems such as computers and servers in which transactions are made using the network protocol (such data stored on disk). The various versions of Bitcoin differ from each other because they have different properties; for instance, many digital currencies use blocks rather than chains but also don't do so with respect not only security measures against frauds caused through malleability-based attacks like those perpetrated by hackers who try hard to steal your bitcoins without knowing what's going into them (\"blocking\") their existence has led to widespread misunderstanding about these technologies' effectiveness within monetary institutions today.[1] In contrast: A bitcoin transaction takes place over\n\n🔹 Test 9: What is public key, and what's the difference between private and public key?\n💬 Response: What is public key, and what's the difference between private and public key?\nA: The Public Key System (PKI) allows users to encrypt a document or any other data. This means that you can use your own personal information on PKIs in order not be tracked by government agencies like NSA without revealing anything about who we are with it! However, this doesn't mean there isn' nothing good out there for people using these systems… if they want access even remotely from their home computers then consider putting them into an encrypted location such as Dropbox...\n","output_type":"stream"}],"execution_count":13}]}