{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12062706,"sourceType":"datasetVersion","datasetId":7592589}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T00:57:58.549977Z","iopub.execute_input":"2025-06-05T00:57:58.550349Z","iopub.status.idle":"2025-06-05T00:57:58.560200Z","shell.execute_reply.started":"2025-06-05T00:57:58.550322Z","shell.execute_reply":"2025-06-05T00:57:58.559095Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Environment Setup - FIXED\n# =================================\nimport os\nimport sys\nimport json\nimport shutil\nimport numpy as np\nimport torch\nimport transformers\nfrom datasets import Dataset, load_dataset, DatasetDict\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"NO_TF\"] = \"1\"  # Prevent TensorFlow import issues\n\n# Install required packages\n!pip uninstall -y tensorflow  # Remove to prevent conflicts\n!pip install --upgrade pip setuptools wheel\n!pip install numpy==1.26.4 scipy==1.11.4\n!pip install torch==2.2.1+cpu torchvision==0.17.1+cpu torchaudio==2.2.1+cpu --index-url https://download.pytorch.org/whl/cpu\n!pip install transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install einops==0.7.0 tokenizers==0.19.1 sentencepiece==0.2.0\n!pip install scikit-learn==1.2.2 matplotlib==3.7.2\n!pip install langchain==0.1.16 faiss-cpu==1.7.4 tqdm==4.66.2 pandas==2.2.2\n\n# Verify installations\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T00:57:58.562155Z","iopub.execute_input":"2025-06-05T00:57:58.562553Z","iopub.status.idle":"2025-06-05T00:58:33.902412Z","shell.execute_reply.started":"2025-06-05T00:57:58.562495Z","shell.execute_reply":"2025-06-05T00:58:33.900555Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.11/dist-packages (1.11.4)\nLooking in indexes: https://download.pytorch.org/whl/cpu\nRequirement already satisfied: torch==2.2.1+cpu in /usr/local/lib/python3.11/dist-packages (2.2.1+cpu)\nRequirement already satisfied: torchvision==0.17.1+cpu in /usr/local/lib/python3.11/dist-packages (0.17.1+cpu)\nRequirement already satisfied: torchaudio==2.2.1+cpu in /usr/local/lib/python3.11/dist-packages (2.2.1+cpu)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (4.14.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1+cpu) (2024.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.1+cpu) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.1+cpu) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.1+cpu) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.1+cpu) (1.3.0)\nRequirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.11/dist-packages (4.41.2)\nRequirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\nRequirement already satisfied: datasets==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: accelerate==0.29.1 in /usr/local/lib/python3.11/dist-packages (0.29.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.32.4)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.66.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (2.2.1+cpu)\nRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (19.0.1)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.7)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0) (2024.2.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.20.0)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.18.0) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.4.26)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\nRequirement already satisfied: einops==0.7.0 in /usr/local/lib/python3.11/dist-packages (0.7.0)\nRequirement already satisfied: tokenizers==0.19.1 in /usr/local/lib/python3.11/dist-packages (0.19.1)\nRequirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers==0.19.1) (0.32.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.66.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (1.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.4.26)\nRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (23.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2) (1.17.0)\nRequirement already satisfied: langchain==0.1.16 in /usr/local/lib/python3.11/dist-packages (0.1.16)\nRequirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.11/dist-packages (1.7.4)\nRequirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.11/dist-packages (4.66.2)\nRequirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.0.40)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (3.11.18)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.6.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.0.38)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.1.53)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.0.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (0.1.147)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.11.4)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (2.32.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.16) (8.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.20.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16) (3.0.0)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain==0.1.16) (23.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (4.14.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.16) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.16) (2.4.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.1.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.3.1)\n\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nNumPy: 1.26.4\nPyTorch: 2.2.1+cpu\nTransformers: 4.41.2\nCUDA available: False\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Model Loading - FIXED\n# ============================\nMODEL_NAME = \"gpt2\"\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics\"\"\"\n    import psutil\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent:.1f}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    device = \"cpu\"\n    torch_dtype = torch.float32\n    \n    try:\n        print(\"Attempting standard CPU load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=None,\n            torch_dtype=torch_dtype\n        ).to(device)\n        print(\"\\n✅ Model loaded successfully on CPU!\")\n        return model\n    except Exception as e:\n        print(f\"\\n❌ Standard load failed: {str(e)}\")\n        raise RuntimeError(\"Unable to load model on CPU\")\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T00:58:33.904041Z","iopub.execute_input":"2025-06-05T00:58:33.905399Z","iopub.status.idle":"2025-06-05T00:58:35.048238Z","shell.execute_reply.started":"2025-06-05T00:58:33.905352Z","shell.execute_reply":"2025-06-05T00:58:35.046774Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 4.7% (1.0/31.4GB)\nAttempting standard CPU load...\n\n✅ Model loaded successfully on CPU!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Tokenizer Setup - FIXED\n# ==============================\ndef load_tokenizer(model_name):\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T00:58:35.051251Z","iopub.execute_input":"2025-06-05T00:58:35.051607Z","iopub.status.idle":"2025-06-05T00:58:35.479125Z","shell.execute_reply.started":"2025-06-05T00:58:35.051584Z","shell.execute_reply":"2025-06-05T00:58:35.477814Z"}},"outputs":[{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Dataset Preparation - EXECUTE IMMEDIATELY\n# ==================================\nfrom pathlib import Path\nfrom datasets import load_dataset, Dataset, DatasetDict\nimport json\nimport re\nimport os\nfrom transformers import AutoTokenizer\n\ndef extract_qna_from_notebook(notebook_path):\n    \"\"\"Extract Q&A pairs from Jupyter notebook cells\"\"\"\n    try:\n        with open(notebook_path, 'r', encoding='utf-8') as f:\n            notebook = json.load(f)\n        \n        qna_pairs = []\n        qna_patterns = [\n            r\"(?:^|\\n)(Q|Question)[:：]?\\s*(.+?)\\s*(A|Answer)[:：]?\\s*(.+?)(?=\\n\\s*(?:Q|Question|$))\",\n            r\"##\\s*(.+?)\\s*\\n([\\s\\S]+?)\\n(?:##|\\Z)\",\n            r\"\\\"\\\"\\\"\\s*Q:(.+?)\\nA:(.+?)\\\"\\\"\\\"\",\n            r\"<qa>\\n<q>(.+?)</q>\\n<a>(.+?)</a>\\n</qa>\"\n        ]\n        \n        for cell in notebook['cells']:\n            if cell['cell_type'] == 'markdown':\n                text = ''.join(cell['source'])\n                \n                for pattern in qna_patterns:\n                    matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n                    if matches:\n                        for match in matches:\n                            if len(match) == 4:\n                                question = match[1].strip()\n                                answer = match[3].strip()\n                            elif len(match) == 2:\n                                question = match[0].strip()\n                                answer = match[1].strip()\n                            else:\n                                continue\n                                \n                            if question and answer:\n                                qna_pairs.append(f\"Q: {question}\\nA: {answer}\")\n        \n        return qna_pairs\n    \n    except Exception as e:\n        print(f\"Error processing notebook: {str(e)}\")\n        return []\n\ndef create_fallback_dataset():\n    \"\"\"Create fallback dataset using reliable public datasets\"\"\"\n    try:\n        return load_dataset(\"cryptodatadog/crypto-qa-fixed\", split='train[:100]')\n    except Exception as e:\n        print(f\"⚠️ Primary fallback failed: {str(e)}\")\n        try:\n            dataset = load_dataset(\"wiki_qa\", split='train[:200]')\n            crypto_keywords = ['bitcoin', 'blockchain', 'crypto', 'ethereum', 'wallet', 'mining']\n            filtered = dataset.filter(\n                lambda x: any(kw in x['question'].lower() for kw in crypto_keywords)\n            return filtered.select(range(min(100, len(filtered))))\n        except Exception as e2:\n            print(f\"⚠️ Secondary fallback failed: {str(e2)}\")\n            manual_qa = [\n                \"Q: What is Bitcoin? A: Bitcoin is a decentralized digital currency.\",\n                \"Q: What is Ethereum? A: Ethereum is a blockchain platform with smart contract functionality.\",\n                \"Q: What is a blockchain? A: A distributed ledger technology recording transactions across networks.\",\n                \"Q: What is a crypto wallet? A: Software/hardware storing private keys to manage cryptocurrencies.\",\n                \"Q: What is mining in cryptocurrency? A: The process of validating transactions and creating new coins.\",\n                \"Q: What is DeFi? A: Decentralized finance using blockchain without traditional intermediaries.\",\n                \"Q: What is an NFT? A: Non-fungible token representing unique digital ownership.\",\n                \"Q: What is a smart contract? A: Self-executing contracts with terms directly written into code.\",\n                \"Q: What is proof-of-stake? A: Consensus mechanism where validators stake crypto to secure network.\",\n                \"Q: What is a DAO? A: Decentralized Autonomous Organization governed by smart contracts.\"\n            ]\n            return Dataset.from_dict({\"text\": manual_qa})\n\ndef prepare_dataset(file_path=\"/kaggle/input/database4\", max_samples=1000):\n    try:\n        print(f\"\\n🔍 Searching for dataset at: {file_path}\")\n        data_path = Path(file_path)\n        \n        if not data_path.exists():\n            raise FileNotFoundError(f\"Dataset path not found: {data_path}\")\n        \n        # 1. Check for specific notebook file\n        specific_notebook = data_path / \"database-0604.ipynb\"\n        if specific_notebook.exists():\n            print(f\"✅ Found specific notebook file: {specific_notebook}\")\n            qna_pairs = extract_qna_from_notebook(specific_notebook)\n            \n            if qna_pairs:\n                print(f\"Extracted {len(qna_pairs)} Q&A pairs from notebook\")\n                return Dataset.from_dict({\"text\": qna_pairs[:max_samples]})\n            else:\n                print(\"No Q&A pairs found in notebook, checking other sources\")\n        \n        # 2. Look for other notebook files\n        notebook_files = list(data_path.rglob('*.ipynb'))\n        if notebook_files:\n            print(f\"Found {len(notebook_files)} notebook files\")\n            for notebook_path in notebook_files:\n                if notebook_path == specific_notebook:\n                    continue\n                print(f\"Processing notebook: {notebook_path.name}\")\n                qna_pairs = extract_qna_from_notebook(notebook_path)\n                if qna_pairs:\n                    print(f\"Extracted {len(qna_pairs)} Q&A pairs from {notebook_path.name}\")\n                    return Dataset.from_dict({\"text\": qna_pairs[:max_samples]})\n        \n        # 3. Try loading Q&A pairs from JSON\n        qna_files = list(data_path.rglob(\"*qna*.json\")) + list(data_path.rglob(\"*qa*.json\"))\n        if qna_files:\n            print(f\"Found {len(qna_files)} Q&A files\")\n            dataset = load_dataset('json', data_files=[str(f) for f in qna_files], \n                                  split=f'train[:{max_samples}]')\n            return dataset\n        \n        # 4. Fallback to raw text extraction\n        text_files = list(data_path.rglob('*.txt')) + list(data_path.rglob('*.md'))\n        if text_files:\n            print(f\"Found {len(text_files)} text files\")\n            texts = []\n            for file in text_files[:10]:\n                try:\n                    with open(file, 'r', encoding='utf-8') as f:\n                        texts.append(f.read())\n                except Exception as e:\n                    print(f\"Skipped file {file.name}: {str(e)}\")\n            return Dataset.from_dict({\"text\": texts[:max_samples]})\n        \n        raise ValueError(\"No usable data files found\")\n    \n    except Exception as e:\n        print(f\"\\n❌ Dataset loading failed: {str(e)}\")\n        print(\"Creating fallback dataset...\")\n        return create_fallback_dataset()\n\ndef safe_tokenize(examples, tokenizer):\n    \"\"\"Tokenization with error handling\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=256,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except Exception as e:\n        print(f\"Tokenization error: {str(e)}\")\n        return {\n            \"input_ids\": [[0]*256],\n            \"attention_mask\": [[1]*256],\n            \"labels\": [[0]*256]\n        }\n\n# --- IMMEDIATE EXECUTION STARTS HERE ---\nprint(\"\\n=== Starting Data Processing ===\")\ndataset = prepare_dataset(\"/kaggle/input/database4\")\n\n# Show sample data\nprint(\"\\nSample data:\")\nfor i in range(min(3, len(dataset))):\n    print(f\"\\nSample {i+1}:\")\n    print(dataset[i]['text'][:200] + \"...\")\n\n# Initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(\n    lambda x: safe_tokenize(x, tokenizer), \n    batched=True, \n    batch_size=4\n)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Split dataset\nsplit_datasets = tokenized_dataset.train_test_split(test_size=0.2)\ntokenized_dataset = DatasetDict({\n    \"train\": split_datasets[\"train\"],\n    \"test\": split_datasets[\"test\"]\n})\nprint(f\"✅ Dataset split: {len(tokenized_dataset['train'])} train, {len(tokenized_dataset['test'])} test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.035408Z","iopub.execute_input":"2025-06-05T01:10:17.036913Z","iopub.status.idle":"2025-06-05T01:10:17.062876Z","shell.execute_reply.started":"2025-06-05T01:10:17.036871Z","shell.execute_reply":"2025-06-05T01:10:17.061129Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_456/1530929372.py\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    filtered = dataset.filter(\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"],"ename":"SyntaxError","evalue":"'(' was never closed (1530929372.py, line 59)","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# Cell 5: Training Configuration - OPTIMIZED FOR SMALL DATA\n# =========================================================\nmodel.gradient_checkpointing_enable()\n\npeft_config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\n        \"Wqkv\", \"out_proj\", \"fc1\", \"fc2\"  # Phi-2 specific modules\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Cosine scheduler with proper warmup\ntraining_args = TrainingArguments(\n    output_dir=f\"./phi-2-crypto-expert\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    num_train_epochs=5,\n    learning_rate=2e-5,\n    optim=\"adamw_torch\",\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    eval_steps = 50,\n    save_strategy=\"epoch\",\n    fp16=torch.cuda.is_available(),  # Auto-enable FP16 if GPU available\n    bf16=False,\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    weight_decay=0.001,\n    use_cpu=True\n)\n\n# Prepare model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.063418Z","iopub.status.idle":"2025-06-05T01:10:17.063771Z","shell.execute_reply.started":"2025-06-05T01:10:17.063613Z","shell.execute_reply":"2025-06-05T01:10:17.063628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset, concatenate_datasets  # Add missing import\n\n# Augment small datasets\nif len(dataset) < 500:\n    print(\"Applying dataset augmentation...\")\n    \n    # 1. Paraphrase existing samples\n    def paraphrase(text):\n        \"\"\"Simple in-place augmentation\"\"\"\n        replacements = [\n            (\"cryptocurrency\", \"crypto\"),\n            (\"blockchain\", \"distributed ledger\"),\n            (\"wallet\", \"digital wallet\"),\n            (\"transaction\", \"tx\"),\n            (\"decentralized\", \"distributed\"),\n            (\"mining\", \"validation process\"),\n            (\"exchange\", \"trading platform\"),\n            (\"token\", \"digital asset\"),\n            (\"coin\", \"digital currency\"),\n            (\"key\", \"cryptographic key\"),\n            (\"proof\", \"consensus mechanism\")\n        ]\n        for a, b in replacements:\n            text = text.replace(a, b)\n        return text\n    \n    # 2. Add paraphrased versions\n    original_texts = dataset['text']\n    new_texts = [paraphrase(t) for t in original_texts]\n    \n    # Create augmented dataset\n    augmented = Dataset.from_dict({\"text\": new_texts})\n    dataset = concatenate_datasets([dataset, augmented])\n    \n    # 3. Add Q&A formatting\n    def format_qa(text):\n        keywords = [\"crypto\", \"blockchain\", \"bitcoin\", \"ethereum\", \n                    \"wallet\", \"defi\", \"nft\", \"key\", \"proof\", \"transaction\"]\n        words = text.split()\n        topic = next((word for word in words if word.lower() in keywords), words[0])\n        return f\"Question: Explain {topic} in cryptocurrency?\\nAnswer: {text}\"\n    \n    # Apply formatting\n    dataset = dataset.map(lambda x: {\"text\": format_qa(x['text'])}, \n                         batched=False,\n                         load_from_cache_file=False)\n    \n    # 4. Shuffle dataset\n    dataset = dataset.shuffle(seed=42)\n    \n    print(f\"Augmented dataset size: {len(dataset)}\")\n    print(\"✅ Applied paraphrasing, Q&A formatting, and shuffling\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.169680Z","iopub.execute_input":"2025-06-05T01:10:17.170170Z","iopub.status.idle":"2025-06-05T01:10:17.226144Z","shell.execute_reply.started":"2025-06-05T01:10:17.170140Z","shell.execute_reply":"2025-06-05T01:10:17.224934Z"}},"outputs":[{"name":"stdout","text":"Applying dataset augmentation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854dc0d0aa6749aebdf45c7c7c97c89e"}},"metadata":{}},{"name":"stdout","text":"Augmented dataset size: 20\n✅ Applied paraphrasing, Q&A formatting, and shuffling\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 6: Training Execution - FIXED\n# =================================\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    model.config.use_cache = False  # Disable cache for gradient checkpointing\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"test\"],\n        data_collator=data_collator\n    )\n    \n    print(\"\\n=== Starting Training ===\")\n    trainer.train()\n    \n    # Save model\n    output_dir = training_args.output_dir\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"\\n✅ Model saved to {output_dir}\")\n    return trainer\n\ntrainer = train_model(model, tokenized_dataset, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.228206Z","iopub.execute_input":"2025-06-05T01:10:17.228614Z","iopub.status.idle":"2025-06-05T01:10:17.276332Z","shell.execute_reply.started":"2025-06-05T01:10:17.228591Z","shell.execute_reply":"2025-06-05T01:10:17.274447Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_456/4280705782.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_456/4280705782.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenized_dataset, training_args)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_collator' is not defined"],"ename":"NameError","evalue":"name 'data_collator' is not defined","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# Cell 7: Enhanced Model Saving with Shard Support\n# ===============================================\n\n# Add missing import\nfrom typing import Optional\n\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"💽 Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n⚠️ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.277471Z","iopub.status.idle":"2025-06-05T01:10:17.277876Z","shell.execute_reply.started":"2025-06-05T01:10:17.277711Z","shell.execute_reply":"2025-06-05T01:10:17.277726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Robust Model Loading and Testing with PEFT support - FIXED\n# ========================================================\n# Add missing imports\nfrom peft import PeftModel\nfrom transformers import pipeline\n\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 160,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.float32\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    \"meta-llama/Llama-2-7b-chat-hf\",  # Load original base model\n                    torch_dtype=torch.float32\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float32\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        model.eval()  # Set to evaluation mode\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            device=-1  # Force CPU usage\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"💬 Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.279451Z","iopub.status.idle":"2025-06-05T01:10:17.279906Z","shell.execute_reply.started":"2025-06-05T01:10:17.279716Z","shell.execute_reply":"2025-06-05T01:10:17.279737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    model_path = \"/kaggle/working/gpt2-lora-trained\"\n    \n    # Save model artifacts\n    save_model_artifacts(model, tokenizer, training_args)\n    \n    # Load with explicit path and PEFT flag\n    load_and_test_model(model_path, is_peft_model=True)\n    \n    # Test with custom prompts\n    custom_prompts = [\n        \"What is software wallet, and what's the difference between hardware and software wallet? \",\n        \"What is PoW? \",\n        \"Explain PoW in 1 sentence. \",\n        \"Describe the key features of PoW using 3 words. \",\n        \"What is PoM? Is it something related to cryptography? \",\n        \"What is a cryptographic product? \",\n        \"What is P2P? \",\n        \"What is block chain? \",\n        \"What is public key, and what's the difference between private and public key? \"\n    ]\n    load_and_test_model(model_path, test_prompts=custom_prompts, is_peft_model=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.281450Z","iopub.status.idle":"2025-06-05T01:10:17.281837Z","shell.execute_reply.started":"2025-06-05T01:10:17.281675Z","shell.execute_reply":"2025-06-05T01:10:17.281690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:17.283436Z","iopub.status.idle":"2025-06-05T01:10:17.284064Z","shell.execute_reply.started":"2025-06-05T01:10:17.283900Z","shell.execute_reply":"2025-06-05T01:10:17.283918Z"}},"outputs":[],"execution_count":null}]}