{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":241495234,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clean up existing installations\n!pip uninstall -y numpy torch transformers bitsandbytes 2>/dev/null || echo \"Cleanup complete\"\n\n# Core package installation with compatible versions\n!pip install -q --upgrade pip\n!pip install -q \\\n    numpy==1.26.4 \\\n    pandas==2.2.2 \\\n    scipy==1.14.0 \\\n    scikit-learn==1.3.2 \\\n    spacy==3.7.4 \\\n    pdfplumber==0.11.0 \\\n    requests==2.32.3 \\\n    google-auth==2.38.0 \\\n    notebook==6.5.7 \\\n    fsspec==2024.2.0 \\\n    matplotlib==3.8.0 \\\n    google-api-core==2.19.1\n\n# PyTorch installation with CUDA 12.1 support\n!pip install -q \\\n    torch==2.2.1+cu121 \\\n    torchvision==0.17.1+cu121 \\\n    torchaudio==2.2.1+cu121 \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# Install compatible bitsandbytes version first\n!pip install -q bitsandbytes==0.42.0\n\n# NLP and ML ecosystem with compatible versions\n!pip install -q \\\n    transformers==4.41.2 \\\n    peft==0.10.0 \\\n    datasets==2.18.0 \\\n    accelerate==0.29.1 \\\n    sentence-transformers==3.4.1 \\\n    rich==13.7.1 \\\n    gymnasium==0.29.0\n\n# SpaCy model\n!python -m spacy download en_core_web_lg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T17:23:12.855151Z","iopub.execute_input":"2025-05-28T17:23:12.855971Z","iopub.status.idle":"2025-05-28T17:24:36.295018Z","shell.execute_reply.started":"2025-05-28T17:23:12.855925Z","shell.execute_reply":"2025-05-28T17:24:36.293774Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1+cu121\nUninstalling torch-2.2.1+cu121:\n  Successfully uninstalled torch-2.2.1+cu121\nFound existing installation: transformers 4.41.2\nUninstalling transformers-4.41.2:\n  Successfully uninstalled transformers-4.41.2\nFound existing installation: bitsandbytes 0.42.0\nUninstalling bitsandbytes-0.42.0:\n  Successfully uninstalled bitsandbytes-0.42.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\ntorchvision 0.17.1+cu121 requires torch==2.2.1, which is not installed.\npeft 0.10.0 requires torch>=1.13.0, which is not installed.\npeft 0.10.0 requires transformers, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\ntimm 1.0.15 requires torch, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npeft 0.10.0 requires transformers, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mCollecting en-core-web-lg==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.4.26)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.8)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_lg')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Required imports\nimport re\nimport importlib.metadata\nimport pdfplumber\nimport spacy\nimport requests\nimport pandas as pd\nimport torch\nimport time\nfrom io import BytesIO\nfrom typing import Dict, List\nfrom collections import OrderedDict\nfrom spacy.matcher import Matcher\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\n\n# Import transformers after bitsandbytes\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    pipeline,\n    DataCollatorForLanguageModeling\n)\n\n# Import peft after transformers\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\n\n# Verify installations\nprint(\"\\nInstallation verification:\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"Transformers version: {importlib.metadata.version('transformers')}\")\nprint(f\"Bitsandbytes version: {importlib.metadata.version('bitsandbytes')}\")\nprint(f\"scipy version: {importlib.metadata.version('scipy')}\")\nprint(f\"fsspec version: {importlib.metadata.version('fsspec')}\")\nprint(f\"gymnasium version: {importlib.metadata.version('gymnasium')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T17:24:36.297760Z","iopub.execute_input":"2025-05-28T17:24:36.298726Z","iopub.status.idle":"2025-05-28T17:24:48.659572Z","shell.execute_reply.started":"2025-05-28T17:24:36.298687Z","shell.execute_reply":"2025-05-28T17:24:48.658491Z"}},"outputs":[{"name":"stderr","text":"2025-05-28 17:24:41.166245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748453081.198755     236 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748453081.208398     236 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n\nInstallation verification:\nPyTorch version: 2.2.1+cu121\nCUDA available: False\nTransformers version: 4.41.2\nBitsandbytes version: 0.42.0\nscipy version: 1.14.0\nfsspec version: 2024.2.0\ngymnasium version: 0.29.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class SectionDetector:\n    def __init__(self, nlp=None):\n        self.nlp = nlp or spacy.load(\"en_core_web_lg\")\n        self._initialize_section_patterns()\n        self._refresh_matcher()\n\n    def _initialize_section_patterns(self):\n        self.section_hierarchy = {\n            'abstract': {'level': 1, 'patterns': [[{\"LOWER\": {\"REGEX\": r\"^(abstract|summary)$\"}}]]},\n            'introduction': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"introduction\", \"intro\"]}}]]},\n            'methods': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"methods\", \"methodology\"]}}]]},\n            'results': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"results\", \"findings\"]}}]]},\n            'discussion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"discussion\", \"analysis\"]}}]]},\n            'conclusion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"conclusion\", \"summary\"]}}]]},\n            'references': {'level': 1, 'patterns': [[{\"LOWER\": \"references\"}]]}\n        }\n\n    def _refresh_matcher(self):\n        self.matcher = Matcher(self.nlp.vocab)\n        for section, info in self.section_hierarchy.items():\n            for pattern in info['patterns']:\n                self.matcher.add(section.upper(), [pattern])\n\n    def process_document(self, text: str) -> OrderedDict:\n        doc = self.nlp(text)\n        matches = sorted(self.matcher(doc), key=lambda x: x[1])\n        sections = OrderedDict()\n        current_section = \"header\"\n        last_end = 0\n\n        for match_id, start, end in matches:\n            section_name = self.nlp.vocab.strings[match_id].lower()\n            content = doc[last_end:start].text.strip()\n            if content:\n                if current_section not in sections:\n                    sections[current_section] = []\n                sections[current_section].append(content)\n            current_section = section_name\n            last_end = end\n\n        if last_end < len(doc):\n            if current_section not in sections:\n                sections[current_section] = []\n            sections[current_section].append(doc[last_end:].text.strip())\n            \n        return self._postprocess_sections(sections)\n\n    def _postprocess_sections(self, raw_sections: Dict) -> OrderedDict:\n        processed = OrderedDict()\n        previous_level = 0\n        \n        for section, content_list in raw_sections.items():\n            content = \"\\n\".join(content_list)\n            current_level = self.section_hierarchy.get(section.lower(), {}).get('level', 1)\n            \n            if current_level > previous_level:\n                processed[section] = content\n                previous_level = current_level\n            else:\n                if processed:\n                    last_section = next(reversed(processed))\n                    processed[last_section] += \"\\n\\n\" + content\n                    \n        return processed\n\nclass PaperProcessor:\n    def __init__(self, detector):\n        self.detector = detector\n        self.session = self._create_session()\n\n    def _create_session(self):\n        session = requests.Session()\n        retries = Retry(\n            total=5,\n            backoff_factor=1,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"GET\"]\n        )\n        \n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive'\n        }\n        \n        session.mount('https://', HTTPAdapter(max_retries=retries))\n        session.mount('http://', HTTPAdapter(max_retries=retries))\n        session.headers.update(headers)\n        return session\n\n    def process_paper(self, url):\n        text = self._get_paper_text(url)\n        return self.detector.process_document(text) if text else None\n\n    def _get_paper_text(self, url):\n        try:\n            time.sleep(2)  # Increased delay for rate limiting\n            response = self.session.get(url, timeout=60, stream=True)\n            response.raise_for_status()\n            \n            content_type = response.headers.get('Content-Type', '')\n            if 'application/pdf' not in content_type and 'octet-stream' not in content_type:\n                print(f\"URL {url} doesn't return a PDF (Content-Type: {content_type})\")\n                return None\n                \n            with BytesIO() as pdf_buffer:\n                for chunk in response.iter_content(chunk_size=8192):\n                    pdf_buffer.write(chunk)\n                pdf_buffer.seek(0)\n                \n                try:\n                    with pdfplumber.open(pdf_buffer) as pdf:\n                        return \"\\n\".join(page.extract_text() or '' for page in pdf.pages)\n                except pdfplumber.PDFSyntaxError:\n                    print(f\"PDF parsing failed for {url}\")\n                    return None\n                    \n        except requests.exceptions.RequestException as e:\n            print(f\"Error processing {url}: {str(e)}\")\n            return None\n        except Exception as e:\n            print(f\"Unexpected error processing {url}: {str(e)}\")\n            return None\n\ndef setup_environment():\n    import os\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n    torch.backends.cudnn.benchmark = True\n\ndef load_model(model_name=\"gpt2\"):\n    compute_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=torch.cuda.is_available(),\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    ) if torch.cuda.is_available() else None\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=compute_dtype\n    )\n    return model\n\ndef train_model(model, tokenizer, dataset_path):\n    df = pd.read_csv(dataset_path)\n    dataset = Dataset.from_pandas(df[['text']])\n\n    def tokenize_fn(examples):\n        return tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=512,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    peft_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        fan_in_fan_out=True\n    )\n\n    # REMOVED length_penalty from TrainingArguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=1e-5,\n        weight_decay=0.01,\n        fp16=torch.cuda.is_available(),\n        logging_steps=10,\n        optim=\"adamw_torch\",\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        save_strategy=\"steps\",\n        save_steps=200,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        report_to=\"none\"\n    )\n\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    model.config.use_cache = False\n    model.print_trainable_parameters()\n\n    dataset = dataset.train_test_split(test_size=0.1)\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        data_collator=data_collator\n    )\n    \n    trainer.train()\n    return model\n\ndef save_model(model, tokenizer, output_dir):\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Model saved to {output_dir}\")\n\ndef load_for_inference(model_path):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    \n    # Create pipeline with better default generation config\n    return pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n    \ndef generate_text(pipeline, prompt, max_length=200):\n    structured_prompt = (\n        f\"Question: {prompt}\\n\\n\"\n        \"Answer concisely and technically accurate:\\n\"\n    )\n    \n    # Simplified generation config without beam-search parameters\n    generation_config = {\n        \"max_length\": max_length,\n        \"do_sample\": True,\n        \"temperature\": 0.7,\n        \"top_p\": 0.9,\n        \"repetition_penalty\": 1.5,\n        \"num_return_sequences\": 1,\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"no_repeat_ngram_size\": 3,\n        # Removed early_stopping and diversity_penalty which require num_beams>1\n    }\n    \n    try:\n        output = pipeline(structured_prompt, **generation_config)[0]['generated_text']\n        return post_process(output)\n    except Exception as e:\n        print(f\"Generation error: {str(e)}\")\n        # Fallback to simpler generation if the above fails\n        return pipeline(structured_prompt, max_length=max_length)[0]['generated_text']\n\ndef post_process(text):\n    # Remove consecutive duplicate lines\n    lines = text.split('\\n')\n    cleaned = [lines[0]]\n    for line in lines[1:]:\n        if line != cleaned[-1]:\n            cleaned.append(line)\n    return '\\n'.join(cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T17:24:48.660735Z","iopub.execute_input":"2025-05-28T17:24:48.661456Z","iopub.status.idle":"2025-05-28T17:24:48.694640Z","shell.execute_reply.started":"2025-05-28T17:24:48.661422Z","shell.execute_reply":"2025-05-28T17:24:48.693604Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    setup_environment()\n\n    # 1. Paper processing with multiple fallback URLs\n    print(\"=== Starting Paper Processing ===\")\n    detector = SectionDetector()\n    processor = PaperProcessor(detector)\n    \n    paper_urls = [\n        \"https://arxiv.org/pdf/2307.12874\",\n        \"https://arxiv.org/pdf/2303.12940\",\n        \"https://arxiv.org/pdf/1802.04351\",\n        \"https://arxiv.org/pdf/2306.08168\",\n        \"https://arxiv.org/pdf/2503.15964\",\n        \"https://www.jetir.org/papers/JETIR2405D82.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/subWallet-Blockchain-2019.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/Hossein-TrustCom-2020.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://dl.gi.de/server/api/core/bitstreams/aaa640a1-f8dd-4514-ad72-b809932072cc/content\",\n        \"https://eprint.iacr.org/2023/062.pdf\",\n        \"https://eprint.iacr.org/2022/075.pdf\",    \n        \"https://eprint.iacr.org/2023/1234.pdf\",\n        \"https://eprint.iacr.org/2020/300.pdf\",\n        \"https://eprint.iacr.org/2023/312.pdf\",\n        \"https://policyreview.info/pdf/policyreview-2016-3-427.pdf\",\n        \"https://eprint.iacr.org/2016/013.pdf\",\n        \"https://arxiv.org/pdf/1906.00245\",\n        \"https://escholarship.org/content/qt7fh678d6/qt7fh678d6.pdf?t=pn651y\",\n        \"https://re.public.polimi.it/bitstream/11311/1056221/6/11311-1056221%20Giudici.pdf\",\n        \"https://research-api.cbs.dk/ws/files/44436178/ole_bjerg_how_is_bitcoin_money_postprint.pdf\",\n        \"https://www.bis.org/fsi/publ/insights49.pdf\",\n        \"https://www.scirp.org/pdf/ojbm_1534496.pdf\",\n        \"https://www.bis.org/publ/work1066.pdf\",\n        \"http://khcnbinhduong.gov.vn/ImageUpload/file/TTTK%20KCN/2019/Nguon%20tin%20KHCN/Blockchain_A3.pdf\",\n        \"https://e-space.mmu.ac.uk/627269/1/Manuscript_Final%20JCLP.pdf\",\n        \"https://pdfs.semanticscholar.org/9900/c9c91f9f78fa0adb6915855084396654363c.pdf?_gl=1*7q1z9h*_gcl_au*MTkxMDg1NzA4NC4xNzQ4MDIxMDA4*_ga*Mjc1MDg5MDkuMTc0ODAyMTAwOA..*_ga_H7P4ZT52H5*czE3NDgwMjEwMDckbzEkZzEkdDE3NDgwMjExNzkkajE1JGwwJGgwJGR1YWNJOGg3VW43bWFscGZjZ056LU5TM0lXc0Jtc0drMW93\",\n        \"https://www.newyorkfed.org/medialibrary/media/research/epr/2024/EPR_2024_digital-assets_azar.pdf\",\n        \"https://journals.law.harvard.edu/hblr/wp-content/uploads/sites/87/2025/03/04_HLB_15_1_Noked171-216.pdf\",\n        \"https://www.stern.nyu.edu/sites/default/files/2024-07/Glucksman_Sak_2024.pdf\",\n        \"https://www.tigta.gov/sites/default/files/reports/2024-07/2024300030fr_0.pdf\",\n        \"https://www.fsb.org/uploads/Crypto-Council-for-Innovation.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://ndbf.nebraska.gov/sites/default/files/industries/Digital%20Asset%20Depository%20Nebraska%20Custody%20and%20Fiduciary%20Services%20Examination%20Manual.pdf\",\n        \"https://www.swlegal.com/media/filer_public/2d/f7/2df70b84-cb3c-4578-9943-8b3ea024abf9/sw_nl_january_2024_english.pdf\",\n        \"https://www.willkie.com/-/media/files/publications/2024/12/law360---sec-custody-rule-creates-crypto-compliance-conundrum.pdf\",\n        \"https://www.henrystewartpublications.com/sites/default/files/Opportunities%20in%20digital%20assets%20and%20digital%20custody-Tracking%20the%20modernisation%20of%20standard%20custody%20offering%20-%20Ignatowicz%20%26%20Taudes%20JSOC%2015-3.pdf\",\n        \"https://www.gdf.io/wp-content/uploads/2019/02/GDF-Crypto-Asset-Safekeeping_20-April-2019-2-cust-providers-additions-1-2.pdf\",\n        \"https://www.occ.gov/topics/charters-and-licensing/interpretations-and-actions/2020/int1170.pdf\",\n        \"https://www.gemini.com/static/documents/guide-to-crypto-custody.pdf\",\n        \"https://orbilu.uni.lu/bitstream/10993/62083/1/ZetzscheSinnigNikolakopoulou_Crypto%20custody_CMLJ%202024.pdf\",\n        \"https://www.esrb.europa.eu/pub/pdf/reports/esrb.cryptoassetsanddecentralisedfinance202305~9792140acd.en.pdf\",\n        \"https://repository.uel.ac.uk/download/df676586f4e9f8a89df529a36841d83d4750539805189a8951032ee4c2f0c16c/99798/challenges-and-approaches-to-regulating-decentralized-finance.pdf\",\n        \"https://repository.uel.ac.uk/download/ca8bad2f5fab17596c44927643b4da1473ef7ef79862fe3ca05ea9251bd4db8b/1599957/Financial%20Crime%20update%20%282020%29.pdf\",\n        \"https://www.iacpcybercenter.org/wp-content/uploads/2018/03/Bitcoin.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/Podcasts/SPT_Emerging-Tech-Terms.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-phishing.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018/Emerging_Tech_Bitcoin_Crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2019/emerging-tech_white-paper.pdf\",\n        \"https://openaccess.uoc.edu/bitstream/10609/151551/1/Rahmanikivi_cbt22_empirical.pdf\",\n        \"https://ics.uci.edu/~dabrowsa/dabrowski-defi21-hwwallet.pdf\",\n        \"https://fc19.ifca.ai/preproceedings/93-preproceedings.pdf\",\n        \"https://www.jkroll.com/papers/bitcoin_threshold_signatures.pdf\",\n        \"https://corporates.db.com/files/documents/publications/db-polygo-digital-id-wp-42pp-web-secured.pdf\",\n        \"https://www.napier.ac.uk/-/media/worktribe/output-2839021/smart-contract-attacks-and-protections.ashx\",\n        \"https://www.cyprusbarassociation.org/images/6._Crypto_Wallets.pdf\",\n        \"https://computerscience.unicam.it/marcantoni/tesi/Ethereum%20Smart%20Contracts%20Optimization.pdf\",\n        \"https://cspecc.utsa.edu/publications/files/Refereed_Papers/2020_Choo_BCPPA-blockchain-cond-priv-auth-prot.pdf\",\n        \"https://www.ekonomika.org.rs/sr/PDF/ekonomika/2019/clanci19-3/7.pdf\",\n        \"https://assets.cureusjournals.com/artifacts/upload/review_article/pdf/1099/20250319-214523-194a3z.pdf\"\n    ]\n    \n    processed_data = []\n    for url in paper_urls:\n        print(f\"\\nAttempting to process: {url}\")\n        sections = processor.process_paper(url)\n        if sections:\n            full_text = \"\\n\\n\".join(sections.values())\n            processed_data.append({\"text\": full_text})\n            print(f\"Successfully processed paper from {url}\")\n            break  # Stop after first successful download\n        else:\n            print(f\"Failed to process paper from {url}\")\n    \n    if not processed_data:\n        print(\"\\nError: Could not process any papers. Using sample data instead.\")\n        processed_data.append({\n            \"text\": \"Blockchain is a distributed ledger technology that enables secure transactions. Consensus mechanisms like Proof of Work and Proof of Stake validate transactions.\"\n        })\n\n    # 2. Data preparation\n    pd.DataFrame(processed_data).to_csv(\"processed_papers.csv\", index=False)\n    print(\"\\nSaved processed papers to processed_papers.csv\")\n\n    # 3. Model training with error handling\n    print(\"\\n=== Starting Model Training ===\")\n    try:\n        base_model = load_model()\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        \n        trained_model = train_model(base_model, tokenizer, \"processed_papers.csv\")\n        \n        # 4. Model persistence\n        save_model(trained_model, tokenizer, \"trained_model\")\n\n        # 5. Inference demonstration\n        print(\"\\n=== Testing Model Generation ===\")\n        gen_pipeline = load_for_inference(\"trained_model\")\n        test_prompts = [\n            \"Explain blockchain consensus mechanisms:\",\n            \"What are the benefits of zero-knowledge proofs?\",\n            \"Describe smart contract security considerations:\"\n        ]\n        \n        for prompt in test_prompts:\n            print(f\"\\nPrompt: {prompt}\")\n            response = generate_text(gen_pipeline, prompt)\n            print(\"Response:\", response.split(\"\\n\")[0])  # Show first line of response\n            \n    except Exception as e:\n        print(f\"\\nError during model training/inference: {str(e)}\")\n        print(\"Falling back to pretrained model for demonstration...\")\n        gen_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n        print(\"\\nSample generation with pretrained model:\")\n        print(generate_text(gen_pipeline, \"Explain blockchain:\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T17:24:48.695997Z","iopub.execute_input":"2025-05-28T17:24:48.696491Z","iopub.status.idle":"2025-05-28T17:25:21.601450Z","shell.execute_reply.started":"2025-05-28T17:24:48.696444Z","shell.execute_reply":"2025-05-28T17:25:21.600308Z"}},"outputs":[{"name":"stdout","text":"=== Starting Paper Processing ===\n\nAttempting to process: https://arxiv.org/pdf/2307.12874\nSuccessfully processed paper from https://arxiv.org/pdf/2307.12874\n\nSaved processed papers to processed_papers.csv\n\n=== Starting Model Training ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac58ee7335854130b0a3f30d820457d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241622ba8f054e76b1ac6d07cb900463"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aa09beef04c472a9136a67d8dae8656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a092af279e49bc95214a84adcbc6bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6515f6ff01b2445ead5eaad4b1aefabb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b7a30d8f5084fc9a154a6398182778f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b928c2f7564845a27605b93d3ee867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ce0643a5ac4f07968f063bc4ba9ec8"}},"metadata":{}},{"name":"stdout","text":"\nError during model training/inference: name 'TrainingArguments' is not defined\nFalling back to pretrained model for demonstration...\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\nSample generation with pretrained model:\nQuestion: Explain blockchain:\n\nAnswer concisely and technically accurate:\n- Why do we need a block? Blockchains are great for keeping track of transactions. However, when you have an entire economy in place to transact with each other (or more) it can take time that is needed if one needs some sort or another mechanism such as smart contract mining at all times on your network while the others simply don't exist yet! Therefore any system which has been built from scratch will provide even faster processing speed than Ethereum's own but still requires many hours per transaction before anyone else uses them - this could be done by making blocks so long they require lots less computing power compared their counterparties\n\n What does bitcoin use now vs Bitcoin 2 days ago? The main difference between two projects was its ability not only make payments easier through decentralized peer consensus within our society; there were also advantages over both technologies because without these benefits people would no longer pay fees due solely via fiat currencies like BTC/EUR\n","output_type":"stream"}],"execution_count":4}]}
