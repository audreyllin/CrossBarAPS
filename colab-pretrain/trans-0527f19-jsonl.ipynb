{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":241495234,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clean up existing installations\n!pip uninstall -y numpy torch transformers 2>/dev/null || echo \"Cleanup complete\"\n\n# Core package installation\n!pip install -q --upgrade pip\n!pip install -q \\\n    numpy==1.26.4 \\\n    pandas==2.2.2 \\\n    scipy==1.13.0 \\\n    scikit-learn==1.3.2 \\\n    spacy==3.7.4 \\\n    pdfplumber==0.11.0 \\\n    requests==2.31.0\n\n# PyTorch installation with CUDA 12.1 support\n!pip install -q \\\n    torch==2.2.1+cu121 \\\n    torchvision==0.17.1+cu121 \\\n    torchaudio==2.2.1+cu121 \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# NLP and ML ecosystem\n!pip install -q \\\n    transformers==4.41.2 \\\n    peft==0.10.0 \\\n    datasets==2.18.0 \\\n    accelerate==0.29.1 \\\n    bitsandbytes==0.43.0 \\\n    sentence-transformers==3.4.1\n\n# SpaCy model\n!python -m spacy download en_core_web_lg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T00:46:28.888918Z","iopub.execute_input":"2025-05-28T00:46:28.889158Z","iopub.status.idle":"2025-05-28T00:51:41.364765Z","shell.execute_reply.started":"2025-05-28T00:46:28.889135Z","shell.execute_reply":"2025-05-28T00:51:41.363270Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: transformers 4.51.3\nUninstalling transformers-4.51.3:\n  Successfully uninstalled transformers-4.51.3\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [spacy]m14/15\u001b[0m [spacy]]r.six]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torch, which is not installed.\ntorchdata 0.11.0 requires torch>=2, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\ntimm 1.0.15 requires torch, which is not installed.\npeft 0.14.0 requires torch>=1.13.0, which is not installed.\npeft 0.14.0 requires transformers, which is not installed.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\naccelerate 1.5.2 requires torch>=2.0.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.6.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [torchaudio]5\u001b[0m [torchaudio]]-cu12]12]2]2]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\npeft 0.14.0 requires transformers, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [peft][32m7/8\u001b[0m [peft]erate]s]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting en-core-web-lg==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.4.26)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.8)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\nInstalling collected packages: en-core-web-lg\nSuccessfully installed en-core-web-lg-3.7.1\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_lg')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nimport pdfplumber\nimport spacy\nimport requests\nimport pandas as pd\nimport torch\nimport time\nfrom io import BytesIO\nfrom typing import Dict, List\nfrom collections import OrderedDict\nfrom spacy.matcher import Matcher\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    pipeline,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\n\nclass SectionDetector:\n    def __init__(self, nlp=None):\n        self.nlp = nlp or spacy.load(\"en_core_web_lg\")\n        self._initialize_section_patterns()\n        self._refresh_matcher()\n\n    def _initialize_section_patterns(self):\n        self.section_hierarchy = {\n            'abstract': {'level': 1, 'patterns': [[{\"LOWER\": {\"REGEX\": r\"^(abstract|summary)$\"}}]]},\n            'introduction': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"introduction\", \"intro\"]}}]]},\n            'methods': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"methods\", \"methodology\"]}}]]},\n            'results': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"results\", \"findings\"]}}]]},\n            'discussion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"discussion\", \"analysis\"]}}]]},\n            'conclusion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"conclusion\", \"summary\"]}}]]},\n            'references': {'level': 1, 'patterns': [[{\"LOWER\": \"references\"}]]}\n        }\n\n    def _refresh_matcher(self):\n        self.matcher = Matcher(self.nlp.vocab)\n        for section, info in self.section_hierarchy.items():\n            for pattern in info['patterns']:\n                self.matcher.add(section.upper(), [pattern])\n\n    def process_document(self, text: str) -> OrderedDict:\n        doc = self.nlp(text)\n        matches = sorted(self.matcher(doc), key=lambda x: x[1])\n        sections = OrderedDict()\n        current_section = \"header\"\n        last_end = 0\n\n        for match_id, start, end in matches:\n            section_name = self.nlp.vocab.strings[match_id].lower()\n            content = doc[last_end:start].text.strip()\n            if content:\n                if current_section not in sections:\n                    sections[current_section] = []\n                sections[current_section].append(content)\n            current_section = section_name\n            last_end = end\n\n        if last_end < len(doc):\n            if current_section not in sections:\n                sections[current_section] = []\n            sections[current_section].append(doc[last_end:].text.strip())\n            \n        return self._postprocess_sections(sections)\n\n    def _postprocess_sections(self, raw_sections: Dict) -> OrderedDict:\n        processed = OrderedDict()\n        previous_level = 0\n        \n        for section, content_list in raw_sections.items():\n            content = \"\\n\".join(content_list)\n            current_level = self.section_hierarchy.get(section.lower(), {}).get('level', 1)\n            \n            if current_level > previous_level:\n                processed[section] = content\n                previous_level = current_level\n            else:\n                if processed:\n                    last_section = next(reversed(processed))\n                    processed[last_section] += \"\\n\\n\" + content\n                    \n        return processed\n\nclass PaperProcessor:\n    def __init__(self, detector):\n        self.detector = detector\n        self.session = self._create_session()\n\n    def _create_session(self):\n        session = requests.Session()\n        retries = Retry(\n            total=5,\n            backoff_factor=1,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"GET\"]\n        )\n        \n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive'\n        }\n        \n        session.mount('https://', HTTPAdapter(max_retries=retries))\n        session.mount('http://', HTTPAdapter(max_retries=retries))\n        session.headers.update(headers)\n        return session\n\n    def process_paper(self, url):\n        text = self._get_paper_text(url)\n        return self.detector.process_document(text) if text else None\n\n    def _get_paper_text(self, url):\n        try:\n            time.sleep(2)  # Increased delay for rate limiting\n            response = self.session.get(url, timeout=60, stream=True)\n            response.raise_for_status()\n            \n            content_type = response.headers.get('Content-Type', '')\n            if 'application/pdf' not in content_type and 'octet-stream' not in content_type:\n                print(f\"URL {url} doesn't return a PDF (Content-Type: {content_type})\")\n                return None\n                \n            with BytesIO() as pdf_buffer:\n                for chunk in response.iter_content(chunk_size=8192):\n                    pdf_buffer.write(chunk)\n                pdf_buffer.seek(0)\n                \n                try:\n                    with pdfplumber.open(pdf_buffer) as pdf:\n                        return \"\\n\".join(page.extract_text() or '' for page in pdf.pages)\n                except pdfplumber.PDFSyntaxError:\n                    print(f\"PDF parsing failed for {url}\")\n                    return None\n                    \n        except requests.exceptions.RequestException as e:\n            print(f\"Error processing {url}: {str(e)}\")\n            return None\n        except Exception as e:\n            print(f\"Unexpected error processing {url}: {str(e)}\")\n            return None\n\ndef setup_environment():\n    import os\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n    torch.backends.cudnn.benchmark = True\n\ndef load_model(model_name=\"gpt2\"):\n    compute_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=torch.cuda.is_available(),\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    ) if torch.cuda.is_available() else None\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=compute_dtype\n    )\n    return model\n\ndef train_model(model, tokenizer, dataset_path):\n    df = pd.read_csv(dataset_path)\n    dataset = Dataset.from_pandas(df[['text']])\n\n    def tokenize_fn(examples):\n        return tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=512,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    peft_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # GPT-2 specific modules\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        fan_in_fan_out=True  # Important for GPT-2\n    )\n\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,  # Increased from 1\n        learning_rate=1e-5,  # Reduced from 2e-5\n        weight_decay=0.01,  # Added to prevent overfitting\n        fp16=torch.cuda.is_available(),\n        logging_steps=10,\n        optim=\"adamw_torch\",\n        evaluation_strategy=\"steps\",  # Added validation\n        eval_steps=100,\n        save_strategy=\"steps\",\n        save_steps=200,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        report_to=\"none\",\n        repetition_penalty=1.2,  # Added to reduce repetition\n        length_penalty=1.0\n    )\n\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    model.config.use_cache = False\n    model.print_trainable_parameters()\n\n    # Split your dataset\n    dataset = dataset.train_test_split(test_size=0.1)\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],  # Added validation set\n        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n    )\n    \n    trainer.train()\n    return model\n\ndef save_model(model, tokenizer, output_dir):\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Model saved to {output_dir}\")\n\ndef load_for_inference(model_path):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\ndef generate_text(pipeline, prompt, max_length=200):\n    # Enhanced prompt template\n    structured_prompt = (\n        f\"Question: {prompt}\\n\\n\"\n        \"Answer concisely and technically accurate:\\n\"\n    )\n    \n    return pipeline(\n        structured_prompt,\n        max_length=max_length,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.5,  # Increased from 1.2\n        num_return_sequences=1,\n        pad_token_id=tokenizer.eos_token_id\n    )[0]['generated_text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T00:51:41.368562Z","iopub.execute_input":"2025-05-28T00:51:41.368974Z","iopub.status.idle":"2025-05-28T00:52:12.215866Z","shell.execute_reply.started":"2025-05-28T00:51:41.368932Z","shell.execute_reply":"2025-05-28T00:52:12.214768Z"}},"outputs":[{"name":"stderr","text":"2025-05-28 00:51:54.217001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748393514.522785      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748393514.602738      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    setup_environment()\n\n    # 1. Paper processing with multiple fallback URLs\n    print(\"=== Starting Paper Processing ===\")\n    detector = SectionDetector()\n    processor = PaperProcessor(detector)\n    \n    paper_urls = [\n        \"https://arxiv.org/pdf/2307.12874\",\n        \"https://arxiv.org/pdf/2303.12940\",\n        \"https://arxiv.org/pdf/1802.04351\",\n        \"https://arxiv.org/pdf/2306.08168\",\n        \"https://arxiv.org/pdf/2503.15964\",\n        \"https://www.jetir.org/papers/JETIR2405D82.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/subWallet-Blockchain-2019.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/Hossein-TrustCom-2020.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://dl.gi.de/server/api/core/bitstreams/aaa640a1-f8dd-4514-ad72-b809932072cc/content\",\n        \"https://eprint.iacr.org/2023/062.pdf\",\n        \"https://eprint.iacr.org/2022/075.pdf\",    \n        \"https://eprint.iacr.org/2023/1234.pdf\",\n        \"https://eprint.iacr.org/2020/300.pdf\",\n        \"https://eprint.iacr.org/2023/312.pdf\",\n        \"https://policyreview.info/pdf/policyreview-2016-3-427.pdf\",\n        \"https://eprint.iacr.org/2016/013.pdf\",\n        \"https://arxiv.org/pdf/1906.00245\",\n        \"https://escholarship.org/content/qt7fh678d6/qt7fh678d6.pdf?t=pn651y\",\n        \"https://re.public.polimi.it/bitstream/11311/1056221/6/11311-1056221%20Giudici.pdf\",\n        \"https://research-api.cbs.dk/ws/files/44436178/ole_bjerg_how_is_bitcoin_money_postprint.pdf\",\n        \"https://www.bis.org/fsi/publ/insights49.pdf\",\n        \"https://www.scirp.org/pdf/ojbm_1534496.pdf\",\n        \"https://www.bis.org/publ/work1066.pdf\",\n        \"http://khcnbinhduong.gov.vn/ImageUpload/file/TTTK%20KCN/2019/Nguon%20tin%20KHCN/Blockchain_A3.pdf\",\n        \"https://e-space.mmu.ac.uk/627269/1/Manuscript_Final%20JCLP.pdf\",\n        \"https://pdfs.semanticscholar.org/9900/c9c91f9f78fa0adb6915855084396654363c.pdf?_gl=1*7q1z9h*_gcl_au*MTkxMDg1NzA4NC4xNzQ4MDIxMDA4*_ga*Mjc1MDg5MDkuMTc0ODAyMTAwOA..*_ga_H7P4ZT52H5*czE3NDgwMjEwMDckbzEkZzEkdDE3NDgwMjExNzkkajE1JGwwJGgwJGR1YWNJOGg3VW43bWFscGZjZ056LU5TM0lXc0Jtc0drMW93\",\n        \"https://www.newyorkfed.org/medialibrary/media/research/epr/2024/EPR_2024_digital-assets_azar.pdf\",\n        \"https://journals.law.harvard.edu/hblr/wp-content/uploads/sites/87/2025/03/04_HLB_15_1_Noked171-216.pdf\",\n        \"https://www.stern.nyu.edu/sites/default/files/2024-07/Glucksman_Sak_2024.pdf\",\n        \"https://www.tigta.gov/sites/default/files/reports/2024-07/2024300030fr_0.pdf\",\n        \"https://www.fsb.org/uploads/Crypto-Council-for-Innovation.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://ndbf.nebraska.gov/sites/default/files/industries/Digital%20Asset%20Depository%20Nebraska%20Custody%20and%20Fiduciary%20Services%20Examination%20Manual.pdf\",\n        \"https://www.swlegal.com/media/filer_public/2d/f7/2df70b84-cb3c-4578-9943-8b3ea024abf9/sw_nl_january_2024_english.pdf\",\n        \"https://www.willkie.com/-/media/files/publications/2024/12/law360---sec-custody-rule-creates-crypto-compliance-conundrum.pdf\",\n        \"https://www.henrystewartpublications.com/sites/default/files/Opportunities%20in%20digital%20assets%20and%20digital%20custody-Tracking%20the%20modernisation%20of%20standard%20custody%20offering%20-%20Ignatowicz%20%26%20Taudes%20JSOC%2015-3.pdf\",\n        \"https://www.gdf.io/wp-content/uploads/2019/02/GDF-Crypto-Asset-Safekeeping_20-April-2019-2-cust-providers-additions-1-2.pdf\",\n        \"https://www.occ.gov/topics/charters-and-licensing/interpretations-and-actions/2020/int1170.pdf\",\n        \"https://www.gemini.com/static/documents/guide-to-crypto-custody.pdf\",\n        \"https://orbilu.uni.lu/bitstream/10993/62083/1/ZetzscheSinnigNikolakopoulou_Crypto%20custody_CMLJ%202024.pdf\",\n        \"https://www.esrb.europa.eu/pub/pdf/reports/esrb.cryptoassetsanddecentralisedfinance202305~9792140acd.en.pdf\",\n        \"https://repository.uel.ac.uk/download/df676586f4e9f8a89df529a36841d83d4750539805189a8951032ee4c2f0c16c/99798/challenges-and-approaches-to-regulating-decentralized-finance.pdf\",\n        \"https://repository.uel.ac.uk/download/ca8bad2f5fab17596c44927643b4da1473ef7ef79862fe3ca05ea9251bd4db8b/1599957/Financial%20Crime%20update%20%282020%29.pdf\",\n        \"https://www.iacpcybercenter.org/wp-content/uploads/2018/03/Bitcoin.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/Podcasts/SPT_Emerging-Tech-Terms.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-phishing.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018/Emerging_Tech_Bitcoin_Crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2019/emerging-tech_white-paper.pdf\",\n        \"https://openaccess.uoc.edu/bitstream/10609/151551/1/Rahmanikivi_cbt22_empirical.pdf\",\n        \"https://ics.uci.edu/~dabrowsa/dabrowski-defi21-hwwallet.pdf\",\n        \"https://fc19.ifca.ai/preproceedings/93-preproceedings.pdf\",\n        \"https://www.jkroll.com/papers/bitcoin_threshold_signatures.pdf\",\n        \"https://corporates.db.com/files/documents/publications/db-polygo-digital-id-wp-42pp-web-secured.pdf\",\n        \"https://www.napier.ac.uk/-/media/worktribe/output-2839021/smart-contract-attacks-and-protections.ashx\",\n        \"https://www.cyprusbarassociation.org/images/6._Crypto_Wallets.pdf\",\n        \"https://computerscience.unicam.it/marcantoni/tesi/Ethereum%20Smart%20Contracts%20Optimization.pdf\",\n        \"https://cspecc.utsa.edu/publications/files/Refereed_Papers/2020_Choo_BCPPA-blockchain-cond-priv-auth-prot.pdf\",\n        \"https://www.ekonomika.org.rs/sr/PDF/ekonomika/2019/clanci19-3/7.pdf\",\n        \"https://assets.cureusjournals.com/artifacts/upload/review_article/pdf/1099/20250319-214523-194a3z.pdf\"\n    ]\n    \n    processed_data = []\n    for url in paper_urls:\n        print(f\"\\nAttempting to process: {url}\")\n        sections = processor.process_paper(url)\n        if sections:\n            full_text = \"\\n\\n\".join(sections.values())\n            processed_data.append({\"text\": full_text})\n            print(f\"Successfully processed paper from {url}\")\n            break  # Stop after first successful download\n        else:\n            print(f\"Failed to process paper from {url}\")\n    \n    if not processed_data:\n        print(\"\\nError: Could not process any papers. Using sample data instead.\")\n        processed_data.append({\n            \"text\": \"Blockchain is a distributed ledger technology that enables secure transactions. Consensus mechanisms like Proof of Work and Proof of Stake validate transactions.\"\n        })\n\n    # 2. Data preparation\n    pd.DataFrame(processed_data).to_csv(\"processed_papers.csv\", index=False)\n    print(\"\\nSaved processed papers to processed_papers.csv\")\n\n    # 3. Model training with error handling\n    print(\"\\n=== Starting Model Training ===\")\n    try:\n        base_model = load_model()\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        \n        trained_model = train_model(base_model, tokenizer, \"processed_papers.csv\")\n        \n        # 4. Model persistence\n        save_model(trained_model, tokenizer, \"trained_model\")\n\n        # 5. Inference demonstration\n        print(\"\\n=== Testing Model Generation ===\")\n        gen_pipeline = load_for_inference(\"trained_model\")\n        test_prompts = [\n            \"Explain blockchain consensus mechanisms:\",\n            \"What are the benefits of zero-knowledge proofs?\",\n            \"Describe smart contract security considerations:\"\n        ]\n        \n        for prompt in test_prompts:\n            print(f\"\\nPrompt: {prompt}\")\n            response = generate_text(gen_pipeline, prompt)\n            print(\"Response:\", response.split(\"\\n\")[0])  # Show first line of response\n            \n    except Exception as e:\n        print(f\"\\nError during model training/inference: {str(e)}\")\n        print(\"Falling back to pretrained model for demonstration...\")\n        gen_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n        print(\"\\nSample generation with pretrained model:\")\n        print(generate_text(gen_pipeline, \"Explain blockchain:\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T00:52:12.216995Z","iopub.execute_input":"2025-05-28T00:52:12.217821Z","iopub.status.idle":"2025-05-28T00:52:45.836519Z","shell.execute_reply.started":"2025-05-28T00:52:12.217776Z","shell.execute_reply":"2025-05-28T00:52:45.835184Z"}},"outputs":[{"name":"stdout","text":"=== Starting Paper Processing ===\n\nAttempting to process: https://arxiv.org/pdf/2307.12874\nSuccessfully processed paper from https://arxiv.org/pdf/2307.12874\n\nSaved processed papers to processed_papers.csv\n\n=== Starting Model Training ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff3ac3e41493403ba6ba5f45981f439f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ab8297bc6264fa192e04c9ab0a0e767"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25af1b7c53514150908160182c548bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25632502ebba46398d04e318310104fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb1a37768dc460f95097f040410dae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d4713c87bf4509a4a9c5cbc811b578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313534f52cea4e1e949e96fc8168edea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d9627e6e2b4a4dbac2230d44efde3b"}},"metadata":{}},{"name":"stdout","text":"\nError during model training/inference: TrainingArguments.__init__() got an unexpected keyword argument 'repetition_penalty'\nFalling back to pretrained model for demonstration...\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\nSample generation with pretrained model:\nQuestion: Explain blockchain:\n\nAnswer concisely and technically accurate:\n. The key difference between a bitcoin, an Ethereum or Ripple is that each has its own set of cryptographic algorithms used to produce the digital asset (e-money). For example there are several different cryptocurrencies which use various blockchains in order for their transactions to be processed efficiently - ethereum's blocksize limit allows transaction processing within one second whereas bitcoins have two minutes; with more than half these systems being built on top as well! In this way they all come together quite seamlessly through distributed consensus mechanisms like Bitcoin Core & Dogecoin. As you can see from my recent article I've described some basic concepts about how money works using cryptography by analogy...I'm sure other people will find it helpful here if anyone wants clarification...so let me know what questions do your readers want answered so we could discuss them further :)\n\n\n","output_type":"stream"}],"execution_count":3}]}