{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":241495234,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clean up existing installations\n!pip uninstall -y numpy torch transformers 2>/dev/null || echo \"Cleanup complete\"\n\n# Core package installation\n!pip install -q --upgrade pip\n!pip install -q \\\n    numpy==1.26.4 \\\n    pandas==2.2.2 \\\n    scipy==1.13.0 \\\n    scikit-learn==1.3.2 \\\n    spacy==3.7.4 \\\n    pdfplumber==0.11.0 \\\n    requests==2.31.0\n\n# PyTorch installation with CUDA 12.1 support\n!pip install -q \\\n    torch==2.2.1+cu121 \\\n    torchvision==0.17.1+cu121 \\\n    torchaudio==2.2.1+cu121 \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# NLP and ML ecosystem\n!pip install -q \\\n    transformers==4.41.2 \\\n    peft==0.10.0 \\\n    datasets==2.18.0 \\\n    accelerate==0.29.1 \\\n    bitsandbytes==0.43.0 \\\n    sentence-transformers==3.4.1\n\n# SpaCy model\n!python -m spacy download en_core_web_lg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:41:12.840985Z","iopub.execute_input":"2025-05-27T23:41:12.841703Z","iopub.status.idle":"2025-05-27T23:42:22.095231Z","shell.execute_reply.started":"2025-05-27T23:41:12.841673Z","shell.execute_reply":"2025-05-27T23:42:22.094014Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1+cu121\nUninstalling torch-2.2.1+cu121:\n  Successfully uninstalled torch-2.2.1+cu121\nFound existing installation: transformers 4.41.2\nUninstalling transformers-4.41.2:\n  Successfully uninstalled transformers-4.41.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\nbitsandbytes 0.43.0 requires torch, which is not installed.\ntorchvision 0.17.1+cu121 requires torch==2.2.1, which is not installed.\npeft 0.10.0 requires torch>=1.13.0, which is not installed.\npeft 0.10.0 requires transformers, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\ntimm 1.0.15 requires torch, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npeft 0.10.0 requires transformers, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mCollecting en-core-web-lg==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.4.26)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.8)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_lg')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nimport pdfplumber\nimport spacy\nimport requests\nimport pandas as pd\nfrom io import BytesIO\nimport heapq\nimport time\nfrom typing import Dict, List\nfrom collections import OrderedDict, Counter\nfrom spacy.matcher import Matcher\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\n\nclass SectionDetector:\n    def __init__(self, nlp=None):\n        self.nlp = nlp or spacy.load(\"en_core_web_lg\")\n        self._initialize_section_patterns()\n        self._refresh_matcher()\n\n    def _initialize_section_patterns(self):\n        self.section_hierarchy = {\n            'abstract': {'level': 1, 'patterns': [[{\"LOWER\": {\"REGEX\": r\"^(abstract|summary)$\"}}]]},\n            'introduction': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"introduction\", \"intro\"]}}]]},\n            'methods': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"methods\", \"methodology\"]}}]]},\n            'results': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"results\", \"findings\"]}}]]},\n            'discussion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"discussion\", \"analysis\"]}}]]},\n            'conclusion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"conclusion\", \"summary\"]}}]]},\n            'references': {'level': 1, 'patterns': [[{\"LOWER\": \"references\"}]]}\n        }\n\n    def _refresh_matcher(self):\n        self.matcher = Matcher(self.nlp.vocab)\n        for section, info in self.section_hierarchy.items():\n            for pattern in info['patterns']:\n                self.matcher.add(section.upper(), [pattern])\n\n    def process_document(self, text: str) -> OrderedDict:\n        doc = self.nlp(text)\n        matches = sorted(self.matcher(doc), key=lambda x: x[1])\n        sections = OrderedDict()\n        current_section = \"header\"\n        last_end = 0\n\n        for match_id, start, end in matches:\n            section_name = self.nlp.vocab.strings[match_id].lower()\n            content = doc[last_end:start].text.strip()\n            if content:\n                if current_section not in sections:\n                    sections[current_section] = []\n                sections[current_section].append(content)\n            current_section = section_name\n            last_end = end\n\n        if last_end < len(doc):\n            if current_section not in sections:\n                sections[current_section] = []\n            sections[current_section].append(doc[last_end:].text.strip())\n            \n        return self._postprocess_sections(sections)\n\n    def _postprocess_sections(self, raw_sections: Dict) -> OrderedDict:\n        processed = OrderedDict()\n        previous_level = 0\n        \n        for section, content_list in raw_sections.items():\n            content = \"\\n\".join(content_list)\n            current_level = self.section_hierarchy.get(section.lower(), {}).get('level', 1)\n            \n            if current_level > previous_level:\n                processed[section] = content\n                previous_level = current_level\n            else:\n                if processed:\n                    last_section = next(reversed(processed))\n                    processed[last_section] += \"\\n\\n\" + content\n                    \n        return processed\n\nclass PaperProcessor:\n    def __init__(self, detector):\n        self.detector = detector\n        self.session = self._create_session()\n\n    def _create_session(self):\n        \"\"\"Create a resilient session with retries and headers\"\"\"\n        session = requests.Session()\n        \n        retries = Retry(\n            total=3,\n            backoff_factor=1,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"GET\"]\n        )\n        \n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'Connection': 'keep-alive'\n        }\n        \n        session.mount('https://', HTTPAdapter(max_retries=retries))\n        session.mount('http://', HTTPAdapter(max_retries=retries))\n        session.headers.update(headers)\n        \n        return session\n\n    def process_paper(self, url):\n        text = self._get_paper_text(url)\n        return self.detector.process_document(text) if text else None\n\n    def _get_paper_text(self, url):\n        \"\"\"Improved PDF download with error handling\"\"\"\n        try:\n            time.sleep(1)  # Rate limiting\n            \n            response = self.session.get(url, timeout=30, stream=True)\n            response.raise_for_status()\n            \n            if 'application/pdf' not in response.headers.get('Content-Type', ''):\n                print(f\"URL {url} doesn't return a PDF (Content-Type: {response.headers.get('Content-Type')})\")\n                return None\n                \n            with BytesIO() as pdf_buffer:\n                for chunk in response.iter_content(chunk_size=8192):\n                    pdf_buffer.write(chunk)\n                pdf_buffer.seek(0)\n                \n                try:\n                    with pdfplumber.open(pdf_buffer) as pdf:\n                        return \"\\n\".join(page.extract_text() or '' for page in pdf.pages)\n                except pdfplumber.PDFSyntaxError:\n                    print(f\"PDF parsing failed for {url} - file may be corrupted\")\n                    return None\n                    \n        except requests.exceptions.RequestException as e:\n            print(f\"Error processing {url}: {str(e)}\")\n            return None\n        except Exception as e:\n            print(f\"Unexpected error processing {url}: {str(e)}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:42:22.096854Z","iopub.execute_input":"2025-05-27T23:42:22.097152Z","iopub.status.idle":"2025-05-27T23:42:26.003173Z","shell.execute_reply.started":"2025-05-27T23:42:22.097121Z","shell.execute_reply":"2025-05-27T23:42:26.002295Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    pipeline\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\ndef setup_environment():\n    \"\"\"Configure system settings for optimal performance\"\"\"\n    import os\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n    torch.backends.cudnn.benchmark = True\n\ndef load_model(model_name=\"gpt2\"):\n    \"\"\"Load model with flexible quantization support\"\"\"\n    compute_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=torch.cuda.is_available(),\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    ) if torch.cuda.is_available() else None\n\n    return AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=compute_dtype\n    )\n\ndef train_model(model, tokenizer, dataset_path):\n    \"\"\"Complete training workflow\"\"\"\n    # Data preparation\n    from datasets import Dataset\n    df = pd.read_csv(dataset_path)\n    dataset = Dataset.from_pandas(df[['text']])\n\n    # Tokenization\n    def tokenize_fn(examples):\n        return tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=512,\n            padding=\"max_length\"\n        )\n\n    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n\n    # LoRA configuration\n    peft_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"attn.c_attn\", \"attn.c_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n\n    # Training setup\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-5,\n        fp16=torch.cuda.is_available(),\n        logging_steps=20,\n        optim=\"adamw_torch\",\n        report_to=\"none\"\n    )\n\n    # Model preparation\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n\n    # Training execution\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n    trainer.train()\n    return model\n\ndef save_model(model, tokenizer, output_dir):\n    \"\"\"Save model artifacts with proper formatting\"\"\"\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Model saved to {output_dir}\")\n\ndef load_for_inference(model_path):\n    \"\"\"Load trained model for generation\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\ndef generate_text(pipeline, prompt, max_length=200):\n    \"\"\"Generation with temperature sampling\"\"\"\n    return pipeline(\n        prompt,\n        max_length=max_length,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9\n    )[0]['generated_text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:42:26.004156Z","iopub.execute_input":"2025-05-27T23:42:26.004695Z","iopub.status.idle":"2025-05-27T23:42:32.775654Z","shell.execute_reply.started":"2025-05-27T23:42:26.004666Z","shell.execute_reply":"2025-05-27T23:42:32.774713Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 23:42:29.085711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748389349.113902     262 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748389349.122625     262 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Environment configuration\n    setup_environment()\n\n    # 1. Paper processing\n    detector = SectionDetector()\n    processor = PaperProcessor(detector)\n    \n    paper_urls = [\n        \"https://arxiv.org/pdf/2307.12874\",\n        \"https://arxiv.org/pdf/2303.12940\",\n        \"https://arxiv.org/pdf/1802.04351\",\n        \"https://arxiv.org/pdf/2306.08168\",\n        \"https://arxiv.org/pdf/2503.15964\",\n        \"https://www.jetir.org/papers/JETIR2405D82.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/subWallet-Blockchain-2019.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/Hossein-TrustCom-2020.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://dl.gi.de/server/api/core/bitstreams/aaa640a1-f8dd-4514-ad72-b809932072cc/content\",\n        \"https://eprint.iacr.org/2023/062.pdf\",\n        \"https://eprint.iacr.org/2022/075.pdf\",    \n        \"https://eprint.iacr.org/2023/1234.pdf\",\n        \"https://eprint.iacr.org/2020/300.pdf\",\n        \"https://eprint.iacr.org/2023/312.pdf\",\n        \"https://policyreview.info/pdf/policyreview-2016-3-427.pdf\",\n        \"https://eprint.iacr.org/2016/013.pdf\",\n        \"https://arxiv.org/pdf/1906.00245\",\n        \"https://escholarship.org/content/qt7fh678d6/qt7fh678d6.pdf?t=pn651y\",\n        \"https://re.public.polimi.it/bitstream/11311/1056221/6/11311-1056221%20Giudici.pdf\",\n        \"https://research-api.cbs.dk/ws/files/44436178/ole_bjerg_how_is_bitcoin_money_postprint.pdf\",\n        \"https://www.bis.org/fsi/publ/insights49.pdf\",\n        \"https://www.scirp.org/pdf/ojbm_1534496.pdf\",\n        \"https://www.bis.org/publ/work1066.pdf\",\n        \"http://khcnbinhduong.gov.vn/ImageUpload/file/TTTK%20KCN/2019/Nguon%20tin%20KHCN/Blockchain_A3.pdf\",\n        \"https://e-space.mmu.ac.uk/627269/1/Manuscript_Final%20JCLP.pdf\",\n        \"https://pdfs.semanticscholar.org/9900/c9c91f9f78fa0adb6915855084396654363c.pdf?_gl=1*7q1z9h*_gcl_au*MTkxMDg1NzA4NC4xNzQ4MDIxMDA4*_ga*Mjc1MDg5MDkuMTc0ODAyMTAwOA..*_ga_H7P4ZT52H5*czE3NDgwMjEwMDckbzEkZzEkdDE3NDgwMjExNzkkajE1JGwwJGgwJGR1YWNJOGg3VW43bWFscGZjZ056LU5TM0lXc0Jtc0drMW93\",\n        \"https://www.newyorkfed.org/medialibrary/media/research/epr/2024/EPR_2024_digital-assets_azar.pdf\",\n        \"https://journals.law.harvard.edu/hblr/wp-content/uploads/sites/87/2025/03/04_HLB_15_1_Noked171-216.pdf\",\n        \"https://www.stern.nyu.edu/sites/default/files/2024-07/Glucksman_Sak_2024.pdf\",\n        \"https://www.tigta.gov/sites/default/files/reports/2024-07/2024300030fr_0.pdf\",\n        \"https://www.fsb.org/uploads/Crypto-Council-for-Innovation.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://ndbf.nebraska.gov/sites/default/files/industries/Digital%20Asset%20Depository%20Nebraska%20Custody%20and%20Fiduciary%20Services%20Examination%20Manual.pdf\",\n        \"https://www.swlegal.com/media/filer_public/2d/f7/2df70b84-cb3c-4578-9943-8b3ea024abf9/sw_nl_january_2024_english.pdf\",\n        \"https://www.willkie.com/-/media/files/publications/2024/12/law360---sec-custody-rule-creates-crypto-compliance-conundrum.pdf\",\n        \"https://www.henrystewartpublications.com/sites/default/files/Opportunities%20in%20digital%20assets%20and%20digital%20custody-Tracking%20the%20modernisation%20of%20standard%20custody%20offering%20-%20Ignatowicz%20%26%20Taudes%20JSOC%2015-3.pdf\",\n        \"https://www.gdf.io/wp-content/uploads/2019/02/GDF-Crypto-Asset-Safekeeping_20-April-2019-2-cust-providers-additions-1-2.pdf\",\n        \"https://www.occ.gov/topics/charters-and-licensing/interpretations-and-actions/2020/int1170.pdf\",\n        \"https://www.gemini.com/static/documents/guide-to-crypto-custody.pdf\",\n        \"https://orbilu.uni.lu/bitstream/10993/62083/1/ZetzscheSinnigNikolakopoulou_Crypto%20custody_CMLJ%202024.pdf\",\n        \"https://www.esrb.europa.eu/pub/pdf/reports/esrb.cryptoassetsanddecentralisedfinance202305~9792140acd.en.pdf\",\n        \"https://repository.uel.ac.uk/download/df676586f4e9f8a89df529a36841d83d4750539805189a8951032ee4c2f0c16c/99798/challenges-and-approaches-to-regulating-decentralized-finance.pdf\",\n        \"https://repository.uel.ac.uk/download/ca8bad2f5fab17596c44927643b4da1473ef7ef79862fe3ca05ea9251bd4db8b/1599957/Financial%20Crime%20update%20%282020%29.pdf\",\n        \"https://www.iacpcybercenter.org/wp-content/uploads/2018/03/Bitcoin.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/Podcasts/SPT_Emerging-Tech-Terms.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-phishing.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018/Emerging_Tech_Bitcoin_Crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2019/emerging-tech_white-paper.pdf\",\n        \"https://openaccess.uoc.edu/bitstream/10609/151551/1/Rahmanikivi_cbt22_empirical.pdf\",\n        \"https://ics.uci.edu/~dabrowsa/dabrowski-defi21-hwwallet.pdf\",\n        \"https://fc19.ifca.ai/preproceedings/93-preproceedings.pdf\",\n        \"https://www.jkroll.com/papers/bitcoin_threshold_signatures.pdf\",\n        \"https://corporates.db.com/files/documents/publications/db-polygo-digital-id-wp-42pp-web-secured.pdf\",\n        \"https://www.napier.ac.uk/-/media/worktribe/output-2839021/smart-contract-attacks-and-protections.ashx\",\n        \"https://www.cyprusbarassociation.org/images/6._Crypto_Wallets.pdf\",\n        \"https://computerscience.unicam.it/marcantoni/tesi/Ethereum%20Smart%20Contracts%20Optimization.pdf\",\n        \"https://cspecc.utsa.edu/publications/files/Refereed_Papers/2020_Choo_BCPPA-blockchain-cond-priv-auth-prot.pdf\",\n        \"https://www.ekonomika.org.rs/sr/PDF/ekonomika/2019/clanci19-3/7.pdf\",\n        \"https://assets.cureusjournals.com/artifacts/upload/review_article/pdf/1099/20250319-214523-194a3z.pdf\"\n    ]\n    \n    processed_data = []\n    for url in paper_urls:\n        if sections := processor.process_paper(url):\n            # Join all section contents into one text\n            full_text = \"\\n\\n\".join(sections.values())\n            processed_data.append({\"text\": full_text})\n\n    # 2. Data preparation\n    pd.DataFrame(processed_data).to_csv(\"processed_papers.csv\", index=False)\n\n    # 3. Model training\n    base_model = load_model()\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    trained_model = train_model(base_model, tokenizer, \"processed_papers.csv\")\n    \n    # 4. Model persistence\n    save_model(trained_model, tokenizer, \"trained_model\")\n\n    # 5. Inference demonstration\n    gen_pipeline = load_for_inference(\"trained_model\")\n    test_prompts = [\n        \"Explain blockchain consensus mechanisms:\",\n        \"What are the benefits of zero-knowledge proofs?\"\n    ]\n    \n    for prompt in test_prompts:\n        print(f\"\\nPrompt: {prompt}\")\n        print(\"Response:\", generate_text(gen_pipeline, prompt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:42:32.777126Z","iopub.execute_input":"2025-05-27T23:42:32.777801Z","iopub.status.idle":"2025-05-27T23:51:49.827469Z","shell.execute_reply.started":"2025-05-27T23:42:32.777773Z","shell.execute_reply":"2025-05-27T23:51:49.826005Z"}},"outputs":[{"name":"stdout","text":"Error processing https://www.scirp.org/pdf/ojbm_1534496.pdf: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943c0641b6b546678a9c093ddd841a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49aa868314194d4a949b550de4863839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddbf0e407ac4f8f9834c3e4a5d7f88f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e430ddaf5e0e4881960171dc0a1de92b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e08a8c5847b6434f86a204b80c7b2784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c6fb30d1dd4baeb423e2d6ee85e91e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ce8d8e930542579023610406388ff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7725940af7ce4408ada04bb10ee2d908"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 125,324,544 || trainable%: 0.7059558900130528\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_262/4152953227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"processed_papers.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# 4. Model persistence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_262/1707253830.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, dataset_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3238\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3282\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   3283\u001b[0m                     \u001b[0;34m\"The model did not return a loss from the inputs, only the following keys: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3284\u001b[0m                     \u001b[0;34mf\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."],"ename":"ValueError","evalue":"The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.","output_type":"error"}],"execution_count":4}]}