{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":241495234,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clean up existing installations\n!pip uninstall -y numpy torch transformers 2>/dev/null || echo \"Cleanup complete\"\n\n# Core package installation\n!pip install -q --upgrade pip\n!pip install -q \\\n    numpy==1.26.4 \\\n    pandas==2.2.2 \\\n    scipy==1.13.0 \\\n    scikit-learn==1.3.2 \\\n    spacy==3.7.4 \\\n    pdfplumber==0.11.0 \\\n    requests==2.31.0\n\n# PyTorch installation with CUDA 12.1 support\n!pip install -q \\\n    torch==2.2.1+cu121 \\\n    torchvision==0.17.1+cu121 \\\n    torchaudio==2.2.1+cu121 \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# NLP and ML ecosystem\n!pip install -q \\\n    transformers==4.41.2 \\\n    peft==0.10.0 \\\n    datasets==2.18.0 \\\n    accelerate==0.29.1 \\\n    bitsandbytes==0.43.0 \\\n    sentence-transformers==3.4.1\n\n# SpaCy model\n!python -m spacy download en_core_web_lg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T00:02:53.406072Z","iopub.execute_input":"2025-05-28T00:02:53.406777Z","iopub.status.idle":"2025-05-28T00:04:31.778148Z","shell.execute_reply.started":"2025-05-28T00:02:53.406741Z","shell.execute_reply":"2025-05-28T00:04:31.776666Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1+cu121\nUninstalling torch-2.2.1+cu121:\n  Successfully uninstalled torch-2.2.1+cu121\nFound existing installation: transformers 4.41.2\nUninstalling transformers-4.41.2:\n  Successfully uninstalled transformers-4.41.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\nbitsandbytes 0.43.0 requires torch, which is not installed.\ntorchvision 0.17.1+cu121 requires torch==2.2.1, which is not installed.\npeft 0.10.0 requires torch>=1.13.0, which is not installed.\npeft 0.10.0 requires transformers, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\ntimm 1.0.15 requires torch, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npeft 0.10.0 requires transformers, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mCollecting en-core-web-lg==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.4.26)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.8)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_lg')\n\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import re\nimport pdfplumber\nimport spacy\nimport requests\nimport pandas as pd\nimport torch\nimport time\nfrom io import BytesIO\nfrom typing import Dict, List\nfrom collections import OrderedDict\nfrom spacy.matcher import Matcher\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    pipeline,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\n\nclass SectionDetector:\n    def __init__(self, nlp=None):\n        self.nlp = nlp or spacy.load(\"en_core_web_lg\")\n        self._initialize_section_patterns()\n        self._refresh_matcher()\n\n    def _initialize_section_patterns(self):\n        self.section_hierarchy = {\n            'abstract': {'level': 1, 'patterns': [[{\"LOWER\": {\"REGEX\": r\"^(abstract|summary)$\"}}]]},\n            'introduction': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"introduction\", \"intro\"]}}]]},\n            'methods': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"methods\", \"methodology\"]}}]]},\n            'results': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"results\", \"findings\"]}}]]},\n            'discussion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"discussion\", \"analysis\"]}}]]},\n            'conclusion': {'level': 1, 'patterns': [[{\"LOWER\": {\"IN\": [\"conclusion\", \"summary\"]}}]]},\n            'references': {'level': 1, 'patterns': [[{\"LOWER\": \"references\"}]]}\n        }\n\n    def _refresh_matcher(self):\n        self.matcher = Matcher(self.nlp.vocab)\n        for section, info in self.section_hierarchy.items():\n            for pattern in info['patterns']:\n                self.matcher.add(section.upper(), [pattern])\n\n    def process_document(self, text: str) -> OrderedDict:\n        doc = self.nlp(text)\n        matches = sorted(self.matcher(doc), key=lambda x: x[1])\n        sections = OrderedDict()\n        current_section = \"header\"\n        last_end = 0\n\n        for match_id, start, end in matches:\n            section_name = self.nlp.vocab.strings[match_id].lower()\n            content = doc[last_end:start].text.strip()\n            if content:\n                if current_section not in sections:\n                    sections[current_section] = []\n                sections[current_section].append(content)\n            current_section = section_name\n            last_end = end\n\n        if last_end < len(doc):\n            if current_section not in sections:\n                sections[current_section] = []\n            sections[current_section].append(doc[last_end:].text.strip())\n            \n        return self._postprocess_sections(sections)\n\n    def _postprocess_sections(self, raw_sections: Dict) -> OrderedDict:\n        processed = OrderedDict()\n        previous_level = 0\n        \n        for section, content_list in raw_sections.items():\n            content = \"\\n\".join(content_list)\n            current_level = self.section_hierarchy.get(section.lower(), {}).get('level', 1)\n            \n            if current_level > previous_level:\n                processed[section] = content\n                previous_level = current_level\n            else:\n                if processed:\n                    last_section = next(reversed(processed))\n                    processed[last_section] += \"\\n\\n\" + content\n                    \n        return processed\n\nclass PaperProcessor:\n    def __init__(self, detector):\n        self.detector = detector\n        self.session = self._create_session()\n\n    def _create_session(self):\n        session = requests.Session()\n        retries = Retry(\n            total=5,\n            backoff_factor=1,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"GET\"]\n        )\n        \n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive'\n        }\n        \n        session.mount('https://', HTTPAdapter(max_retries=retries))\n        session.mount('http://', HTTPAdapter(max_retries=retries))\n        session.headers.update(headers)\n        return session\n\n    def process_paper(self, url):\n        text = self._get_paper_text(url)\n        return self.detector.process_document(text) if text else None\n\n    def _get_paper_text(self, url):\n        try:\n            time.sleep(2)  # Increased delay for rate limiting\n            response = self.session.get(url, timeout=60, stream=True)\n            response.raise_for_status()\n            \n            content_type = response.headers.get('Content-Type', '')\n            if 'application/pdf' not in content_type and 'octet-stream' not in content_type:\n                print(f\"URL {url} doesn't return a PDF (Content-Type: {content_type})\")\n                return None\n                \n            with BytesIO() as pdf_buffer:\n                for chunk in response.iter_content(chunk_size=8192):\n                    pdf_buffer.write(chunk)\n                pdf_buffer.seek(0)\n                \n                try:\n                    with pdfplumber.open(pdf_buffer) as pdf:\n                        return \"\\n\".join(page.extract_text() or '' for page in pdf.pages)\n                except pdfplumber.PDFSyntaxError:\n                    print(f\"PDF parsing failed for {url}\")\n                    return None\n                    \n        except requests.exceptions.RequestException as e:\n            print(f\"Error processing {url}: {str(e)}\")\n            return None\n        except Exception as e:\n            print(f\"Unexpected error processing {url}: {str(e)}\")\n            return None\n\ndef setup_environment():\n    import os\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n    torch.backends.cudnn.benchmark = True\n\ndef load_model(model_name=\"gpt2\"):\n    compute_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=torch.cuda.is_available(),\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n    ) if torch.cuda.is_available() else None\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=compute_dtype\n    )\n    return model\n\ndef train_model(model, tokenizer, dataset_path):\n    df = pd.read_csv(dataset_path)\n    dataset = Dataset.from_pandas(df[['text']])\n\n    def tokenize_fn(examples):\n        return tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=512,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    peft_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,  # Reduced for demo\n        learning_rate=2e-5,\n        fp16=torch.cuda.is_available(),\n        logging_steps=10,\n        optim=\"adamw_torch\",\n        report_to=\"none\",\n        save_strategy=\"no\",\n        evaluation_strategy=\"no\"\n    )\n\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    model.config.use_cache = False\n    model.print_trainable_parameters()\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=data_collator\n    )\n    trainer.train()\n    return model\n\ndef save_model(model, tokenizer, output_dir):\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Model saved to {output_dir}\")\n\ndef load_for_inference(model_path):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\ndef generate_text(pipeline, prompt, max_length=200):\n    return pipeline(\n        prompt,\n        max_length=max_length,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9\n    )[0]['generated_text']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T00:04:31.781315Z","iopub.execute_input":"2025-05-28T00:04:31.781736Z","iopub.status.idle":"2025-05-28T00:04:31.821675Z","shell.execute_reply.started":"2025-05-28T00:04:31.781700Z","shell.execute_reply":"2025-05-28T00:04:31.820746Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    setup_environment()\n\n    # 1. Paper processing with multiple fallback URLs\n    print(\"=== Starting Paper Processing ===\")\n    detector = SectionDetector()\n    processor = PaperProcessor(detector)\n    \n    paper_urls = [\n        \"https://arxiv.org/pdf/2307.12874\",\n        \"https://arxiv.org/pdf/2303.12940\",\n        \"https://arxiv.org/pdf/1802.04351\",\n        \"https://arxiv.org/pdf/2306.08168\",\n        \"https://arxiv.org/pdf/2503.15964\",\n        \"https://www.jetir.org/papers/JETIR2405D82.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/subWallet-Blockchain-2019.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/Hossein-TrustCom-2020.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://dl.gi.de/server/api/core/bitstreams/aaa640a1-f8dd-4514-ad72-b809932072cc/content\",\n        \"https://eprint.iacr.org/2023/062.pdf\",\n        \"https://eprint.iacr.org/2022/075.pdf\",    \n        \"https://eprint.iacr.org/2023/1234.pdf\",\n        \"https://eprint.iacr.org/2020/300.pdf\",\n        \"https://eprint.iacr.org/2023/312.pdf\",\n        \"https://policyreview.info/pdf/policyreview-2016-3-427.pdf\",\n        \"https://eprint.iacr.org/2016/013.pdf\",\n        \"https://arxiv.org/pdf/1906.00245\",\n        \"https://escholarship.org/content/qt7fh678d6/qt7fh678d6.pdf?t=pn651y\",\n        \"https://re.public.polimi.it/bitstream/11311/1056221/6/11311-1056221%20Giudici.pdf\",\n        \"https://research-api.cbs.dk/ws/files/44436178/ole_bjerg_how_is_bitcoin_money_postprint.pdf\",\n        \"https://www.bis.org/fsi/publ/insights49.pdf\",\n        \"https://www.scirp.org/pdf/ojbm_1534496.pdf\",\n        \"https://www.bis.org/publ/work1066.pdf\",\n        \"http://khcnbinhduong.gov.vn/ImageUpload/file/TTTK%20KCN/2019/Nguon%20tin%20KHCN/Blockchain_A3.pdf\",\n        \"https://e-space.mmu.ac.uk/627269/1/Manuscript_Final%20JCLP.pdf\",\n        \"https://pdfs.semanticscholar.org/9900/c9c91f9f78fa0adb6915855084396654363c.pdf?_gl=1*7q1z9h*_gcl_au*MTkxMDg1NzA4NC4xNzQ4MDIxMDA4*_ga*Mjc1MDg5MDkuMTc0ODAyMTAwOA..*_ga_H7P4ZT52H5*czE3NDgwMjEwMDckbzEkZzEkdDE3NDgwMjExNzkkajE1JGwwJGgwJGR1YWNJOGg3VW43bWFscGZjZ056LU5TM0lXc0Jtc0drMW93\",\n        \"https://www.newyorkfed.org/medialibrary/media/research/epr/2024/EPR_2024_digital-assets_azar.pdf\",\n        \"https://journals.law.harvard.edu/hblr/wp-content/uploads/sites/87/2025/03/04_HLB_15_1_Noked171-216.pdf\",\n        \"https://www.stern.nyu.edu/sites/default/files/2024-07/Glucksman_Sak_2024.pdf\",\n        \"https://www.tigta.gov/sites/default/files/reports/2024-07/2024300030fr_0.pdf\",\n        \"https://www.fsb.org/uploads/Crypto-Council-for-Innovation.pdf\",\n        \"https://www.cs.ucf.edu/~czou/research/HosseinDissertation-2020.pdf\",\n        \"https://ndbf.nebraska.gov/sites/default/files/industries/Digital%20Asset%20Depository%20Nebraska%20Custody%20and%20Fiduciary%20Services%20Examination%20Manual.pdf\",\n        \"https://www.swlegal.com/media/filer_public/2d/f7/2df70b84-cb3c-4578-9943-8b3ea024abf9/sw_nl_january_2024_english.pdf\",\n        \"https://www.willkie.com/-/media/files/publications/2024/12/law360---sec-custody-rule-creates-crypto-compliance-conundrum.pdf\",\n        \"https://www.henrystewartpublications.com/sites/default/files/Opportunities%20in%20digital%20assets%20and%20digital%20custody-Tracking%20the%20modernisation%20of%20standard%20custody%20offering%20-%20Ignatowicz%20%26%20Taudes%20JSOC%2015-3.pdf\",\n        \"https://www.gdf.io/wp-content/uploads/2019/02/GDF-Crypto-Asset-Safekeeping_20-April-2019-2-cust-providers-additions-1-2.pdf\",\n        \"https://www.occ.gov/topics/charters-and-licensing/interpretations-and-actions/2020/int1170.pdf\",\n        \"https://www.gemini.com/static/documents/guide-to-crypto-custody.pdf\",\n        \"https://orbilu.uni.lu/bitstream/10993/62083/1/ZetzscheSinnigNikolakopoulou_Crypto%20custody_CMLJ%202024.pdf\",\n        \"https://www.esrb.europa.eu/pub/pdf/reports/esrb.cryptoassetsanddecentralisedfinance202305~9792140acd.en.pdf\",\n        \"https://repository.uel.ac.uk/download/df676586f4e9f8a89df529a36841d83d4750539805189a8951032ee4c2f0c16c/99798/challenges-and-approaches-to-regulating-decentralized-finance.pdf\",\n        \"https://repository.uel.ac.uk/download/ca8bad2f5fab17596c44927643b4da1473ef7ef79862fe3ca05ea9251bd4db8b/1599957/Financial%20Crime%20update%20%282020%29.pdf\",\n        \"https://www.iacpcybercenter.org/wp-content/uploads/2018/03/Bitcoin.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/Podcasts/SPT_Emerging-Tech-Terms.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018-materials/emerging-tech_glossary-phishing.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018/Emerging_Tech_Bitcoin_Crypto.pdf\",\n        \"https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2019/emerging-tech_white-paper.pdf\",\n        \"https://openaccess.uoc.edu/bitstream/10609/151551/1/Rahmanikivi_cbt22_empirical.pdf\",\n        \"https://ics.uci.edu/~dabrowsa/dabrowski-defi21-hwwallet.pdf\",\n        \"https://fc19.ifca.ai/preproceedings/93-preproceedings.pdf\",\n        \"https://www.jkroll.com/papers/bitcoin_threshold_signatures.pdf\",\n        \"https://corporates.db.com/files/documents/publications/db-polygo-digital-id-wp-42pp-web-secured.pdf\",\n        \"https://www.napier.ac.uk/-/media/worktribe/output-2839021/smart-contract-attacks-and-protections.ashx\",\n        \"https://www.cyprusbarassociation.org/images/6._Crypto_Wallets.pdf\",\n        \"https://computerscience.unicam.it/marcantoni/tesi/Ethereum%20Smart%20Contracts%20Optimization.pdf\",\n        \"https://cspecc.utsa.edu/publications/files/Refereed_Papers/2020_Choo_BCPPA-blockchain-cond-priv-auth-prot.pdf\",\n        \"https://www.ekonomika.org.rs/sr/PDF/ekonomika/2019/clanci19-3/7.pdf\",\n        \"https://assets.cureusjournals.com/artifacts/upload/review_article/pdf/1099/20250319-214523-194a3z.pdf\"\n    ]\n    \n    processed_data = []\n    for url in paper_urls:\n        print(f\"\\nAttempting to process: {url}\")\n        sections = processor.process_paper(url)\n        if sections:\n            full_text = \"\\n\\n\".join(sections.values())\n            processed_data.append({\"text\": full_text})\n            print(f\"Successfully processed paper from {url}\")\n            break  # Stop after first successful download\n        else:\n            print(f\"Failed to process paper from {url}\")\n    \n    if not processed_data:\n        print(\"\\nError: Could not process any papers. Using sample data instead.\")\n        processed_data.append({\n            \"text\": \"Blockchain is a distributed ledger technology that enables secure transactions. Consensus mechanisms like Proof of Work and Proof of Stake validate transactions.\"\n        })\n\n    # 2. Data preparation\n    pd.DataFrame(processed_data).to_csv(\"processed_papers.csv\", index=False)\n    print(\"\\nSaved processed papers to processed_papers.csv\")\n\n    # 3. Model training with error handling\n    print(\"\\n=== Starting Model Training ===\")\n    try:\n        base_model = load_model()\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n        \n        trained_model = train_model(base_model, tokenizer, \"processed_papers.csv\")\n        \n        # 4. Model persistence\n        save_model(trained_model, tokenizer, \"trained_model\")\n\n        # 5. Inference demonstration\n        print(\"\\n=== Testing Model Generation ===\")\n        gen_pipeline = load_for_inference(\"trained_model\")\n        test_prompts = [\n            \"Explain blockchain consensus mechanisms:\",\n            \"What are the benefits of zero-knowledge proofs?\",\n            \"Describe smart contract security considerations:\"\n        ]\n        \n        for prompt in test_prompts:\n            print(f\"\\nPrompt: {prompt}\")\n            response = generate_text(gen_pipeline, prompt)\n            print(\"Response:\", response.split(\"\\n\")[0])  # Show first line of response\n            \n    except Exception as e:\n        print(f\"\\nError during model training/inference: {str(e)}\")\n        print(\"Falling back to pretrained model for demonstration...\")\n        gen_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n        print(\"\\nSample generation with pretrained model:\")\n        print(generate_text(gen_pipeline, \"Explain blockchain:\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T00:04:31.822729Z","iopub.execute_input":"2025-05-28T00:04:31.823003Z","iopub.status.idle":"2025-05-28T00:05:00.079986Z","shell.execute_reply.started":"2025-05-28T00:04:31.822981Z","shell.execute_reply":"2025-05-28T00:05:00.078877Z"}},"outputs":[{"name":"stdout","text":"=== Starting Paper Processing ===\n\nAttempting to process: https://arxiv.org/pdf/2307.12874\nSuccessfully processed paper from https://arxiv.org/pdf/2307.12874\n\nSaved processed papers to processed_papers.csv\n\n=== Starting Model Training ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebcb0162d14643afbe7ec72e42be3983"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nError during model training/inference: Target modules {'v_proj', 'q_proj'} not found in the base model. Please check the target modules and try again.\nFalling back to pretrained model for demonstration...\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nSample generation with pretrained model:\nExplain blockchain: how to use it\n\nThis is a list of the core blockchain features.\n\n1. Transactions\n\nThe blockchain will send and receive transactions between your node and the rest of the network. You can view this list by clicking the \"View\" button.\n\n2. Smart Contracts\n\nSmart Contracts are a new way of creating contracts for your nodes. These contracts will be executed by the nodes that created them.\n\n3. Decentralized Autonomous Organizations\n\nThis is a new feature that allows you to create decentralized autonomous organizations, such as a bank or an insurance company. This is a smart contract that you can use to create a smart contract.\n\n4. Blockchain Technology\n\nThis is a new feature that allows you to build a smart contract that uses blockchain technology. This is a smart contract that you can use to build a smart contract.\n\n5. Ethereum and Smart Contracts\n\nThis is a new feature that allows you\n","output_type":"stream"}],"execution_count":7}]}