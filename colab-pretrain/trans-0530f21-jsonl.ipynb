{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12000428,"sourceType":"datasetVersion","datasetId":7548900},{"sourceId":12000455,"sourceType":"datasetVersion","datasetId":7548921},{"sourceId":12009799,"sourceType":"datasetVersion","datasetId":7555612},{"sourceId":12010409,"sourceType":"datasetVersion","datasetId":7556008}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:28:43.155337Z","iopub.execute_input":"2025-05-30T21:28:43.155989Z","iopub.status.idle":"2025-05-30T21:28:43.166397Z","shell.execute_reply.started":"2025-05-30T21:28:43.155949Z","shell.execute_reply":"2025-05-30T21:28:43.165068Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Complete Environment Setup\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Remove problematic directories manually\nproblematic_path = \"/usr/local/lib/python3.11/dist-packages/~vidia-cudnn-cu12\"\n\n\n# Clear pip cache\n!pip cache purge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:28:43.168064Z","iopub.execute_input":"2025-05-30T21:28:43.168387Z","iopub.status.idle":"2025-05-30T21:28:47.700325Z","shell.execute_reply.started":"2025-05-30T21:28:43.168361Z","shell.execute_reply":"2025-05-30T21:28:47.699139Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1+cu121\nUninstalling torch-2.2.1+cu121:\n  Successfully uninstalled torch-2.2.1+cu121\nFound existing installation: torchvision 0.17.1+cu121\nUninstalling torchvision-0.17.1+cu121:\n  Successfully uninstalled torchvision-0.17.1+cu121\nFound existing installation: torchaudio 2.2.1+cu121\nUninstalling torchaudio-2.2.1+cu121:\n  Successfully uninstalled torchaudio-2.2.1+cu121\nFound existing installation: transformers 4.41.2\nUninstalling transformers-4.41.2:\n  Successfully uninstalled transformers-4.41.2\nFound existing installation: peft 0.10.0\nUninstalling peft-0.10.0:\n  Successfully uninstalled peft-0.10.0\nFound existing installation: bitsandbytes 0.43.0\nUninstalling bitsandbytes-0.43.0:\n  Successfully uninstalled bitsandbytes-0.43.0\nFiles removed: 90\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install NumPy FIRST with clean environment\n!pip install -q --ignore-installed numpy==1.26.4\n\n# Install PyTorch with CUDA 12.1 (Kaggle's version)\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# Install transformer-related packages with compatible versions\n!pip install -q transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install -q bitsandbytes==0.43.0 einops==0.7.0\n\n# Handle gymnasium separately to avoid conflicts\n!pip install -q gymnasium==0.29.0 --no-deps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:28:47.701753Z","iopub.execute_input":"2025-05-30T21:28:47.702064Z","iopub.status.idle":"2025-05-30T21:30:10.249353Z","shell.execute_reply.started":"2025-05-30T21:28:47.702033Z","shell.execute_reply":"2025-05-30T21:30:10.248043Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sys\nimport os\nimport json\nimport numpy as np\nimport psutil\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    pipeline,\n    GenerationConfig\n)\nfrom sentence_transformers import util as semantic_util","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:10.252707Z","iopub.execute_input":"2025-05-30T21:30:10.253023Z","iopub.status.idle":"2025-05-30T21:30:21.543431Z","shell.execute_reply.started":"2025-05-30T21:30:10.252994Z","shell.execute_reply":"2025-05-30T21:30:21.542229Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"},{"name":"stderr","text":"2025-05-30 21:30:17.549415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748640617.586711     212 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748640617.597883     212 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Model Loading\n# =====================\nMODEL_NAME = \"gpt2\"  # Change to \"meta-llama/Llama-2-7b-chat-hf\" for Llama\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics for the environment\"\"\"\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    \"\"\"Load model with improved error handling and phi-1.5 specific settings\"\"\"\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    # Phi-1.5 specific configuration\n    trust_remote_code = True  # Required for phi-1.5\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    # Quantization config for memory efficiency\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch_dtype\n    )\n    \n    try:\n        print(\"Attempting quantized load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=trust_remote_code,\n            device_map=\"auto\",\n            torch_dtype=torch_dtype\n        )\n        \n        print(\"\\n✅ Model loaded successfully!\")\n        print_memory()\n        return model\n        \n    except Exception as e:\n        print(f\"\\n❌ Model loading failed: {str(e)}\")\n        print(\"Attempting standard load without quantization...\")\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                torch_dtype=torch_dtype\n            )\n            print(\"\\n✅ Model loaded successfully without quantization!\")\n            print_memory()\n            return model\n        except Exception as e:\n            print(f\"\\n❌ Standard load failed: {str(e)}\")\n            print(\"Attempting CPU-only fallback...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"cpu\",\n                torch_dtype=torch.float32\n            )\n            print(\"\\n✅ Model loaded on CPU\")\n            print_memory()\n            return model\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:21.544905Z","iopub.execute_input":"2025-05-30T21:30:21.545965Z","iopub.status.idle":"2025-05-30T21:30:22.876979Z","shell.execute_reply.started":"2025-05-30T21:30:21.545904Z","shell.execute_reply":"2025-05-30T21:30:22.875699Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 5.7% (1.3/31.4GB)\nAttempting quantized load...\n\n❌ Model loading failed: No GPU found. A GPU is needed for quantization.\nAttempting standard load without quantization...\n\n✅ Model loaded successfully without quantization!\nRAM: 7.2% (1.8/31.4GB)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Tokenizer Setup\n# =====================\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:22.878161Z","iopub.execute_input":"2025-05-30T21:30:22.878549Z","iopub.status.idle":"2025-05-30T21:30:23.197839Z","shell.execute_reply.started":"2025-05-30T21:30:22.878494Z","shell.execute_reply":"2025-05-30T21:30:23.196414Z"}},"outputs":[{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Data Preparation\n# =====================\ndef prepare_dataset(file_path=\"/kaggle/input/database\", max_samples=1000):\n    \"\"\"Prepare dataset with robust error handling\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"❌ Path not found: {file_path}\")\n            \n        # Load JSONL file directly using datasets library\n        dataset = load_dataset('json', data_files=file_path, split='train[:{}]'.format(max_samples))\n        \n        # If the dataset has multiple columns, we just want the text\n        if 'text' not in dataset.features:\n            # Try to find the first text-like column\n            text_columns = [col for col in dataset.features if any(t in col.lower() for t in ['text', 'content', 'body'])]\n            if text_columns:\n                dataset = dataset.rename_column(text_columns[0], 'text')\n            else:\n                # If no text column found, combine all string columns\n                string_columns = [col for col in dataset.features if dataset.features[col].dtype == 'string']\n                if string_columns:\n                    def combine_columns(examples):\n                        return {'text': ' '.join(str(examples[col]) for col in string_columns)}\n                    dataset = dataset.map(combine_columns)\n                else:\n                    raise ValueError(\"No text columns found in dataset\")\n        \n        print(f\"✅ Loaded dataset with {len(dataset)} samples\")\n        return dataset\n        \n    except Exception as e:\n        print(f\"\\n❌ Dataset preparation failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\"Sample text \" + str(i) for i in range(10)]})\n\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with explicit numpy workarounds\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=90,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        # Convert to lists explicitly\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except RuntimeError as e:\n        if \"Numpy is not available\" in str(e):\n            # Fallback using pure Python\n            return {\n                \"input_ids\": [[0]*512],\n                \"attention_mask\": [[1]*512],\n                \"labels\": [[0]*512]\n            }\n        raise\n\ntry:\n    print(\"\\n=== Starting Processing ===\")\n    dataset = prepare_dataset()\n    \n    # Small batch test first\n    test_batch = dataset.select(range(2))\n    test_tokenized = test_batch.map(safe_tokenize, batched=True)\n    \n    # If test succeeds, process full dataset\n    tokenized_dataset = dataset.map(safe_tokenize, batched=True, batch_size=4)\n    tokenized_dataset.set_format(type='torch')\n    \n    print(\"✅ Processing completed successfully!\")\n    \nexcept Exception as e:\n    print(f\"\\n❌ Error: {str(e)}\")\n    print(\"Creating minimal fallback dataset...\")\n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": [[0,1,2,3]],\n        \"attention_mask\": [[1,1,1,1]],\n        \"labels\": [[0,1,2,3]]\n    })\n    tokenized_dataset.set_format(type='torch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.198814Z","iopub.execute_input":"2025-05-30T21:30:23.199173Z","iopub.status.idle":"2025-05-30T21:30:23.638022Z","shell.execute_reply.started":"2025-05-30T21:30:23.199146Z","shell.execute_reply":"2025-05-30T21:30:23.636999Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Processing ===\n\n❌ Dataset preparation failed: Unable to find '/kaggle/input/database'\nCreating minimal fallback dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742bcdf16fd447f18db14c1ae66596d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe7591f91052449aacbd7fe580f63cee"}},"metadata":{}},{"name":"stdout","text":"✅ Processing completed successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Training Configuration\n# =====================\ndef configure_training(model):\n    \"\"\"Configure training parameters and LoRA setup\"\"\"\n    # Enable gradient checkpointing to save memory\n    model.gradient_checkpointing_enable()\n\n    # LoRA configuration\n    peft_config = LoraConfig(\n        r=16,  \n        lora_alpha=32,\n        target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # GPT-2 compatible modules\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        fan_in_fan_out=True\n    )\n\n    # Training arguments optimized for Kaggle\n    training_args = TrainingArguments(\n        output_dir=\"/kaggle/working/phi1.5-lora-results\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,  # Reduced for Kaggle\n        learning_rate=2e-5,\n        optim=\"adamw_torch\",\n        logging_steps=10,\n        save_steps=500,\n        fp16=torch.cuda.is_available(),\n        max_grad_norm=0.3,\n        warmup_ratio=0.1,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"none\"\n    )\n\n    # Prepare model for training\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n\n    # Print trainable parameters\n    model.print_trainable_parameters()\n    \n    return model, training_args","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.639112Z","iopub.execute_input":"2025-05-30T21:30:23.639494Z","iopub.status.idle":"2025-05-30T21:30:23.646709Z","shell.execute_reply.started":"2025-05-30T21:30:23.639441Z","shell.execute_reply":"2025-05-30T21:30:23.645581Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Training Execution\n# =====================\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n                             'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n                             'labels': torch.stack([f['input_ids'] for f in data])}\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ndef generate_contrastive_examples(example):\n    \"\"\"Generate contrastive examples for training\"\"\"\n    # Generate negative sample by:\n    # 1. Random Q/A from different category\n    # 2. GPT-generated incorrect answer\n    # 3. Perturbed correct answer\n    return {\n        'anchor': example['answer'],\n        'positive': augment_answer(example['answer']),\n        'negative': get_negative_sample(example)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.647848Z","iopub.execute_input":"2025-05-30T21:30:23.648222Z","iopub.status.idle":"2025-05-30T21:30:23.672669Z","shell.execute_reply.started":"2025-05-30T21:30:23.648190Z","shell.execute_reply":"2025-05-30T21:30:23.671433Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Model Saving\n# =====================\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"💽 Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n⚠️ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.675474Z","iopub.execute_input":"2025-05-30T21:30:23.675835Z","iopub.status.idle":"2025-05-30T21:30:23.696142Z","shell.execute_reply.started":"2025-05-30T21:30:23.675808Z","shell.execute_reply":"2025-05-30T21:30:23.695020Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Model Loading and Testing\n# =====================\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 250,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline - REMOVED device parameter since we're using device_map=\"auto\"\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"💬 Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.697485Z","iopub.execute_input":"2025-05-30T21:30:23.697969Z","iopub.status.idle":"2025-05-30T21:30:23.730858Z","shell.execute_reply.started":"2025-05-30T21:30:23.697930Z","shell.execute_reply":"2025-05-30T21:30:23.729851Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Enhanced Model Wrapper\n# =====================\nclass ConstrainedCryptoModel(nn.Module):\n    def __init__(self, base_model, tokenizer, knowledge_base=None):\n        super().__init__()\n        self.model = base_model\n        self.tokenizer = tokenizer\n        self.knowledge_base = knowledge_base\n        \n        # Technical term dictionary for crypto concepts\n        self.technical_terms = {\n            'PoW': ['consensus', 'mining', 'difficulty', 'hashrate'],\n            'blockchain': ['ledger', 'immutable', 'blocks', 'decentralized'],\n            'cryptography': ['encryption', 'keys', 'security', 'algorithm'],\n            'wallet': ['private key', 'public key', 'address', 'seed phrase']\n        }\n        \n        # Banned phrases and sequences\n        self.banned_sequences = [\n            \"I don't know\", \"as an AI\", \"I'm not sure\", \n            \"I can't answer\", \"my training data\"\n        ]\n        \n        # Required technical terms\n        self.force_included = [\n            \"blockchain\", \"cryptography\", \"decentralized\",\n            \"consensus\", \"encryption\", \"keys\"\n        ]\n\n    def forward(self, input_ids, **kwargs):\n        outputs = self.model(input_ids, **kwargs)\n        outputs.logits = self.apply_crypto_constraints(outputs.logits, input_ids)\n        return outputs\n    \n    def apply_crypto_constraints(self, logits, input_ids):\n        \"\"\"Apply multi-dimensional constraints for crypto-specific generation\"\"\"\n        generated_text = self.tokenizer.decode(input_ids[0])\n        \n        # 1. Force inclusion of key technical terms\n        for term in self.force_included:\n            if term in generated_text.lower():\n                term_ids = self.tokenizer.encode(term, add_special_tokens=False)\n                for tid in term_ids:\n                    logits[0, -1, tid] += 5.0\n        \n        # 2. Ban non-technical or uncertain phrases\n        for phrase in self.banned_sequences:\n            phrase_ids = self.tokenizer.encode(phrase, add_special_tokens=False)\n            if len(phrase_ids) > 0:\n                logits[0, -1, phrase_ids[0]] = -float('inf')\n        \n        # 3. Validate technical consistency\n        last_term = self.get_last_term(generated_text)\n        if last_term in self.technical_terms:\n            for term in self.technical_terms[last_term]:\n                term_ids = self.tokenizer.encode(term, add_special_tokens=False)\n                for tid in term_ids:\n                    logits[0, -1, tid] += 3.0\n        \n        return logits\n    \n    def get_last_term(self, text):\n        \"\"\"Extract the main technical term from the query\"\"\"\n        question_words = [\"what\", \"how\", \"explain\", \"describe\"]\n        words = [w for w in text.lower().split() if w not in question_words]\n        return words[-1].strip('?') if words else \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.732077Z","iopub.execute_input":"2025-05-30T21:30:23.732424Z","iopub.status.idle":"2025-05-30T21:30:23.756428Z","shell.execute_reply.started":"2025-05-30T21:30:23.732389Z","shell.execute_reply":"2025-05-30T21:30:23.755355Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Enhanced Generation\n# =====================\nCRYPTO_GENERATION_CONFIG = GenerationConfig(\n    max_new_tokens=150,\n    no_repeat_ngram_size=4,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=40,\n    repetition_penalty=1.15,\n    num_beams=3,\n    early_stopping=True\n)\n\ndef generate_with_validation(model, tokenizer, prompt, max_length=200):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # First pass generation\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        generation_config=CRYPTO_GENERATION_CONFIG\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Validation checks\n    validation_passed = True\n    validation_notes = []\n    \n    # 1. Technical term check\n    last_term = model.get_last_term(prompt)\n    if last_term in model.technical_terms:\n        missing = [t for t in model.technical_terms[last_term] \n                  if t.lower() not in response.lower()]\n        if missing:\n            validation_passed = False\n            validation_notes.append(f\"Missing technical terms: {missing}\")\n    \n    # 2. Hallucination check\n    if any(phrase in response for phrase in model.banned_sequences):\n        validation_passed = False\n        validation_notes.append(\"Potential hallucination\")\n    \n    # Generate final output\n    if not validation_passed:\n        print(f\"⚠️ Validation issues: {validation_notes}\")\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            generation_config=CRYPTO_GENERATION_CONFIG,\n            bad_words_ids=[[tid] for tid in tokenizer.encode(\" \".join(model.banned_sequences), add_special_tokens=False)]\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return {\n        'response': response,\n        'validation_passed': validation_passed,\n        'validation_notes': validation_notes\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.757561Z","iopub.execute_input":"2025-05-30T21:30:23.757939Z","iopub.status.idle":"2025-05-30T21:30:23.780696Z","shell.execute_reply.started":"2025-05-30T21:30:23.757906Z","shell.execute_reply":"2025-05-30T21:30:23.779408Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# =====================\n# Main Training Flow (Fixed Version)\n# =====================\nif __name__ == \"__main__\":\n    # Initialize with memory check\n    print(\"=== Initializing Training Process ===\")\n    print_memory()\n    \n    # 1. Load model and tokenizer with error handling\n    try:\n        print(\"\\n=== Loading Model and Tokenizer ===\")\n        model = load_model(MODEL_NAME)\n        tokenizer = load_tokenizer(MODEL_NAME)\n        print(\"✅ Model and tokenizer loaded successfully\")\n    except Exception as e:\n        print(f\"❌ Failed to load model/tokenizer: {str(e)}\")\n        raise\n\n    # 2. Dataset preparation with robust error handling\n    try:\n        print(\"\\n=== Preparing Dataset ===\")\n        dataset = prepare_dataset()\n        \n        # Small validation batch first\n        print(\"Running validation on small batch...\")\n        test_batch = dataset.select(range(min(2, len(dataset))))\n        test_tokenized = test_batch.map(safe_tokenize, batched=True)\n        \n        # Full dataset processing\n        print(\"Processing full dataset...\")\n        tokenized_dataset = dataset.map(\n            safe_tokenize,\n            batched=True,\n            batch_size=4,\n            remove_columns=dataset.column_names\n        )\n        tokenized_dataset.set_format(type='torch')\n        print(f\"✅ Dataset prepared with {len(tokenized_dataset)} samples\")\n        print_memory()\n        \n    except Exception as e:\n        print(f\"\\n❌ Dataset processing failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        tokenized_dataset = Dataset.from_dict({\n            \"input_ids\": [[0,1,2,3]],\n            \"attention_mask\": [[1,1,1,1]],\n            \"labels\": [[0,1,2,3]]\n        })\n        tokenized_dataset.set_format(type='torch')\n\n    # 3. Training configuration\n    try:\n        print(\"\\n=== Configuring Training ===\")\n        model, training_args = configure_training(model)\n        print(\"✅ Training configured with parameters:\")\n        print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n        print(f\"- Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n        print(f\"- Epochs: {training_args.num_train_epochs}\")\n        print(f\"- Learning rate: {training_args.learning_rate}\")\n        print_memory()\n    except Exception as e:\n        print(f\"❌ Training configuration failed: {str(e)}\")\n        raise\n\n    # 4. Training execution\n    try:\n        print(\"\\n=== Starting Training ===\")\n        trainer = train_model(model, tokenized_dataset, training_args)\n        print(\"✅ Training completed successfully!\")\n        print_memory()\n    except Exception as e:\n        print(f\"❌ Training failed: {str(e)}\")\n        raise\n\n    # 5. Model saving\n    try:\n        print(\"\\n=== Saving Model Artifacts ===\")\n        model_path = save_model_artifacts(model, tokenizer, training_args)\n        print(f\"✅ Model saved to: {model_path}\")\n    except Exception as e:\n        print(f\"❌ Model saving failed: {str(e)}\")\n        raise\n\n    # 6. Model testing and validation\n    try:\n        print(\"\\n=== Testing Model ===\")\n        # Test prompts covering key crypto concepts\n        custom_prompts = [\n            \"What is software wallet, and what's the difference between hardware and software wallet?\",\n            \"Explain Proof of Work (PoW) in simple terms\",\n            \"Describe blockchain technology in 2-3 sentences\",\n            \"What is the cryptographic purpose of public/private key pairs?\",\n            \"How does peer-to-peer (P2P) networking relate to cryptocurrency?\"\n        ]\n        \n        # Load and test the saved model\n        loaded_model, loaded_tokenizer = load_and_test_model(\n            model_path, \n            test_prompts=custom_prompts, \n            is_peft_model=True\n        )\n        \n        # Fix: Ensure model has device attribute before validation\n        if not hasattr(loaded_model, 'device'):\n            loaded_model.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Generate and validate responses\n        print(\"\\n=== Generating Validated Responses ===\")\n        for i, query in enumerate(custom_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {query}\")\n            inputs = loaded_tokenizer(query, return_tensors=\"pt\").to(loaded_model.device)\n            outputs = loaded_model.generate(**inputs, max_length=200)\n            response = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Comprehensive prompt quality assessment\n            print(\"\\n=== Prompt Quality Assessment ===\")\n            print(f\"1. Clarity: {'✅' if len(query.split()) <= 15 else '⚠️'} (Length: {len(query.split())} words)\")\n            print(f\"2. Specificity: {'✅' if '?' in query else '⚠️'} (Contains explicit question)\")\n            print(f\"3. Technical Focus: {'✅' if any(term.lower() in query.lower() for term in ['wallet', 'PoW', 'blockchain', 'crypt', 'P2P']) else '⚠️'}\")\n            print(f\"4. Avoids Ambiguity: {'✅' if not any(word in query for word in ['it', 'this', 'that']) else '⚠️'}\")\n            print(f\"5. Answer Format Hint: {'✅' if any(phrase in query for phrase in ['explain', 'describe', 'what is']) else '⚠️'}\")\n            \n            print(\"\\n💬 Generated Response:\")\n            print(response)\n            \n            # Technical validation\n            validation_passed = True\n            validation_notes = []\n            \n            # Check for technical terms\n            required_terms = {\n                'wallet': ['private key', 'public key', 'address', 'security'],\n                'PoW': ['consensus', 'mining', 'difficulty', 'hash'],\n                'blockchain': ['ledger', 'immutable', 'blocks', 'decentralized'],\n                'key': ['encryption', 'signature', 'asymmetric', 'cryptography'],\n                'P2P': ['network', 'nodes', 'decentralized', 'direct']\n            }\n            \n            for term, keywords in required_terms.items():\n                if term.lower() in query.lower():\n                    missing = [kw for kw in keywords if kw.lower() not in response.lower()]\n                    if missing:\n                        validation_passed = False\n                        validation_notes.append(f\"Missing technical terms for {term}: {missing}\")\n            \n            # Check for hallucinations\n            banned_phrases = [\"I don't know\", \"as an AI\", \"I'm not sure\", \"I can't answer\"]\n            if any(phrase in response for phrase in banned_phrases):\n                validation_passed = False\n                validation_notes.append(\"Contains uncertain/hallucinated content\")\n            \n            print(f\"\\n✅ Validation: {'PASSED' if validation_passed else 'FAILED'}\")\n            if not validation_passed:\n                print(f\"⚠️ Issues: {', '.join(validation_notes)}\")\n        \n        print(\"\\n=== Training and Evaluation Complete ===\")\n        \n    except Exception as e:\n        print(f\"❌ Model testing failed: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:35:42.694479Z","iopub.execute_input":"2025-05-30T21:35:42.695776Z","iopub.status.idle":"2025-05-30T21:37:23.326333Z","shell.execute_reply.started":"2025-05-30T21:35:42.695737Z","shell.execute_reply":"2025-05-30T21:37:23.324455Z"}},"outputs":[{"name":"stdout","text":"=== Initializing Training Process ===\nRAM: 9.4% (2.5/31.4GB)\n\n=== Loading Model and Tokenizer ===\n\n=== Loading Model: gpt2 ===\nRAM: 9.4% (2.5/31.4GB)\nAttempting quantized load...\n\n❌ Model loading failed: No GPU found. A GPU is needed for quantization.\nAttempting standard load without quantization...\n\n✅ Model loaded successfully without quantization!\nRAM: 10.8% (2.9/31.4GB)\nTokenizer loaded successfully\n✅ Model and tokenizer loaded successfully\n\n=== Preparing Dataset ===\n\n❌ Dataset preparation failed: Unable to find '/kaggle/input/database'\nCreating minimal fallback dataset...\nRunning validation on small batch...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe27d61e6e74b239f4add112910d5c6"}},"metadata":{}},{"name":"stdout","text":"Processing full dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7279fd2a77ae4dc7a60c8872e92fde92"}},"metadata":{}},{"name":"stdout","text":"✅ Dataset prepared with 10 samples\nRAM: 10.9% (3.0/31.4GB)\n\n=== Configuring Training ===\ntrainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635\n✅ Training configured with parameters:\n- Batch size: 1\n- Gradient accumulation: 4\n- Epochs: 1\n- Learning rate: 2e-05\nRAM: 10.9% (3.0/31.4GB)\n\n=== Starting Training ===\nStarting training...\nRAM: 10.9% (3.0/31.4GB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:03, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n✅ Training completed successfully!\nRAM: 10.6% (2.9/31.4GB)\n\n=== Saving Model Artifacts ===\n\n💾 Saving model artifacts to: /kaggle/working/gpt2-lora-trained\n💽 Saving model and adapter...\n🔤 Saving tokenizer...\n📝 Saving training arguments...\n\n🔍 Verifying saved files:\n- README.md (4.96 KB)\n- merges.txt (445.62 KB)\n- training_args.json (3.86 KB)\n- adapter_config.json (0.66 KB)\n- tokenizer_config.json (0.49 KB)\n- tokenizer.json (2058.55 KB)\n- vocab.json (779.45 KB)\n- adapter_model.safetensors (9227.88 KB)\n- special_tokens_map.json (0.13 KB)\n✅ Model saved to: /kaggle/working/gpt2-lora-trained\n\n=== Testing Model ===\n\n🔍 Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\n📂 Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.66 KB)\n- adapter_model.safetensors (9227.88 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\nFound adapter files: ['adapter_config.json', 'adapter_model.safetensors']\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\n🎉 Model loaded successfully!\n\n🚀 Creating text generation pipeline...\n\n🧪 Running generation tests...\n\n🔹 Test 1: What is software wallet, and what's the difference between hardware and software wallet?\n💬 Response: What is software wallet, and what's the difference between hardware and software wallet?\nWell there are two different types of wallets: a physical Bitcoin (which has no electronic signature) or an encrypted digital one. The latter allows users to have secure connections with their funds without any risk whatsoever; however it also offers many advantages over both paper-based payment systems such as PayPal which rely on signing documents instead like your bank account numbers etc… So you can use either type for cash transfers that don't require multiple signatures in order not need authentication by third parties... This will allow those who want to transfer money from banks using traditional currencies but still retain security when they spend online rather than having them sign something else completely private! You won`t be able get hold at home if things go wrong because each person may only give out $10 worth depending on how much time passes before people notice... And even then all other transactions must come through securely so this doesn´re really needed unless everyone knows about its existence within bitcoin itself! A lot more information comes into play though since some folks actually make decisions based off evidence derived directly from \"experts\" - ie I say experts themselves. These include financial analysts working independently looking around every corner trying new ways & techniques\n\n🔹 Test 2: Explain Proof of Work (PoW) in simple terms\n💬 Response: Explain Proof of Work (PoW) in simple terms, but with a large number and easy to understand proofs.\nWe are working on the proof-ofwork code for P2P payments from PoS blockchain by adding support into wallets such as BTCRSS or ETHDASH that can be used together when signing your coins through Paypal which will also allow you to get paid out automatically once done!\n\n:);). The best way I know is if it's possible not to do this immediately after any transaction? Or maybe during some time later at work? For example my wallet did nothing until today! :)) We have already implemented both premine functionality within our own API so we don't need anything special here. However there were still many bugs due largely related back end development issues like scaling up system performance etc...If anyone has suggestions please share them below\n\n🔹 Test 3: Describe blockchain technology in 2-3 sentences\n💬 Response: Describe blockchain technology in 2-3 sentences\nThis is a very long post, but the gist of it all applies to bitcoin. This means that you can understand what makes digital currency work and how your ideas will be realized by using its features (and not just some vague \"bitcoin\" thing like bitcoins). But before we go further here, let's look at this idea: Bitcoin works because people use different currencies over time - they are more likely than others else/they don't have any need for their own money or identities when buying something online so there isn' no way anyone could change them without having already created an account with one through another bank card on each day etc. There simply aren\\'t many ways around those problems which make cryptocurrencies unique enough as well... So if someone wants to create his / her personal wallet then he needs to find out where every single coin has been mined since inception, otherwise nothing ever changes until everyone knows who owns whose coins! If I want to know about other things too i would say : ) The point being rather simple right? You're saying anything doesn`T exist yet? Thats why most companies do stuff quickly & easily based on existing knowledge. And these guys think nobody cares even though miners dont\n\n🔹 Test 4: What is the cryptographic purpose of public/private key pairs?\n💬 Response: What is the cryptographic purpose of public/private key pairs?\nThe main problem with RSA-based private keys, in this case for example encryption and decryption algorithms are based on an idea called \"quantum cryptography\". This notion (which I call a quantum random number generator) requires that you have some form or other associated to make your data secure. It's not possible without doing something more sophisticated like using different protocols which can be used by any one person at once - so even if it works well enough there will still be problems due just about every single protocol from ECDSA backdoors down until now... If anything could happen when we try new things such as elliptic curves etc then our system would probably become much weaker than what was promised... So why should people care anyway? The answer lies somewhere between two extremes: firstly because nobody uses them unless they need all sorts 'proof' necessary before actually sending their secret bits into space but secondly after having no way to prevent those risks being taken off balance within time frame since most trusted systems require very little knowledge beyond how long each second has been sent out. You might ask yourself whether anyone really cares too deeply regarding security issues; although many do who take issue themselves may feel strongly against others\n\n🔹 Test 5: How does peer-to-peer (P2P) networking relate to cryptocurrency?\n💬 Response: How does peer-to-peer (P2P) networking relate to cryptocurrency?\nBolstered by Bitcoin's decentralized nature, the Peercoin network is a distributed and anonymous protocol that allows anyone with access to any computer or device in North America—or anywhere else on earth—without ever being required of an Internet connection. Unlike traditional banking systems like credit cards, which require users' identification as they pay bills through their mobile devices at regular intervals, P3P networks are made up entirely offline; instead consumers can use physical objects such \"virtual\" wallets for payment when there isn't enough money available online from them. The system also enables customers to send payments electronically without needing funds stored elsewhere: no one needs your personal information if you're not using it digitally! As long ago as 2008, more than 300 million people worldwide used pxoin during its first week – many were unaware what was going around…\n\n. This means we have seen massive improvements over our last two years — including new protocols allowing bitcoin transactions across multiple machines simultaneously - but this has still left us completely blindsided once again due largely towards how poorly established those standards will be before 2014 begins coming into force later today.... There really aren\\'t too much interesting things about\n\n=== Generating Validated Responses ===\n\n🔹 Test 1: What is software wallet, and what's the difference between hardware and software wallet?\n\n=== Prompt Quality Assessment ===\n1. Clarity: ✅ (Length: 13 words)\n2. Specificity: ✅ (Contains explicit question)\n3. Technical Focus: ✅\n4. Avoids Ambiguity: ✅\n5. Answer Format Hint: ⚠️\n\n💬 Generated Response:\nWhat is software wallet, and what's the difference between hardware and software wallet?\n\nWe've talked to every major company where we had a major Bitcoin/USD event, some having huge hardware and others needing their own.\n\nSoftware wallets are almost always a first choice for anyone who wants to become tech savvy, and also are a good way to get started with a lot of resources, especially if you have no formal training…but we can help you find the right platform and platform for yourself. We're one of the few companies who offers free Android payments. The way to find software wallets is by looking on the Internet. We try to use Google, Bitpay and other payment services to help you find the right software wallet. If you're already using any of these services, use Bitcoin to make purchases on Bitpay, and then use Bitcoin to shop online. Finally, don't forget to bring your wallet (or other payment methods) on a day-to-day basis. By\n\n✅ Validation: FAILED\n⚠️ Issues: Missing technical terms for wallet: ['private key', 'public key', 'address', 'security']\n\n🔹 Test 2: Explain Proof of Work (PoW) in simple terms\n\n=== Prompt Quality Assessment ===\n1. Clarity: ✅ (Length: 8 words)\n2. Specificity: ⚠️ (Contains explicit question)\n3. Technical Focus: ✅\n4. Avoids Ambiguity: ✅\n5. Answer Format Hint: ⚠️\n\n💬 Generated Response:\nExplain Proof of Work (PoW) in simple terms.\n\nUsing proof of work\n\nA proof of work that is not a proof of work is one that is easily accepted by any scientific community based on the laws of physics. The most recent version of Proof of Work (PoW) is the IEEE/IEEE 1006 standard for verifying code that is publicly available.\n\nProof of Work does not mean that the proof is valid or accurate. Proof can't be easily falsified, as demonstrated by the fact that people who attempt to perform the following computations will receive several more errors during the proof process. In addition, it does mean that the proof is imperfect but does not imply that it is an impossible proof.\n\nWith PoW, one can easily demonstrate that the proof is falsifiable without proof of work. There are two ways that proof of work can be used:\n\nSignatures\n\nThe first method consists of the following steps:\n\nSet\n\n✅ Validation: FAILED\n⚠️ Issues: Missing technical terms for PoW: ['consensus', 'mining', 'difficulty', 'hash']\n\n🔹 Test 3: Describe blockchain technology in 2-3 sentences\n\n=== Prompt Quality Assessment ===\n1. Clarity: ✅ (Length: 6 words)\n2. Specificity: ⚠️ (Contains explicit question)\n3. Technical Focus: ✅\n4. Avoids Ambiguity: ✅\n5. Answer Format Hint: ⚠️\n\n💬 Generated Response:\nDescribe blockchain technology in 2-3 sentences, which sounds like it would have been nice to be able to do so?\n\nIn the same way that we're exploring blockchain technology in the world today, by building an advanced distributed ledger like the Ethereum blockchain, we're expanding our role as a technical and analytical firm, creating more people and our knowledge as we seek to enable a wide range of businesses and institutions with a blockchain-based model to be created on a global level.\n\nAnd in that spirit, here are two statements from the board which you will see used before and after the interview—\n\nFirst, we are very pleased to welcome you to our team. We believe that every project will be unique, unique. How can a team of people that has already worked at IBM and others, come to our firm and work together to advance the most exciting blockchain technology that it has ever created?\n\nOur team has been exploring blockchain technology for a very long time, but it\n\n✅ Validation: FAILED\n⚠️ Issues: Missing technical terms for blockchain: ['immutable', 'blocks', 'decentralized']\n\n🔹 Test 4: What is the cryptographic purpose of public/private key pairs?\n\n=== Prompt Quality Assessment ===\n1. Clarity: ✅ (Length: 9 words)\n2. Specificity: ✅ (Contains explicit question)\n3. Technical Focus: ✅\n4. Avoids Ambiguity: ✅\n5. Answer Format Hint: ⚠️\n\n💬 Generated Response:\nWhat is the cryptographic purpose of public/private key pairs?\n\nThis question is important. What is the cryptographic purpose of private key pairs? A \"public key\" like public/private key pairs is used by cryptography to identify keys. For example, if a public key was provided to the cryptographic library, the library would determine how many strings would be included so its results would be correct.\n\nThat answer is really important, because you're going to have to ask someone who is intimately familiar with encryption and public key encryption algorithms: who are the cryptographic algorithms that make a public key? In public key encryption there's a lot of things in terms of complexity. It's complicated to say, you know, \"Who did this?\" and \"How did I get that string out of there?\" But, that's a good question to have, because it's also a good question to ask. We can try to understand what is the cryptographic purpose behind that, but this is just one of a\n\n✅ Validation: FAILED\n⚠️ Issues: Missing technical terms for key: ['signature', 'asymmetric']\n\n🔹 Test 5: How does peer-to-peer (P2P) networking relate to cryptocurrency?\n\n=== Prompt Quality Assessment ===\n1. Clarity: ✅ (Length: 8 words)\n2. Specificity: ✅ (Contains explicit question)\n3. Technical Focus: ✅\n4. Avoids Ambiguity: ✅\n5. Answer Format Hint: ⚠️\n\n💬 Generated Response:\nHow does peer-to-peer (P2P) networking relate to cryptocurrency?\n\nWe are a distributed system for sharing data about the world.\n\nWe are a system for building open source projects and using these projects with a single person.\n\nWe are a platform with some powerful features, we are doing some new ideas such as decentralized financial technology but we are still doing some big technical efforts so we are developing new ideas.\n\nOur main purpose is to improve our user experience, making sure that you have the most efficient system which is possible for both the user and the development team involved.\n\nIn 2015 our main goal was to create the largest Bitcoin client for all businesses and people on every continent. In a year, Bitcoin and decentralized money technology have arrived as we have some strong business model, network and technology.\n\nThe P2P network is so large that a small number of customers could easily make money on our exchange by selling.\n\nAnd the blockchain\n\n✅ Validation: FAILED\n⚠️ Issues: Missing technical terms for P2P: ['nodes', 'direct']\n\n=== Training and Evaluation Complete ===\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:37:23.328012Z","iopub.execute_input":"2025-05-30T21:37:23.328387Z","iopub.status.idle":"2025-05-30T21:37:23.334037Z","shell.execute_reply.started":"2025-05-30T21:37:23.328351Z","shell.execute_reply":"2025-05-30T21:37:23.332973Z"}},"outputs":[{"name":"stdout","text":"Total notebook execution time: 520.17 seconds\n","output_type":"stream"}],"execution_count":16}]}