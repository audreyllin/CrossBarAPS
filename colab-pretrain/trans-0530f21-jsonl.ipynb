{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12000428,"sourceType":"datasetVersion","datasetId":7548900},{"sourceId":12000455,"sourceType":"datasetVersion","datasetId":7548921},{"sourceId":12009799,"sourceType":"datasetVersion","datasetId":7555612},{"sourceId":12010409,"sourceType":"datasetVersion","datasetId":7556008}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nnotebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:28:43.155337Z","iopub.execute_input":"2025-05-30T21:28:43.155989Z","iopub.status.idle":"2025-05-30T21:28:43.166397Z","shell.execute_reply.started":"2025-05-30T21:28:43.155949Z","shell.execute_reply":"2025-05-30T21:28:43.165068Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Complete Environment Setup\n!pip uninstall -y numpy torch torchvision torchaudio transformers peft bitsandbytes 2>/dev/null || echo \"No packages to uninstall\"\n\n# Remove problematic directories manually\nproblematic_path = \"/usr/local/lib/python3.11/dist-packages/~vidia-cudnn-cu12\"\n\n\n# Clear pip cache\n!pip cache purge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:28:43.168064Z","iopub.execute_input":"2025-05-30T21:28:43.168387Z","iopub.status.idle":"2025-05-30T21:28:47.700325Z","shell.execute_reply.started":"2025-05-30T21:28:43.168361Z","shell.execute_reply":"2025-05-30T21:28:47.699139Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: torch 2.2.1+cu121\nUninstalling torch-2.2.1+cu121:\n  Successfully uninstalled torch-2.2.1+cu121\nFound existing installation: torchvision 0.17.1+cu121\nUninstalling torchvision-0.17.1+cu121:\n  Successfully uninstalled torchvision-0.17.1+cu121\nFound existing installation: torchaudio 2.2.1+cu121\nUninstalling torchaudio-2.2.1+cu121:\n  Successfully uninstalled torchaudio-2.2.1+cu121\nFound existing installation: transformers 4.41.2\nUninstalling transformers-4.41.2:\n  Successfully uninstalled transformers-4.41.2\nFound existing installation: peft 0.10.0\nUninstalling peft-0.10.0:\n  Successfully uninstalled peft-0.10.0\nFound existing installation: bitsandbytes 0.43.0\nUninstalling bitsandbytes-0.43.0:\n  Successfully uninstalled bitsandbytes-0.43.0\nFiles removed: 90\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install NumPy FIRST with clean environment\n!pip install -q --ignore-installed numpy==1.26.4\n\n# Install PyTorch with CUDA 12.1 (Kaggle's version)\n!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n\n# Install transformer-related packages with compatible versions\n!pip install -q transformers==4.41.2 peft==0.10.0 datasets==2.18.0 accelerate==0.29.1\n!pip install -q bitsandbytes==0.43.0 einops==0.7.0\n\n# Handle gymnasium separately to avoid conflicts\n!pip install -q gymnasium==0.29.0 --no-deps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:28:47.701753Z","iopub.execute_input":"2025-05-30T21:28:47.702064Z","iopub.status.idle":"2025-05-30T21:30:10.249353Z","shell.execute_reply.started":"2025-05-30T21:28:47.702033Z","shell.execute_reply":"2025-05-30T21:30:10.248043Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.29.1 requires torch>=1.10.0, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntorchmetrics 1.7.1 requires torch>=2.0.0, which is not installed.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, which is not installed.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nsentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\nfastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sys\nimport os\nimport json\nimport numpy as np\nimport psutil\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    pipeline,\n    GenerationConfig\n)\nfrom sentence_transformers import util as semantic_util","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:10.252707Z","iopub.execute_input":"2025-05-30T21:30:10.253023Z","iopub.status.idle":"2025-05-30T21:30:21.543431Z","shell.execute_reply.started":"2025-05-30T21:30:10.252994Z","shell.execute_reply":"2025-05-30T21:30:21.542229Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n","output_type":"stream"},{"name":"stderr","text":"2025-05-30 21:30:17.549415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748640617.586711     212 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748640617.597883     212 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Model Loading\n# =====================\nMODEL_NAME = \"gpt2\"  # Change to \"meta-llama/Llama-2-7b-chat-hf\" for Llama\n\ndef print_memory():\n    \"\"\"Memory usage diagnostics for the environment\"\"\"\n    if torch.cuda.is_available():\n        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n        print(f\"GPU Memory: {gpu_mem:.2f}GB\", end=\" | \")\n    ram = psutil.virtual_memory()\n    print(f\"RAM: {ram.percent}% ({ram.used/1024**3:.1f}/{ram.total/1024**3:.1f}GB)\")\n\ndef load_model(model_name):\n    \"\"\"Load model with improved error handling and phi-1.5 specific settings\"\"\"\n    print(f\"\\n=== Loading Model: {model_name} ===\")\n    print_memory()\n    \n    # Phi-1.5 specific configuration\n    trust_remote_code = True  # Required for phi-1.5\n    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n    \n    # Quantization config for memory efficiency\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch_dtype\n    )\n    \n    try:\n        print(\"Attempting quantized load...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=trust_remote_code,\n            device_map=\"auto\",\n            torch_dtype=torch_dtype\n        )\n        \n        print(\"\\n✅ Model loaded successfully!\")\n        print_memory()\n        return model\n        \n    except Exception as e:\n        print(f\"\\n❌ Model loading failed: {str(e)}\")\n        print(\"Attempting standard load without quantization...\")\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                torch_dtype=torch_dtype\n            )\n            print(\"\\n✅ Model loaded successfully without quantization!\")\n            print_memory()\n            return model\n        except Exception as e:\n            print(f\"\\n❌ Standard load failed: {str(e)}\")\n            print(\"Attempting CPU-only fallback...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                trust_remote_code=trust_remote_code,\n                device_map=\"cpu\",\n                torch_dtype=torch.float32\n            )\n            print(\"\\n✅ Model loaded on CPU\")\n            print_memory()\n            return model\n\nmodel = load_model(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:21.544905Z","iopub.execute_input":"2025-05-30T21:30:21.545965Z","iopub.status.idle":"2025-05-30T21:30:22.876979Z","shell.execute_reply.started":"2025-05-30T21:30:21.545904Z","shell.execute_reply":"2025-05-30T21:30:22.875699Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model: gpt2 ===\nRAM: 5.7% (1.3/31.4GB)\nAttempting quantized load...\n\n❌ Model loading failed: No GPU found. A GPU is needed for quantization.\nAttempting standard load without quantization...\n\n✅ Model loaded successfully without quantization!\nRAM: 7.2% (1.8/31.4GB)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Tokenizer Setup\n# =====================\ndef load_tokenizer(model_name):\n    \"\"\"Load and configure tokenizer\"\"\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"Tokenizer loaded successfully\")\n        return tokenizer\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {str(e)}\")\n        raise\n\ntokenizer = load_tokenizer(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:22.878161Z","iopub.execute_input":"2025-05-30T21:30:22.878549Z","iopub.status.idle":"2025-05-30T21:30:23.197839Z","shell.execute_reply.started":"2025-05-30T21:30:22.878494Z","shell.execute_reply":"2025-05-30T21:30:23.196414Z"}},"outputs":[{"name":"stdout","text":"Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Data Preparation\n# =====================\ndef prepare_dataset(file_path=\"/kaggle/input/database\", max_samples=1000):\n    \"\"\"Prepare dataset with robust error handling\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"❌ Path not found: {file_path}\")\n            \n        # Load JSONL file directly using datasets library\n        dataset = load_dataset('json', data_files=file_path, split='train[:{}]'.format(max_samples))\n        \n        # If the dataset has multiple columns, we just want the text\n        if 'text' not in dataset.features:\n            # Try to find the first text-like column\n            text_columns = [col for col in dataset.features if any(t in col.lower() for t in ['text', 'content', 'body'])]\n            if text_columns:\n                dataset = dataset.rename_column(text_columns[0], 'text')\n            else:\n                # If no text column found, combine all string columns\n                string_columns = [col for col in dataset.features if dataset.features[col].dtype == 'string']\n                if string_columns:\n                    def combine_columns(examples):\n                        return {'text': ' '.join(str(examples[col]) for col in string_columns)}\n                    dataset = dataset.map(combine_columns)\n                else:\n                    raise ValueError(\"No text columns found in dataset\")\n        \n        print(f\"✅ Loaded dataset with {len(dataset)} samples\")\n        return dataset\n        \n    except Exception as e:\n        print(f\"\\n❌ Dataset preparation failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        return Dataset.from_dict({\"text\": [\"Sample text \" + str(i) for i in range(10)]})\n\ndef safe_tokenize(examples):\n    \"\"\"Tokenization with explicit numpy workarounds\"\"\"\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=90,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        # Convert to lists explicitly\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].tolist(),\n            \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n            \"labels\": tokenized[\"input_ids\"].tolist()\n        }\n    except RuntimeError as e:\n        if \"Numpy is not available\" in str(e):\n            # Fallback using pure Python\n            return {\n                \"input_ids\": [[0]*512],\n                \"attention_mask\": [[1]*512],\n                \"labels\": [[0]*512]\n            }\n        raise\n\ntry:\n    print(\"\\n=== Starting Processing ===\")\n    dataset = prepare_dataset()\n    \n    # Small batch test first\n    test_batch = dataset.select(range(2))\n    test_tokenized = test_batch.map(safe_tokenize, batched=True)\n    \n    # If test succeeds, process full dataset\n    tokenized_dataset = dataset.map(safe_tokenize, batched=True, batch_size=4)\n    tokenized_dataset.set_format(type='torch')\n    \n    print(\"✅ Processing completed successfully!\")\n    \nexcept Exception as e:\n    print(f\"\\n❌ Error: {str(e)}\")\n    print(\"Creating minimal fallback dataset...\")\n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": [[0,1,2,3]],\n        \"attention_mask\": [[1,1,1,1]],\n        \"labels\": [[0,1,2,3]]\n    })\n    tokenized_dataset.set_format(type='torch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.198814Z","iopub.execute_input":"2025-05-30T21:30:23.199173Z","iopub.status.idle":"2025-05-30T21:30:23.638022Z","shell.execute_reply.started":"2025-05-30T21:30:23.199146Z","shell.execute_reply":"2025-05-30T21:30:23.636999Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Processing ===\n\n❌ Dataset preparation failed: Unable to find '/kaggle/input/database'\nCreating minimal fallback dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742bcdf16fd447f18db14c1ae66596d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe7591f91052449aacbd7fe580f63cee"}},"metadata":{}},{"name":"stdout","text":"✅ Processing completed successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Training Configuration\n# =====================\ndef configure_training(model):\n    \"\"\"Configure training parameters and LoRA setup\"\"\"\n    # Enable gradient checkpointing to save memory\n    model.gradient_checkpointing_enable()\n\n    # LoRA configuration\n    peft_config = LoraConfig(\n        r=16,  \n        lora_alpha=32,\n        target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # GPT-2 compatible modules\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        fan_in_fan_out=True\n    )\n\n    # Training arguments optimized for Kaggle\n    training_args = TrainingArguments(\n        output_dir=\"/kaggle/working/phi1.5-lora-results\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,  # Reduced for Kaggle\n        learning_rate=2e-5,\n        optim=\"adamw_torch\",\n        logging_steps=10,\n        save_steps=500,\n        fp16=torch.cuda.is_available(),\n        max_grad_norm=0.3,\n        warmup_ratio=0.1,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"none\"\n    )\n\n    # Prepare model for training\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n\n    # Print trainable parameters\n    model.print_trainable_parameters()\n    \n    return model, training_args","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.639112Z","iopub.execute_input":"2025-05-30T21:30:23.639494Z","iopub.status.idle":"2025-05-30T21:30:23.646709Z","shell.execute_reply.started":"2025-05-30T21:30:23.639441Z","shell.execute_reply":"2025-05-30T21:30:23.645581Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Training Execution\n# =====================\ndef train_model(model, tokenized_dataset, training_args):\n    \"\"\"Execute the training process\"\"\"\n    # Disable cache if gradient checkpointing is enabled\n    if training_args.gradient_checkpointing:\n        model.config.use_cache = False\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n                             'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n                             'labels': torch.stack([f['input_ids'] for f in data])}\n    )\n    \n    print(\"Starting training...\")\n    print_memory()\n    trainer.train()\n    print(\"Training completed!\")\n    return trainer\n\ndef generate_contrastive_examples(example):\n    \"\"\"Generate contrastive examples for training\"\"\"\n    # Generate negative sample by:\n    # 1. Random Q/A from different category\n    # 2. GPT-generated incorrect answer\n    # 3. Perturbed correct answer\n    return {\n        'anchor': example['answer'],\n        'positive': augment_answer(example['answer']),\n        'negative': get_negative_sample(example)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.647848Z","iopub.execute_input":"2025-05-30T21:30:23.648222Z","iopub.status.idle":"2025-05-30T21:30:23.672669Z","shell.execute_reply.started":"2025-05-30T21:30:23.648190Z","shell.execute_reply":"2025-05-30T21:30:23.671433Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Model Saving\n# =====================\ndef save_model_artifacts(\n    model, \n    tokenizer, \n    training_args: Optional[TrainingArguments] = None, \n    output_dir: str = \"/kaggle/working/gpt2-lora-trained\"\n) -> str:\n    \"\"\"\n    Save all model artifacts with comprehensive verification.\n    Handles both single-file and sharded model formats.\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\n💾 Saving model artifacts to: {output_dir}\")\n    \n    # For LoRA models - DON'T merge adapters before saving\n    # We want to save the adapter separately\n    print(\"💽 Saving model and adapter...\")\n    \n    # Save the entire model (base model + adapter)\n    model.save_pretrained(\n        output_dir,\n        safe_serialization=True,\n        state_dict=model.state_dict()  # Save the complete state including LoRA\n    )\n    \n    # Save tokenizer\n    print(\"🔤 Saving tokenizer...\")\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save training arguments if provided\n    if training_args is not None:\n        print(\"📝 Saving training arguments...\")\n        try:\n            args_path = os.path.join(output_dir, \"training_args.json\")\n            if hasattr(training_args, 'to_dict'):\n                with open(args_path, \"w\") as f:\n                    json.dump(training_args.to_dict(), f, indent=2)\n            elif hasattr(training_args, 'to_json_string'):\n                with open(args_path, \"w\") as f:\n                    f.write(training_args.to_json_string())\n            else:\n                print(\"⚠️ Warning: TrainingArguments has no serialization method\")\n        except Exception as e:\n            print(f\"⚠️ Warning: Failed to save training args - {str(e)}\")\n    \n    # Verify the adapter files were saved\n    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n    missing_files = []\n    for file in required_files:\n        if not os.path.exists(os.path.join(output_dir, file)):\n            missing_files.append(file)\n    \n    if missing_files:\n        print(f\"\\n⚠️ Warning: Missing adapter files: {missing_files}\")\n        print(\"Trying alternative save method...\")\n        # Explicitly save the adapter\n        model.save_pretrained(\n            output_dir,\n            safe_serialization=True,\n            adapter_only=True  # This ensures adapter files are saved\n        )\n    \n    print(\"\\n🔍 Verifying saved files:\")\n    for file in os.listdir(output_dir):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"- {file} ({size:.2f} KB)\")\n    \n    return output_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.675474Z","iopub.execute_input":"2025-05-30T21:30:23.675835Z","iopub.status.idle":"2025-05-30T21:30:23.696142Z","shell.execute_reply.started":"2025-05-30T21:30:23.675808Z","shell.execute_reply":"2025-05-30T21:30:23.695020Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Model Loading and Testing\n# =====================\ndef load_and_test_model(\n    model_path: str = \"/kaggle/working/gpt2-lora-trained\", \n    max_length: int = 250,\n    test_prompts: Optional[list] = None,\n    is_peft_model: bool = True\n):\n    \"\"\"\n    Load and test a saved model with comprehensive error handling\n    \"\"\"\n    print(f\"\\n🔍 Preparing to load model from: {model_path}\")\n    \n    # Verify model directory exists\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model directory {model_path} does not exist\")\n    \n    # Show directory contents for debugging\n    print(\"\\n📂 Model directory contents:\")\n    for f in sorted(os.listdir(model_path)):\n        size = os.path.getsize(os.path.join(model_path, f)) / 1024\n        print(f\"- {f} ({size:.2f} KB)\")\n    \n    try:\n        print(\"\\n🔄 Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            local_files_only=True\n        )\n        \n        print(\"\\n🔄 Loading model...\")\n        if is_peft_model:\n            # First check if we have adapter files\n            adapter_files = [\n                f for f in os.listdir(model_path) \n                if f.startswith('adapter_') or f == 'adapter_config.json'\n            ]\n            \n            if not adapter_files:\n                print(\"⚠️ No adapter files found. Loading as regular model.\")\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n            else:\n                print(f\"Found adapter files: {adapter_files}\")\n                # Load base model first\n                base_model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    local_files_only=True\n                )\n                \n                # Then load the PEFT adapter\n                model = PeftModel.from_pretrained(\n                    base_model,\n                    model_path,\n                    local_files_only=True\n                )\n                \n                # Merge and unload for inference\n                model = model.merge_and_unload()\n        else:\n            # For regular models\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=\"auto\",\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                local_files_only=True\n            )\n            \n        print(\"\\n🎉 Model loaded successfully!\")\n        \n        # Default test prompts if none provided\n        if test_prompts is None:\n            test_prompts = [\n                \"What is hardware wallet?? \",\n                \"What is Proof of Work (PoW)?? \",\n                \"What is cryptography?? \",\n                \"What is Peer-to-Peer (P2P)?? \",\n                \"What is block chain?? \",\n                \"What is private key?? \"\n            ]\n        \n        # Create pipeline - REMOVED device parameter since we're using device_map=\"auto\"\n        print(\"\\n🚀 Creating text generation pipeline...\")\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        )\n        \n        # Run tests\n        print(\"\\n🧪 Running generation tests...\")\n        for i, prompt in enumerate(test_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {prompt}\")\n            output = pipe(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                num_return_sequences=1,\n                repetition_penalty=1.2\n            )\n            print(\"💬 Response:\", output[0]['generated_text'])\n            \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"\\n❌ Critical error loading model: {str(e)}\")\n        print(\"\\n🛠️ Debugging info:\")\n        print(f\"- Path: {os.path.abspath(model_path)}\")\n        print(f\"- Directory exists: {os.path.exists(model_path)}\")\n        if os.path.exists(model_path):\n            print(\"- Contents:\", os.listdir(model_path))\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.697485Z","iopub.execute_input":"2025-05-30T21:30:23.697969Z","iopub.status.idle":"2025-05-30T21:30:23.730858Z","shell.execute_reply.started":"2025-05-30T21:30:23.697930Z","shell.execute_reply":"2025-05-30T21:30:23.729851Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Enhanced Model Wrapper\n# =====================\nclass ConstrainedCryptoModel(nn.Module):\n    def __init__(self, base_model, tokenizer, knowledge_base=None):\n        super().__init__()\n        self.model = base_model\n        self.tokenizer = tokenizer\n        self.knowledge_base = knowledge_base\n        \n        # Technical term dictionary for crypto concepts\n        self.technical_terms = {\n            'PoW': ['consensus', 'mining', 'difficulty', 'hashrate'],\n            'blockchain': ['ledger', 'immutable', 'blocks', 'decentralized'],\n            'cryptography': ['encryption', 'keys', 'security', 'algorithm'],\n            'wallet': ['private key', 'public key', 'address', 'seed phrase']\n        }\n        \n        # Banned phrases and sequences\n        self.banned_sequences = [\n            \"I don't know\", \"as an AI\", \"I'm not sure\", \n            \"I can't answer\", \"my training data\"\n        ]\n        \n        # Required technical terms\n        self.force_included = [\n            \"blockchain\", \"cryptography\", \"decentralized\",\n            \"consensus\", \"encryption\", \"keys\"\n        ]\n\n    def forward(self, input_ids, **kwargs):\n        outputs = self.model(input_ids, **kwargs)\n        outputs.logits = self.apply_crypto_constraints(outputs.logits, input_ids)\n        return outputs\n    \n    def apply_crypto_constraints(self, logits, input_ids):\n        \"\"\"Apply multi-dimensional constraints for crypto-specific generation\"\"\"\n        generated_text = self.tokenizer.decode(input_ids[0])\n        \n        # 1. Force inclusion of key technical terms\n        for term in self.force_included:\n            if term in generated_text.lower():\n                term_ids = self.tokenizer.encode(term, add_special_tokens=False)\n                for tid in term_ids:\n                    logits[0, -1, tid] += 5.0\n        \n        # 2. Ban non-technical or uncertain phrases\n        for phrase in self.banned_sequences:\n            phrase_ids = self.tokenizer.encode(phrase, add_special_tokens=False)\n            if len(phrase_ids) > 0:\n                logits[0, -1, phrase_ids[0]] = -float('inf')\n        \n        # 3. Validate technical consistency\n        last_term = self.get_last_term(generated_text)\n        if last_term in self.technical_terms:\n            for term in self.technical_terms[last_term]:\n                term_ids = self.tokenizer.encode(term, add_special_tokens=False)\n                for tid in term_ids:\n                    logits[0, -1, tid] += 3.0\n        \n        return logits\n    \n    def get_last_term(self, text):\n        \"\"\"Extract the main technical term from the query\"\"\"\n        question_words = [\"what\", \"how\", \"explain\", \"describe\"]\n        words = [w for w in text.lower().split() if w not in question_words]\n        return words[-1].strip('?') if words else \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.732077Z","iopub.execute_input":"2025-05-30T21:30:23.732424Z","iopub.status.idle":"2025-05-30T21:30:23.756428Z","shell.execute_reply.started":"2025-05-30T21:30:23.732389Z","shell.execute_reply":"2025-05-30T21:30:23.755355Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Enhanced Generation\n# =====================\nCRYPTO_GENERATION_CONFIG = GenerationConfig(\n    max_new_tokens=150,\n    no_repeat_ngram_size=4,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=40,\n    repetition_penalty=1.15,\n    num_beams=3,\n    early_stopping=True\n)\n\ndef generate_with_validation(model, tokenizer, prompt, max_length=200):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # First pass generation\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        generation_config=CRYPTO_GENERATION_CONFIG\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Validation checks\n    validation_passed = True\n    validation_notes = []\n    \n    # 1. Technical term check\n    last_term = model.get_last_term(prompt)\n    if last_term in model.technical_terms:\n        missing = [t for t in model.technical_terms[last_term] \n                  if t.lower() not in response.lower()]\n        if missing:\n            validation_passed = False\n            validation_notes.append(f\"Missing technical terms: {missing}\")\n    \n    # 2. Hallucination check\n    if any(phrase in response for phrase in model.banned_sequences):\n        validation_passed = False\n        validation_notes.append(\"Potential hallucination\")\n    \n    # Generate final output\n    if not validation_passed:\n        print(f\"⚠️ Validation issues: {validation_notes}\")\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            generation_config=CRYPTO_GENERATION_CONFIG,\n            bad_words_ids=[[tid] for tid in tokenizer.encode(\" \".join(model.banned_sequences), add_special_tokens=False)]\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return {\n        'response': response,\n        'validation_passed': validation_passed,\n        'validation_notes': validation_notes\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.757561Z","iopub.execute_input":"2025-05-30T21:30:23.757939Z","iopub.status.idle":"2025-05-30T21:30:23.780696Z","shell.execute_reply.started":"2025-05-30T21:30:23.757906Z","shell.execute_reply":"2025-05-30T21:30:23.779408Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# =====================\n# Main Training Flow\n# =====================\nif __name__ == \"__main__\":\n    # Initialize with memory check\n    print(\"=== Initializing Training Process ===\")\n    print_memory()\n    \n    # 1. Load model and tokenizer with error handling\n    try:\n        print(\"\\n=== Loading Model and Tokenizer ===\")\n        model = load_model(MODEL_NAME)\n        tokenizer = load_tokenizer(MODEL_NAME)\n        print(\"✅ Model and tokenizer loaded successfully\")\n    except Exception as e:\n        print(f\"❌ Failed to load model/tokenizer: {str(e)}\")\n        raise\n\n    # 2. Dataset preparation with robust error handling\n    try:\n        print(\"\\n=== Preparing Dataset ===\")\n        dataset = prepare_dataset()\n        \n        # Small validation batch first\n        print(\"Running validation on small batch...\")\n        test_batch = dataset.select(range(min(2, len(dataset))))\n        test_tokenized = test_batch.map(safe_tokenize, batched=True)\n        \n        # Full dataset processing\n        print(\"Processing full dataset...\")\n        tokenized_dataset = dataset.map(\n            safe_tokenize,\n            batched=True,\n            batch_size=4,\n            remove_columns=dataset.column_names\n        )\n        tokenized_dataset.set_format(type='torch')\n        print(f\"✅ Dataset prepared with {len(tokenized_dataset)} samples\")\n        print_memory()\n        \n    except Exception as e:\n        print(f\"\\n❌ Dataset processing failed: {str(e)}\")\n        print(\"Creating minimal fallback dataset...\")\n        tokenized_dataset = Dataset.from_dict({\n            \"input_ids\": [[0,1,2,3]],\n            \"attention_mask\": [[1,1,1,1]],\n            \"labels\": [[0,1,2,3]]\n        })\n        tokenized_dataset.set_format(type='torch')\n\n    # 3. Training configuration\n    try:\n        print(\"\\n=== Configuring Training ===\")\n        model, training_args = configure_training(model)\n        print(\"✅ Training configured with parameters:\")\n        print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n        print(f\"- Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n        print(f\"- Epochs: {training_args.num_train_epochs}\")\n        print(f\"- Learning rate: {training_args.learning_rate}\")\n        print_memory()\n    except Exception as e:\n        print(f\"❌ Training configuration failed: {str(e)}\")\n        raise\n\n    # 4. Training execution\n    try:\n        print(\"\\n=== Starting Training ===\")\n        trainer = train_model(model, tokenized_dataset, training_args)\n        print(\"✅ Training completed successfully!\")\n        print_memory()\n    except Exception as e:\n        print(f\"❌ Training failed: {str(e)}\")\n        raise\n\n    # 5. Model saving\n    try:\n        print(\"\\n=== Saving Model Artifacts ===\")\n        model_path = save_model_artifacts(model, tokenizer, training_args)\n        print(f\"✅ Model saved to: {model_path}\")\n    except Exception as e:\n        print(f\"❌ Model saving failed: {str(e)}\")\n        raise\n\n    # 6. Model testing and validation\n    try:\n        print(\"\\n=== Testing Model ===\")\n        # Test prompts covering key crypto concepts\n        custom_prompts = [\n            \"What is software wallet, and what's the difference between hardware and software wallet? \",\n            \"What is PoW? \",\n            \"Explain PoW in 1 sentence. \",\n            \"Describe the key features of PoW using 3 words. \",\n            \"What is PoM? Is it something related to cryptography? \",\n            \"What is a cryptographic product? \",\n            \"What is P2P? \",\n            \"What is block chain? \",\n            \"What is public key, and what's the difference between private and public key? \"\n        ]\n        \n        # Load and test the saved model\n        loaded_model, loaded_tokenizer = load_and_test_model(\n            model_path, \n            test_prompts=custom_prompts, \n            is_peft_model=True\n        )\n        \n        # Wrap with constrained model for technical validation\n        constrained_model = ConstrainedCryptoModel(loaded_model, loaded_tokenizer)\n        \n        # Generate and validate responses\n        print(\"\\n=== Generating Validated Responses ===\")\n        for i, query in enumerate(custom_prompts, 1):\n            print(f\"\\n🔹 Test {i}: {query}\")\n            result = generate_with_validation(constrained_model, loaded_tokenizer, query)\n            print(f\"💬 Response: {result['response']}\")\n            print(f\"✅ Validation: {'PASSED' if result['validation_passed'] else 'FAILED'}\")\n            if not result['validation_passed']:\n                print(f\"⚠️ Issues: {result['validation_notes']}\")\n        \n        print(\"\\n=== Training and Evaluation Complete ===\")\n        \n    except Exception as e:\n        print(f\"❌ Model testing failed: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:30:23.782101Z","iopub.execute_input":"2025-05-30T21:30:23.782411Z","iopub.status.idle":"2025-05-30T21:31:48.237882Z","shell.execute_reply.started":"2025-05-30T21:30:23.782380Z","shell.execute_reply":"2025-05-30T21:31:48.236331Z"}},"outputs":[{"name":"stdout","text":"=== Initializing Training Process ===\nRAM: 7.4% (1.9/31.4GB)\n\n=== Loading Model and Tokenizer ===\n\n=== Loading Model: gpt2 ===\nRAM: 7.4% (1.9/31.4GB)\nAttempting quantized load...\n\n❌ Model loading failed: No GPU found. A GPU is needed for quantization.\nAttempting standard load without quantization...\n\n✅ Model loaded successfully without quantization!\nRAM: 8.9% (2.3/31.4GB)\nTokenizer loaded successfully\n✅ Model and tokenizer loaded successfully\n\n=== Preparing Dataset ===\n\n❌ Dataset preparation failed: Unable to find '/kaggle/input/database'\nCreating minimal fallback dataset...\nRunning validation on small batch...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaccd5fe72d34cf1851bc77bc73b098f"}},"metadata":{}},{"name":"stdout","text":"Processing full dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c59b880be8447889966d0507bf706a5"}},"metadata":{}},{"name":"stdout","text":"✅ Dataset prepared with 10 samples\nRAM: 8.5% (2.2/31.4GB)\n\n=== Configuring Training ===\ntrainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635\n✅ Training configured with parameters:\n- Batch size: 1\n- Gradient accumulation: 4\n- Epochs: 1\n- Learning rate: 2e-05\nRAM: 8.5% (2.2/31.4GB)\n\n=== Starting Training ===\nStarting training...\nRAM: 8.5% (2.2/31.4GB)\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:03, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n✅ Training completed successfully!\nRAM: 8.7% (2.3/31.4GB)\n\n=== Saving Model Artifacts ===\n\n💾 Saving model artifacts to: /kaggle/working/gpt2-lora-trained\n💽 Saving model and adapter...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"🔤 Saving tokenizer...\n📝 Saving training arguments...\n\n🔍 Verifying saved files:\n- README.md (4.96 KB)\n- merges.txt (445.62 KB)\n- training_args.json (3.86 KB)\n- adapter_config.json (0.66 KB)\n- tokenizer_config.json (0.49 KB)\n- tokenizer.json (2058.55 KB)\n- vocab.json (779.45 KB)\n- adapter_model.safetensors (9227.88 KB)\n- special_tokens_map.json (0.13 KB)\n✅ Model saved to: /kaggle/working/gpt2-lora-trained\n\n=== Testing Model ===\n\n🔍 Preparing to load model from: /kaggle/working/gpt2-lora-trained\n\n📂 Model directory contents:\n- README.md (4.96 KB)\n- adapter_config.json (0.66 KB)\n- adapter_model.safetensors (9227.88 KB)\n- merges.txt (445.62 KB)\n- special_tokens_map.json (0.13 KB)\n- tokenizer.json (2058.55 KB)\n- tokenizer_config.json (0.49 KB)\n- training_args.json (3.86 KB)\n- vocab.json (779.45 KB)\n\n🔄 Loading tokenizer...\n\n🔄 Loading model...\nFound adapter files: ['adapter_config.json', 'adapter_model.safetensors']\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\n🎉 Model loaded successfully!\n\n🚀 Creating text generation pipeline...\n\n🧪 Running generation tests...\n\n🔹 Test 1: What is software wallet, and what's the difference between hardware and software wallet? \n💬 Response: What is software wallet, and what's the difference between hardware and software wallet?  It depends on who you ask.\nHardware wallets are not for everyone - there can be many different reasons to buy a computer or laptop in any given year (as long as it has an internet connection). Hardware computers have much lower costs of installation than their digital equivalents which means they offer more security features such that users will never need multiple devices at once when using them simultaneously! For instance we've seen some laptops with no built-in storage hard drive like those from Dell but even these models get plenty outlay if purchased separately so having two separate units without one could cost around $500 per month too!. Software wallets usually come preinstalled into your home desktop system where customers pay upfront via credit card while all other services may take up extra time before installing new functionality. It doesn't matter how good this first set off sound because just use something else instead:\n\nTrying both should give me about half way through my purchase...\n\n🔹 Test 2: What is PoW? \n💬 Response: What is PoW?  PoWs are simple to implement and can be used with any Java program. They work on a single core or even multiple cores, so if you want some flexibility in your codebase (which I'm not), there's no need for much more than this basic functionality:\npublic class MyClass { private static final String CONTAINER = \"myclass\"; public int myMyClass() throws Exception{ return getCompletion(); } }; // Create the new object Public void main(String[] args) {}\n\n🔹 Test 3: Explain PoW in 1 sentence. \n💬 Response: Explain PoW in 1 sentence.  When you add the verb, it is really difficult to write a poem without giving your author credit for some of their work: I've always thought that poetry has all kinds and characters which are so important they would be better left alone or even ignored by people who wouldn't understand them at first glance (e\nThis essay originally appeared on this site).\n\n🔹 Test 4: Describe the key features of PoW using 3 words. \n💬 Response: Describe the key features of PoW using 3 words.  I will use an alternative method that I have used before, but this time with a little more clarity:\nNote 1 : The following is not my favorite way to describe these elements in terms such as \"key\" and other similar phrases (although it's worth noting some are better than others).   It was probably one-dimensional when describing them because they had all sorts involved -- for example, there were two people on each side who could see which person looked at their eyes or what type of eye would be visible if you walked into any building where everyone else saw your face. In fact, we don't even know how many names someone has given us so far without actually asking about our faces! For instance though anyone can create whatever name he wants based off his own unique facial expression style--all those things just happen automatically from within code! Also note 2 - There may well exist certain typesetting techniques by hand here called'syntactic primitives' written down below... although perhaps no amount should ever compare directly against every single piece of writing done upon creating new expressions :) So please do ask yourself why does programming make sense? Do programmers want something different? Or maybe developers\n\n🔹 Test 5: What is PoM? Is it something related to cryptography? \n💬 Response: What is PoM? Is it something related to cryptography?  It's a tool that makes use of the same idea, but in some ways different.\nHow do you think this can be used for your project? I have no real knowledge about what they are doing right now and how things could go wrong if someone did decide to make such an attack on Bitcoin Core using their own code. But at least these two tools will help us out :)\n\n🔹 Test 6: What is a cryptographic product? \n💬 Response: What is a cryptographic product?  You can call it any way you like, but I'm going to use the term \"crypto\" here because that's what we're talking about.     If there are many possible ways of storing your data on disk or in memory (or even just with an encryption key), then this type will be useful for us as well!\nIn terms where crypto comes from and how they work out...I have already mentioned some interesting things such all types of software-based systems come together when looking at hardware: cryptographic algorithms; cryptography libraries/decoders - most popular tools used by Linux developers etc., which include both AES vs RSA based applications along side these products & services. Here also seems pretty obvious why Cryptography should not exist outside codebase : ) Why do so few people understand its benefits? It has been stated above before, however if one considers every other piece of information related only to security itself – including whether certain components were designed using CryptoCore2D3B5C6F4E9A064CD8DF1BB7BEBA9877AAAB9906CA79484047506045902534352697379630895827\n\n🔹 Test 7: What is P2P? \n💬 Response: What is P2P?  In my book, The Digital Age, I describe the concept of \"p2pa\" (pronounced pup-buh), which was developed in response to a recent issue on an article about how digital communications are changing our lives. In this post, we will look at some basic principles and what they mean for us as consumers today.\nThe Basics: Why Do Consumers Need A Smart Phone Today? It's Not About Getting One You're Using On Your Own Device - What Is it Anyway For Us To Use Our Apps And Services That We Don't Have Accessible From Another Person When Doing Business Online or With Others Who Are Having Problems Finding Themselves : There isn´t much you can do with your phone without using one that has no built-in battery! If someone else does use their phones like yours did before then perhaps there won`T be any problems when purchasing them online because those people have all sorts going on outside devices including smartwatches running Android versions 3+, iPhones 6+ & 7+. People who own smartphones but don�ts know anything more than software development tools need something called Microsoft WordPad™! After getting used here first thing out of reading through these posts -- if anyone\n\n🔹 Test 8: What is block chain? \n💬 Response: What is block chain?  \"Blockchain\", a blockchain, uses the hash power of transactions to determine which addresses are accepted on behalf by an organization.\nThe difference between \"blockchains,\" as they're called in China and India, was that those were used for government-issued digital currencies like bitcoin or dollars; Bitcoin itself has no currency whatsoever but simply consists entirely of its own private keys (which it does use). This means you can't just buy from anyone who's willing enough to pay your fees while using bitcoins - if someone doesn' want them then why should I trust their money?\" That said this isn 'bitcoin,' so there won\\'t be any change made until after 2017 because all wallets will support only one version at time! The fact remains though: although Satoshi Nakamoto recently announced his intention not long ago he hasn tabled more than 100 different versions across several countries throughout history before finally announcing something else – namely we'll get rid probably some form OF transaction fee system implemented into our existing systems…if ever possible.\"\n\n\n\n🔹 Test 9: What is public key, and what's the difference between private and public key? \n💬 Response: What is public key, and what's the difference between private and public key?  This has been a bit of an ongoing discussion around crypto.com over some time now but I'd like to bring you up-to date information about how it works in practice for your needs.\nThe Private Key: The Public Keys are only used by certain parties (e which does not require any specific permission from anyone) when they provide encryption keys or access codes with no other means than getting one at their own expense through email messages on exchange sites such as CryptoCompare, CoinMarketCap etc... This can be found online if using \"Private\" mode that allows users to send encrypted data via ethereum exchanges without needing authorization before sending them back into circulation/mining pools! You will need 3 separate copies - this requires 4 different people involved so use 2 because most accounts have multiple account holders who share similar IP addresses.... As far my personal opinion goes though these aren't needed since there isn�t even proof yet just another way available :) For those interested here we go : http://www7daybanker.info /privatekeylesswallet Here comes our second point...\n\nTaken together all three points mean very little really except perhaps keeping track where each\n\n=== Generating Validated Responses ===\n\n🔹 Test 1: What is software wallet, and what's the difference between hardware and software wallet? \n❌ Model testing failed: 'ConstrainedCryptoModel' object has no attribute 'device'\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_212/2449245985.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n🔹 Test {i}: {query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_with_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"💬 Response: {result['response']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Validation: {'PASSED' if result['validation_passed'] else 'FAILED'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_212/643574594.py\u001b[0m in \u001b[0;36mgenerate_with_validation\u001b[0;34m(model, tokenizer, prompt, max_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_with_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# First pass generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ConstrainedCryptoModel' object has no attribute 'device'"],"ename":"AttributeError","evalue":"'ConstrainedCryptoModel' object has no attribute 'device'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"notebook_end = time.time()\nprint(f\"Total notebook execution time: {notebook_end - notebook_start:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T21:31:48.238758Z","iopub.status.idle":"2025-05-30T21:31:48.239103Z","shell.execute_reply.started":"2025-05-30T21:31:48.238960Z","shell.execute_reply":"2025-05-30T21:31:48.238975Z"}},"outputs":[],"execution_count":null}]}