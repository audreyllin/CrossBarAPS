{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Environment Setup\n!pip install faiss-cpu langdetect python-docx googletrans==4.0.0-rc1\n!pip install sentence-transformers transformers torch datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:24:56.017145Z","iopub.execute_input":"2025-06-05T06:24:56.017623Z","iopub.status.idle":"2025-06-05T06:27:04.080518Z","shell.execute_reply.started":"2025-06-05T06:24:56.017585Z","shell.execute_reply":"2025-06-05T06:27:04.077631Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\nRequirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\nRequirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\nRequirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.11/dist-packages (4.0.0rc1)\nRequirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.11/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.4.26)\nRequirement already satisfied: hstspreload in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\nRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\nRequirement already satisfied: idna==2.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\nRequirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\nRequirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\nRequirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\nRequirement already satisfied: h2==3.* in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\nRequirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\nRequirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\nRequirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport re\nimport docx\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport transformers\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    pipeline,\n    BitsAndBytesConfig\n)\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom langdetect import detect\nfrom googletrans import Translator\nfrom typing import List, Dict, Union, Optional\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nclass DocumentProcessor:\n    \"\"\"Handles document text extraction (simplified version)\"\"\"\n    \n    @staticmethod\n    def extract_text_from_docx(file_path: str) -> str:\n        \"\"\"Extract text from DOCX files\"\"\"\n        try:\n            doc = docx.Document(file_path)\n            return \"\\n\".join([para.text for para in doc.paragraphs])\n        except Exception as e:\n            print(f\"Error reading DOCX: {str(e)}\")\n            return \"\"\n    \n    @staticmethod\n    def extract_text_from_txt(file_path: str) -> str:\n        \"\"\"Extract text from TXT files\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                return f.read()\n        except Exception as e:\n            print(f\"Error reading TXT: {str(e)}\")\n            return \"\"\n    \n    @staticmethod\n    def process_uploaded_file(file_path: str) -> str:\n        \"\"\"Process supported file formats\"\"\"\n        ext = Path(file_path).suffix.lower()\n        if ext == '.docx':\n            return DocumentProcessor.extract_text_from_docx(file_path)\n        elif ext == '.txt':\n            return DocumentProcessor.extract_text_from_txt(file_path)\n        else:\n            raise ValueError(f\"Unsupported file format: {ext}\")\n\nclass KnowledgeBase:\n    \"\"\"Manages the vector database and document storage\"\"\"\n    \n    def __init__(self):\n        self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n        self.index = None\n        self.documents = []\n        self.doc_embeddings = []\n        \n    def add_document(self, text: str, metadata: dict = None):\n        \"\"\"Add a document to the knowledge base\"\"\"\n        if not text.strip():\n            return\n            \n        chunks = self._chunk_text(text)\n        \n        for chunk in chunks:\n            self.documents.append({\n                \"text\": chunk,\n                \"metadata\": metadata or {}\n            })\n            \n    def _chunk_text(self, text: str, chunk_size: int = 512) -> List[str]:\n        \"\"\"Split text into manageable chunks\"\"\"\n        words = text.split()\n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for word in words:\n            if current_length + len(word) + 1 <= chunk_size:\n                current_chunk.append(word)\n                current_length += len(word) + 1\n            else:\n                chunks.append(\" \".join(current_chunk))\n                current_chunk = [word]\n                current_length = len(word)\n        \n        if current_chunk:\n            chunks.append(\" \".join(current_chunk))\n            \n        return chunks\n    \n    def build_index(self):\n        \"\"\"Create FAISS index from document embeddings\"\"\"\n        if not self.documents:\n            raise ValueError(\"No documents to index\")\n            \n        texts = [doc[\"text\"] for doc in self.documents]\n        self.doc_embeddings = self.embedder.encode(texts, show_progress_bar=True)\n        \n        dimension = self.doc_embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(dimension)\n        self.index.add(self.doc_embeddings)\n        \n    def search(self, query: str, k: int = 3) -> List[Dict]:\n        \"\"\"Search for relevant documents\"\"\"\n        if self.index is None:\n            self.build_index()\n            \n        query_embedding = self.embedder.encode([query])\n        distances, indices = self.index.search(query_embedding, k)\n        \n        results = []\n        for idx, distance in zip(indices[0], distances[0]):\n            if idx >= 0:\n                results.append({\n                    \"text\": self.documents[idx][\"text\"],\n                    \"metadata\": self.documents[idx][\"metadata\"],\n                    \"score\": float(distance)\n                })\n        \n        return results\n\nclass MultilingualSupport:\n    \"\"\"Handles language detection and translation\"\"\"\n    \n    def __init__(self):\n        self.translator = Translator()\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"Detect language of input text\"\"\"\n        try:\n            return detect(text)\n        except:\n            return \"en\"  # Default to English\n    \n    def translate_to_english(self, text: str, src_lang: str = None) -> str:\n        \"\"\"Translate non-English text to English\"\"\"\n        if not src_lang:\n            src_lang = self.detect_language(text)\n            \n        if src_lang == 'en':\n            return text\n            \n        try:\n            translated = self.translator.translate(text, src=src_lang, dest='en')\n            return translated.text\n        except Exception as e:\n            print(f\"Translation error: {str(e)}\")\n            return text\n    \n    def translate_from_english(self, text: str, dest_lang: str) -> str:\n        \"\"\"Translate English text to target language\"\"\"\n        if dest_lang == 'en':\n            return text\n            \n        try:\n            translated = self.translator.translate(text, src='en', dest=dest_lang)\n            return translated.text\n        except Exception as e:\n            print(f\"Translation error: {str(e)}\")\n            return text\n\nclass CryptoWalletAssistant:\n    \"\"\"Main RAG system for crypto wallet Q&A\"\"\"\n    \n    def __init__(self, model_name: str = \"gpt2\"):\n        self.knowledge_base = KnowledgeBase()\n        self.multilingual = MultilingualSupport()\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Load language model\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n        ).to(self.device)\n        \n        self.generator = pipeline(\n            \"text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            device=0 if self.device == \"cuda\" else -1\n        )\n        \n        # Add default crypto knowledge\n        self._initialize_default_knowledge()\n    \n    def _initialize_default_knowledge(self):\n        \"\"\"Add default crypto knowledge to the knowledge base\"\"\"\n        default_knowledge = [\n            \"A hardware wallet is a physical device that stores users' private keys offline.\",\n            \"A software wallet is an application that stores private keys on internet-connected devices.\",\n            \"Proof of Work (PoW) is a consensus mechanism that requires computational work to validate transactions.\",\n            \"A private key is a secret number that allows cryptocurrency to be spent.\",\n            \"A public key is derived from a private key and can be shared to receive cryptocurrency.\",\n            \"Ledger is a popular hardware wallet brand that provides secure storage for crypto assets.\",\n            \"Cold storage refers to keeping cryptocurrency completely offline for maximum security.\",\n            \"A seed phrase (or recovery phrase) is a set of words that can regenerate all private keys in a wallet.\"\n        ]\n        \n        for text in default_knowledge:\n            self.knowledge_base.add_document(text, {\"source\": \"default_knowledge\"})\n        \n        self.knowledge_base.build_index()\n    \n    def add_documents(self, file_paths: List[str]):\n        \"\"\"Process and add uploaded documents to knowledge base\"\"\"\n        for file_path in file_paths:\n            try:\n                text = DocumentProcessor.process_uploaded_file(file_path)\n                if text:\n                    filename = Path(file_path).name\n                    self.knowledge_base.add_document(text, {\"source\": filename})\n                    print(f\"Processed: {filename}\")\n            except Exception as e:\n                print(f\"Error processing {file_path}: {str(e)}\")\n        \n        self.knowledge_base.build_index()\n    \n    def generate_response(self, query: str, max_length: int = 300) -> str:\n        \"\"\"Generate answer using RAG approach\"\"\"\n        query_lang = self.multilingual.detect_language(query)\n        \n        if query_lang != 'en':\n            english_query = self.multilingual.translate_to_english(query, query_lang)\n        else:\n            english_query = query\n        \n        retrieved_docs = self.knowledge_base.search(english_query)\n        context = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n        \n        prompt = f\"\"\"Answer the question based on the context below. Keep your response concise and technical.\nIf you don't know the answer, say you don't know.\n\nContext: {context}\n\nQuestion: {english_query}\nAnswer:\"\"\"\n        \n        response = self.generator(\n            prompt,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )[0]['generated_text']\n        \n        answer = response.split(\"Answer:\")[-1].strip()\n        \n        if query_lang != 'en':\n            answer = self.multilingual.translate_from_english(answer, query_lang)\n        \n        return answer\n    \n    def chat_interface(self):\n        \"\"\"Simple command-line chat interface\"\"\"\n        print(\"\\nWelcome to Crypto Wallet Assistant!\")\n        print(\"Type 'quit' to exit or 'upload' to add documents.\\n\")\n        \n        while True:\n            query = input(\"User: \")\n            if query.lower() in ['quit', 'exit']:\n                break\n                \n            if query.lower() == 'upload':\n                file_paths = input(\"Enter file paths (comma separated): \").split(',')\n                file_paths = [f.strip() for f in file_paths]\n                self.add_documents(file_paths)\n                print(\"Documents processed successfully!\")\n                continue\n                \n            response = self.generate_response(query)\n            print(f\"\\nAssistant: {response}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:27:04.084986Z","iopub.execute_input":"2025-06-05T06:27:04.085534Z","iopub.status.idle":"2025-06-05T06:27:04.130547Z","shell.execute_reply.started":"2025-06-05T06:27:04.085491Z","shell.execute_reply":"2025-06-05T06:27:04.128869Z"}},"outputs":[{"name":"stdout","text":"\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nPyTorch: 2.6.0+cu124\nTransformers: 4.51.3\nCUDA available: False\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    assistant = CryptoWalletAssistant()\n    assistant.chat_interface()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:27:04.131739Z","iopub.execute_input":"2025-06-05T06:27:04.132280Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0491904c169446983686b3b3d0a520c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"323a38248c43477b9e035d91b7ce94a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1bdcd0171944b7a7e38303c9fbc600"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"741b1859d61540f0a47ae589a72492f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860ec432e051415a9ab9322c5a049109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c1cc57817747ee874f1a7cd7ccf91c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc8eeca9c4c34bfb842e52f9590db860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31b0e79dc0b4379b661c011084d99f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ade5c43170643c1ae6e988a32aa8033"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868910ea18634ce8a85660dc8a143037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b20f56aa444e54a349ce85e40fd582"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5bbea1bacd7475aa0c0df79e1109025"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"419cdd9b0e864397b8bf5ea285e1ecdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c69d4997e3435589f24c7df48dca60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14757766d8b84bd092c71b469802828c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1422659f8323415d9c86654e87c26a73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a0a2389d13e498d8a146440b12a2e9a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc8129cd6c648a9b5276dc5035f2a2a"}},"metadata":{}},{"name":"stdout","text":"\nWelcome to Crypto Wallet Assistant!\nType 'quit' to exit or 'upload' to add documents.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  Explain PoW\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb9d08bae23d4265b6d3525d8665b267"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: Proof of Work is a consensus mechanism that requires computational work to validate transactions.\n\nA seed phrase (or recovery phrase) is a set of words that can regenerate all private keys in a wallet.\n\nA software wallet is an application that stores private keys on internet-\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  what is a consensus mechanism\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b6df27db4744c0eb0498c95547e3306"}},"metadata":{}},{"name":"stdout","text":"\nAssistant: a consensus mechanism is a mechanism that is not only for a consensus mechanism but also for a mechanism that can be used to implement an entire system.\n\nA consensus mechanism is an algorithm that is used to determine the exact rules of a system.\n\nThe consensus mechanism is a mechanism that can be used to implement a whole system.\n\nQuestions about a consensus mechanism:\n\nWhat is a consensus mechanism?\n\nWhat is a proof of work mechanism?\n\nWhat is a proof of work mechanism (PoW)?\n\nWhat is a proof of work mechanism (PoW) for a protocol?\n\nWhat is a proof of work mechanism (PoW) for a security policy?\n\nHow can a consensus mechanism be used to implement an entire system?\n\nHow can a consensus mechanism be used to implement a whole system?\n\nWhat is a consensus mechanism?\n\nA consensus mechanism is a mechanism that can be used\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  it can be used for what\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ef87b249b149f29ec01fdd0af6a0c3"}},"metadata":{}},{"name":"stdout","text":"\nAssistant: It is a security measure\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  is consensus mechanism a mechanism that can be used for secuirty measure?\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b784ea2ede046f0858e2ee935c1e76d"}},"metadata":{}},{"name":"stdout","text":"\nAssistant: Yes, consensus mechanism is a mechanism to validate transactions.\n\nFact: consensus mechanism is an algorithm that can be used to validate transactions.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  what transactions are we talking about\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8575d9d39cc44a02822d0f909f60fa04"}},"metadata":{}},{"name":"stdout","text":"\nAssistant: we're talking about the proof of work.\n\nProof of work is a new type of proof of work, which means that a proof of work is not just a proof of work for a specific chain of transactions, but also a proof of work for a chain of transactions that does not have any previous proof of work.\n\nThe proof of work chain is the chain of transactions that contains all of the assets that are in the chain of transactions.\n\nThe proof of work chain is composed of two parts.\n\nThe first part is the chain of transactions that contains all of the assets in the chain.\n\nThe second part is the chain of transactions that does not contain the assets that were in the chain.\n\nThis is the second part of the proof of work chain.\n\nThe second part is the proof of work chain that contains all of the assets that were in the chain.\n\nProof of work is also known as a block chain.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  what chain are we talking about regarding PoW\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c640327f9a45f39ed6c851116fb000"}},"metadata":{}},{"name":"stdout","text":"\nAssistant: a block chain is a cryptographic protocol (or protocol) that is built on top of a network of nodes, which is what we're talking about here.\n\nA block chain is an encrypted block, which is the chain of data that is sent to a computer.\n\nA block is an encrypted block is the block that is sent to the computer.\n\nThere are two main types of blocks:\n\nBEGINNING: A block is created by a computer (or network) that is connected to the internet. This is called a \"computer\".\n\nBEGINNING ENDING: A block is created by a computer (or network) that is connected to the internet. This is called a \"computer\".\n\nIt's not a \"computer\", it's a computer that runs on a computer.\n\nThe main purpose of a computer is to process data, which is the data that can be sent to\n\n","output_type":"stream"}],"execution_count":null}]}