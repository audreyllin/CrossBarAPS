{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install faiss-cpu\n!pip install langdetect\n!pip install python-docx\n# !pip install ebooklib\n!pip install googletrans==4.0.0-rc1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:08.986225Z","iopub.execute_input":"2025-06-05T06:05:08.986628Z","iopub.status.idle":"2025-06-05T06:05:35.376214Z","shell.execute_reply.started":"2025-06-05T06:05:08.986600Z","shell.execute_reply":"2025-06-05T06:05:35.374634Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\nRequirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\nRequirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\nCollecting googletrans==4.0.0-rc1\n  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.4.26)\nCollecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\nCollecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\nCollecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\nDownloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m913.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nDownloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=c783048a11a1443d646294ce3dd7ac34e06ecd8c4333ad5c8e0a610a7b824908\n  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\nSuccessfully built googletrans\nInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n  Attempting uninstall: hyperframe\n    Found existing installation: hyperframe 6.1.0\n    Uninstalling hyperframe-6.1.0:\n      Successfully uninstalled hyperframe-6.1.0\n  Attempting uninstall: hpack\n    Found existing installation: hpack 4.1.0\n    Uninstalling hpack-4.1.0:\n      Successfully uninstalled hpack-4.1.0\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: chardet\n    Found existing installation: chardet 5.2.0\n    Uninstalling chardet-5.2.0:\n      Successfully uninstalled chardet-5.2.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: h2\n    Found existing installation: h2 4.2.0\n    Uninstalling h2-4.2.0:\n      Successfully uninstalled h2-4.2.0\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.7\n    Uninstalling httpcore-1.0.7:\n      Successfully uninstalled httpcore-1.0.7\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.28.1\n    Uninstalling httpx-0.28.1:\n      Successfully uninstalled httpx-0.28.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nopenai 1.70.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\nlangsmith 0.3.23 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-genai 1.9.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 1: Enhanced Environment Setup\n# =================================\nimport os\nimport sys\nimport json\nimport re\n# import fitz  # PyMuPDF\nimport docx\n# import epub\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport transformers\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    pipeline,\n    BitsAndBytesConfig\n)\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom langdetect import detect\nfrom googletrans import Translator\nfrom typing import List, Dict, Union, Optional\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"NO_TF\"] = \"1\"\n\nprint(\"\\n=== Core Package Versions ===\")\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:35.378531Z","iopub.execute_input":"2025-06-05T06:05:35.379818Z","iopub.status.idle":"2025-06-05T06:05:35.521004Z","shell.execute_reply.started":"2025-06-05T06:05:35.379762Z","shell.execute_reply":"2025-06-05T06:05:35.519620Z"}},"outputs":[{"name":"stdout","text":"\n=== Core Package Versions ===\nPython: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nPyTorch: 2.6.0+cu124\nTransformers: 4.51.3\nCUDA available: False\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 2: Document Processing System\n# =================================\nclass DocumentProcessor:\n    \"\"\"Handles document uploads and text extraction\"\"\"\n    \n    @staticmethod\n    def extract_text_from_pdf(file_path: str) -> str:\n        \"\"\"Extract text from PDF files\"\"\"\n        text = \"\"\n        try:\n            with fitz.open(file_path) as doc:\n                for page in doc:\n                    text += page.get_text()\n        except Exception as e:\n            print(f\"Error reading PDF: {str(e)}\")\n        return text\n    \n    @staticmethod\n    def extract_text_from_docx(file_path: str) -> str:\n        \"\"\"Extract text from DOCX files\"\"\"\n        try:\n            doc = docx.Document(file_path)\n            return \"\\n\".join([para.text for para in doc.paragraphs])\n        except Exception as e:\n            print(f\"Error reading DOCX: {str(e)}\")\n            return \"\"\n    \n    @staticmethod\n    def extract_text_from_txt(file_path: str) -> str:\n        \"\"\"Extract text from TXT files\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                return f.read()\n        except Exception as e:\n            print(f\"Error reading TXT: {str(e)}\")\n            return \"\"\n            \n    \n    @staticmethod\n    def extract_text_from_epub(file_path: str) -> str:\n        \"\"\"Extract text from EPUB files\"\"\"\n        text = \"\"\n        try:\n            book = epub.read_epub(file_path)\n            for item in book.get_items():\n                if item.get_type() == epub.EpubHtml:\n                    text += item.get_content().decode('utf-8')\n        except Exception as e:\n            print(f\"Error reading EPUB: {str(e)}\")\n        return text\n    \n    @staticmethod\n    def process_uploaded_file(file_path: str) -> str:\n        \"\"\"Process any supported file format\"\"\"\n        ext = Path(file_path).suffix.lower()\n        if ext == '.pdf':\n            return DocumentProcessor.extract_text_from_pdf(file_path)\n        elif ext == '.docx':\n            return DocumentProcessor.extract_text_from_docx(file_path)\n        elif ext == '.txt':\n            return DocumentProcessor.extract_text_from_txt(file_path)\n        elif ext == '.epub':\n            return DocumentProcessor.extract_text_from_epub(file_path)\n        else:\n            raise ValueError(f\"Unsupported file format: {ext}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:35.522120Z","iopub.execute_input":"2025-06-05T06:05:35.522457Z","iopub.status.idle":"2025-06-05T06:05:35.535619Z","shell.execute_reply.started":"2025-06-05T06:05:35.522424Z","shell.execute_reply":"2025-06-05T06:05:35.534453Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cell 3: Knowledge Base Management\n# ================================\nclass KnowledgeBase:\n    \"\"\"Manages the vector database and document storage\"\"\"\n    \n    def __init__(self):\n        self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n        self.index = None\n        self.documents = []\n        self.doc_embeddings = []\n        \n    def add_document(self, text: str, metadata: dict = None):\n        \"\"\"Add a document to the knowledge base\"\"\"\n        if not text.strip():\n            return\n            \n        # Split into chunks (adjust based on your needs)\n        chunks = self._chunk_text(text)\n        \n        for chunk in chunks:\n            self.documents.append({\n                \"text\": chunk,\n                \"metadata\": metadata or {}\n            })\n            \n    \n    def _chunk_text(self, text: str, chunk_size: int = 512) -> List[str]:\n        \"\"\"Split text into manageable chunks\"\"\"\n        words = text.split()\n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for word in words:\n            if current_length + len(word) + 1 <= chunk_size:\n                current_chunk.append(word)\n                current_length += len(word) + 1\n            else:\n                chunks.append(\" \".join(current_chunk))\n                current_chunk = [word]\n                current_length = len(word)\n        \n        if current_chunk:\n            chunks.append(\" \".join(current_chunk))\n            \n        return chunks\n    \n    def build_index(self):\n        \"\"\"Create FAISS index from document embeddings\"\"\"\n        if not self.documents:\n            raise ValueError(\"No documents to index\")\n            \n        texts = [doc[\"text\"] for doc in self.documents]\n        self.doc_embeddings = self.embedder.encode(texts, show_progress_bar=True)\n        \n        # Create FAISS index\n        dimension = self.doc_embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(dimension)\n        self.index.add(self.doc_embeddings)\n        \n    \n    def search(self, query: str, k: int = 3) -> List[Dict]:\n        \"\"\"Search for relevant documents\"\"\"\n        if self.index is None:\n            self.build_index()\n            \n        query_embedding = self.embedder.encode([query])\n        distances, indices = self.index.search(query_embedding, k)\n        \n        results = []\n        for idx, distance in zip(indices[0], distances[0]):\n            if idx >= 0:  # FAISS may return -1 for invalid indices\n                results.append({\n                    \"text\": self.documents[idx][\"text\"],\n                    \"metadata\": self.documents[idx][\"metadata\"],\n                    \"score\": float(distance)\n                })\n        \n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:35.537016Z","iopub.execute_input":"2025-06-05T06:05:35.537452Z","iopub.status.idle":"2025-06-05T06:05:35.565724Z","shell.execute_reply.started":"2025-06-05T06:05:35.537399Z","shell.execute_reply":"2025-06-05T06:05:35.564624Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from typing import List","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:35.567951Z","iopub.execute_input":"2025-06-05T06:05:35.568646Z","iopub.status.idle":"2025-06-05T06:05:35.594280Z","shell.execute_reply.started":"2025-06-05T06:05:35.568610Z","shell.execute_reply":"2025-06-05T06:05:35.592734Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 4: Multilingual Support\n# ============================\nclass MultilingualSupport:\n    \"\"\"Handles language detection and translation\"\"\"\n    \n    def __init__(self):\n        self.translator = Translator()\n    \n    def detect_language(self, text: str) -> str:\n        \"\"\"Detect language of input text\"\"\"\n        try:\n            return detect(text)\n        except:\n            return \"en\"  # Default to English\n    \n    def translate_to_english(self, text: str, src_lang: str = None) -> str:\n        \"\"\"Translate non-English text to English\"\"\"\n        if not src_lang:\n            src_lang = self.detect_language(text)\n            \n        if src_lang == 'en':\n            return text\n            \n        try:\n            translated = self.translator.translate(text, src=src_lang, dest='en')\n            return translated.text\n        except Exception as e:\n            print(f\"Translation error: {str(e)}\")\n            return text\n    \n    def translate_from_english(self, text: str, dest_lang: str) -> str:\n        \"\"\"Translate English text to target language\"\"\"\n        if dest_lang == 'en':\n            return text\n            \n        try:\n            translated = self.translator.translate(text, src='en', dest=dest_lang)\n            return translated.text\n        except Exception as e:\n            print(f\"Translation error: {str(e)}\")\n            return text\n\n# Cell 5: RAG System Integration\n# ==============================\nclass AudreyRAGSystem:\n    \"\"\"Main RAG system for crypto wallet Q&A\"\"\"\n    \n    def __init__(self, model_name: str = \"gpt2\"):\n        # Initialize components\n        self.knowledge_base = KnowledgeBase()\n        self.multilingual = MultilingualSupport()\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Load language model\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n        ).to(self.device)\n        \n        # Initialize pipeline\n        self.generator = pipeline(\n            \"text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            device=0 if self.device == \"cuda\" else -1\n        )\n    \n    def add_documents(self, file_paths: List[str]):\n        \"\"\"Process and add uploaded documents to knowledge base\"\"\"\n        for file_path in file_paths:\n            try:\n                text = DocumentProcessor.process_uploaded_file(file_path)\n                if text:\n                    filename = Path(file_path).name\n                    self.knowledge_base.add_document(text, {\"source\": filename})\n                    print(f\"Processed: {filename}\")\n            except Exception as e:\n                print(f\"Error processing {file_path}: {str(e)}\")\n        \n        # Rebuild index after adding documents\n        self.knowledge_base.build_index()\n    \n    def generate_response(self, query: str, max_length: int = 300) -> str:\n        \"\"\"Generate answer using RAG approach\"\"\"\n        # Detect query language\n        query_lang = self.multilingual.detect_language(query)\n        \n        # Translate non-English queries to English for retrieval\n        if query_lang != 'en':\n            english_query = self.multilingual.translate_to_english(query, query_lang)\n        else:\n            english_query = query\n        \n        # Retrieve relevant documents\n        retrieved_docs = self.knowledge_base.search(english_query)\n        context = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n        \n        # Prepare prompt with context\n        prompt = f\"\"\"Answer the question based on the context below. If you don't know the answer, say you don't know.\n\nContext: {context}\n\nQuestion: {english_query}\nAnswer:\"\"\"\n        \n        # Generate response\n        response = self.generator(\n            prompt,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )[0]['generated_text']\n        \n        # Extract just the answer part\n        answer = response.split(\"Answer:\")[-1].strip()\n        \n        # Translate back to original language if needed\n        if query_lang != 'en':\n            answer = self.multilingual.translate_from_english(answer, query_lang)\n        \n        return answer\n    \n    def chat_interface(self):\n        \"\"\"Simple command-line chat interface\"\"\"\n        print(\"\\nWelcome to Audrey Crypto Wallet Assistant!\")\n        print(\"Type 'quit' to exit.\\n\")\n        \n        while True:\n            # User query\n            query = input(\"You: \")\n            if query.lower() in ['quit', 'exit']:\n                break\n                \n            # Document upload option\n            if query.lower() == 'upload':\n                file_paths = input(\"Enter file paths (comma separated): \").split(',')\n                file_paths = [f.strip() for f in file_paths]\n                self.add_documents(file_paths)\n                print(\"Documents processed successfully!\")\n                continue\n                \n            # Get response\n            response = self.generate_response(query)\n            print(f\"\\nAudrey: {response}\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:35.595593Z","iopub.execute_input":"2025-06-05T06:05:35.595890Z","iopub.status.idle":"2025-06-05T06:05:35.619693Z","shell.execute_reply.started":"2025-06-05T06:05:35.595864Z","shell.execute_reply":"2025-06-05T06:05:35.617857Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from your_module_name import KnowledgeBase  # Replace 'your_module_name' with the correct module name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:35.621227Z","iopub.execute_input":"2025-06-05T06:05:35.621657Z","iopub.status.idle":"2025-06-05T06:05:35.707688Z","shell.execute_reply.started":"2025-06-05T06:05:35.621622Z","shell.execute_reply":"2025-06-05T06:05:35.706036Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_185/3239801318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myour_module_name\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKnowledgeBase\u001b[0m  \u001b[0;31m# Replace 'your_module_name' with the correct module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'your_module_name'"],"ename":"ModuleNotFoundError","evalue":"No module named 'your_module_name'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# Cell 6: Main Execution\n# ======================\nif __name__ == \"__main__\":\n    # Initialize system\n    audrey = AudreyRAGSystem()\n    \n    # Add some default crypto knowledge (optional)\n    default_knowledge = [\n        \"A hardware wallet is a physical device that stores users' private keys offline.\",\n        \"A software wallet is an application that stores private keys on internet-connected devices.\",\n        \"Proof of Work (PoW) is a consensus mechanism that requires computational work to validate transactions.\",\n        \"A private key is a secret number that allows cryptocurrency to be spent.\",\n        \"A public key is derived from a private key and can be shared to receive cryptocurrency.\"\n    ]\n    \n    for text in default_knowledge:\n        audrey.knowledge_base.add_document(text)\n    \n    # Start chat interface\n    audrey.chat_interface()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T06:05:35.708482Z","iopub.status.idle":"2025-06-05T06:05:35.708837Z","shell.execute_reply.started":"2025-06-05T06:05:35.708686Z","shell.execute_reply":"2025-06-05T06:05:35.708702Z"}},"outputs":[],"execution_count":null}]}